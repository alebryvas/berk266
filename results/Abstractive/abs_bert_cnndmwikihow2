[2020-03-29 01:32:53,788 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=256, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=1000, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=200000, use_bert_emb=True, use_interval=True, visible_gpus='0', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=1)
[2020-03-29 01:32:53,788 INFO] Device ID 0
[2020-03-29 01:32:53,788 INFO] Device cuda
[2020-03-29 01:32:59,033 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:32:59,034 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:32:59,305 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:33:19,453 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 01:33:19,742 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpnqnwib6e
[2020-03-29 01:33:20,213 INFO] copying /tmp/tmpnqnwib6e to cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:33:20,213 INFO] creating metadata file for ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:33:20,214 INFO] removing temp file /tmp/tmpnqnwib6e
[2020-03-29 01:33:20,214 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:33:20,243 INFO] * number of parameters: 180222522
[2020-03-29 01:33:20,243 INFO] Start training...
[2020-03-29 01:33:20,425 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 01:34:53,059 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=512, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=1000, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=200000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 01:34:53,059 INFO] Device ID 0
[2020-03-29 01:34:53,060 INFO] Device cuda
[2020-03-29 01:34:57,273 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:34:57,274 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:34:57,274 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:34:57,274 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:34:57,278 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:34:57,278 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:34:57,279 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:34:57,280 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:34:57,546 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:34:57,549 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:34:57,557 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:34:57,594 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:35:04,592 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 01:35:04,938 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:35:04,968 INFO] * number of parameters: 180222522
[2020-03-29 01:35:04,968 INFO] Start training...
[2020-03-29 01:35:05,038 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:35:05,120 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:35:05,149 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 01:35:05,467 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:36:44,417 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=140, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=1000, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=200000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 01:36:44,418 INFO] Device ID 0
[2020-03-29 01:36:44,418 INFO] Device cuda
[2020-03-29 01:36:48,576 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:36:48,577 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:36:48,587 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:36:48,587 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:36:48,592 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:36:48,593 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:36:48,606 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:36:48,607 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:36:48,853 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:36:48,854 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:36:48,864 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:36:48,871 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:36:56,151 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 01:36:56,433 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:36:56,444 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:36:56,465 INFO] * number of parameters: 180222522
[2020-03-29 01:36:56,465 INFO] Start training...
[2020-03-29 01:36:56,622 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:36:56,635 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:36:56,666 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 01:42:32,889 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=140, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=1000, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=200000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 01:42:32,890 INFO] Device ID 0
[2020-03-29 01:42:32,890 INFO] Device cuda
[2020-03-29 01:42:36,825 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:42:36,825 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:42:36,827 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:42:36,827 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:42:36,831 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:42:36,831 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:42:36,846 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:42:36,846 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:42:37,090 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:42:37,133 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:42:37,135 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:42:37,135 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:42:44,421 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 01:42:44,632 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:42:44,707 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:42:44,743 INFO] * number of parameters: 180222522
[2020-03-29 01:42:44,743 INFO] Start training...
[2020-03-29 01:42:44,772 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:42:44,991 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 01:42:45,154 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:53:48,539 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=140, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=1000, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=200000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=3)
[2020-03-29 01:53:48,539 INFO] Device ID 0
[2020-03-29 01:53:48,539 INFO] Device cuda
[2020-03-29 01:53:51,886 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:53:51,887 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:53:51,892 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:53:51,892 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:53:51,915 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 01:53:51,916 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 01:53:52,158 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:53:52,182 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:53:52,194 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 01:53:58,945 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:53:59,006 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 01:53:59,311 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:53:59,340 INFO] * number of parameters: 180222522
[2020-03-29 01:53:59,340 INFO] Start training...
[2020-03-29 01:53:59,442 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 01:53:59,529 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 02:09:59,446 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=140, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=1000, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=200000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 02:09:59,452 INFO] Device ID 0
[2020-03-29 02:09:59,452 INFO] Device cuda
[2020-03-29 02:10:02,696 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:10:02,699 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:10:02,700 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:10:02,700 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:10:02,701 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:10:02,702 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:10:02,712 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:10:02,713 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:10:02,963 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:10:03,000 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:10:03,008 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:10:03,042 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:10:27,390 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 02:10:27,477 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:10:27,693 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:10:27,715 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:10:27,723 INFO] * number of parameters: 180222522
[2020-03-29 02:10:27,724 INFO] Start training...
[2020-03-29 02:10:27,825 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:10:28,031 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 02:13:05,749 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=100, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=1000, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 02:13:05,749 INFO] Device ID 0
[2020-03-29 02:13:05,749 INFO] Device cuda
[2020-03-29 02:13:10,006 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:13:10,007 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:13:10,012 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:13:10,012 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:13:10,020 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:13:10,021 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:13:10,031 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:13:10,032 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:13:10,288 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:13:10,290 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:13:10,300 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:13:10,316 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:13:17,165 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 02:13:17,309 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:13:17,481 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:13:17,510 INFO] * number of parameters: 180222522
[2020-03-29 02:13:17,510 INFO] Start training...
[2020-03-29 02:13:17,626 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:13:17,665 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:13:17,690 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 02:14:50,367 INFO] Step 50/210000; acc:   0.00; ppl: 15151.81; xent: 9.63; lr: 0.00000004;   0/916 tok/s;     93 sec
[2020-03-29 02:18:36,981 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=90, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=1000, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 02:18:36,981 INFO] Device ID 0
[2020-03-29 02:18:36,981 INFO] Device cuda
[2020-03-29 02:18:41,236 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:18:41,237 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:18:41,238 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:18:41,238 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:18:41,241 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:18:41,241 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:18:41,249 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 02:18:41,250 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 02:18:41,525 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:18:41,527 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:18:41,532 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:18:41,535 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 02:18:48,369 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 02:18:48,667 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:18:48,696 INFO] * number of parameters: 180222522
[2020-03-29 02:18:48,696 INFO] Start training...
[2020-03-29 02:18:48,747 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:18:48,807 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:18:48,846 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 02:18:48,883 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 02:20:18,515 INFO] Step 50/210000; acc:   0.00; ppl: 13502.64; xent: 9.51; lr: 0.00000004;   0/757 tok/s;     90 sec
[2020-03-29 02:21:28,113 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-29 02:21:48,765 INFO] Step 100/210000; acc:   0.60; ppl: 1810.48; xent: 7.50; lr: 0.00000007;   0/618 tok/s;    180 sec
[2020-03-29 02:23:17,290 INFO] Step 150/210000; acc:   1.56; ppl: 1059.72; xent: 6.97; lr: 0.00000011;   0/674 tok/s;    268 sec
[2020-03-29 02:24:08,805 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-29 02:24:48,005 INFO] Step 200/210000; acc:   2.42; ppl: 736.11; xent: 6.60; lr: 0.00000014;   0/868 tok/s;    359 sec
[2020-03-29 02:26:17,375 INFO] Step 250/210000; acc:   2.81; ppl: 703.52; xent: 6.56; lr: 0.00000018;   0/782 tok/s;    448 sec
[2020-03-29 02:26:48,429 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-29 02:27:47,122 INFO] Step 300/210000; acc:   3.94; ppl: 548.04; xent: 6.31; lr: 0.00000021;   0/1071 tok/s;    538 sec
[2020-03-29 02:29:17,321 INFO] Step 350/210000; acc:   6.35; ppl: 475.05; xent: 6.16; lr: 0.00000025;   0/1223 tok/s;    628 sec
[2020-03-29 02:29:28,556 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-29 02:30:48,406 INFO] Step 400/210000; acc:   6.98; ppl: 509.77; xent: 6.23; lr: 0.00000028;   0/568 tok/s;    720 sec
[2020-03-29 02:32:09,837 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-29 02:32:19,273 INFO] Step 450/210000; acc:   9.42; ppl: 310.28; xent: 5.74; lr: 0.00000032;   0/730 tok/s;    810 sec
[2020-03-29 02:33:48,982 INFO] Step 500/210000; acc:  10.24; ppl: 306.58; xent: 5.73; lr: 0.00000035;   0/777 tok/s;    900 sec
[2020-03-29 02:34:51,483 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-29 02:35:20,522 INFO] Step 550/210000; acc:  12.15; ppl: 258.76; xent: 5.56; lr: 0.00000039;   0/971 tok/s;    992 sec
[2020-03-29 02:36:50,744 INFO] Step 600/210000; acc:  11.67; ppl: 257.47; xent: 5.55; lr: 0.00000042;   0/1008 tok/s;   1082 sec
[2020-03-29 02:37:32,387 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-29 02:38:21,613 INFO] Step 650/210000; acc:  12.41; ppl: 237.43; xent: 5.47; lr: 0.00000046;   0/591 tok/s;   1173 sec
[2020-03-29 02:39:51,684 INFO] Step 700/210000; acc:  12.52; ppl: 214.33; xent: 5.37; lr: 0.00000049;   0/600 tok/s;   1263 sec
[2020-03-29 02:40:13,032 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-29 02:41:21,874 INFO] Step 750/210000; acc:  13.66; ppl: 214.64; xent: 5.37; lr: 0.00000053;   0/727 tok/s;   1353 sec
[2020-03-29 02:42:51,641 INFO] Step 800/210000; acc:  12.97; ppl: 192.87; xent: 5.26; lr: 0.00000057;   0/787 tok/s;   1443 sec
[2020-03-29 03:07:50,083 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=90, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 03:07:50,083 INFO] Device ID 0
[2020-03-29 03:07:50,083 INFO] Device cuda
[2020-03-29 03:07:54,018 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:07:54,019 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:07:54,025 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:07:54,026 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:07:54,026 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:07:54,026 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:07:54,027 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:07:54,027 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:07:54,289 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:07:54,294 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:07:54,301 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:07:54,324 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:08:01,575 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 03:08:01,854 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:08:01,861 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:08:01,875 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:08:01,891 INFO] * number of parameters: 180222522
[2020-03-29 03:08:01,891 INFO] Start training...
[2020-03-29 03:08:01,977 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:08:02,087 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 03:09:31,629 INFO] Step 50/210000; acc:   0.00; ppl: 13502.64; xent: 9.51; lr: 0.00000004;   0/743 tok/s;     90 sec
[2020-03-29 03:14:39,235 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=100, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 03:14:39,240 INFO] Device ID 0
[2020-03-29 03:14:39,240 INFO] Device cuda
[2020-03-29 03:14:43,491 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:14:43,494 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:14:43,494 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:14:43,495 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:14:43,500 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:14:43,501 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:14:43,522 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:14:43,523 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:14:43,775 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:14:43,793 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:14:43,793 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:14:43,818 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:15:09,864 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 03:15:10,040 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:15:10,131 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:15:10,154 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:15:10,186 INFO] * number of parameters: 180222522
[2020-03-29 03:15:10,186 INFO] Start training...
[2020-03-29 03:15:10,284 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:15:10,693 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 03:16:43,256 INFO] Step 50/210000; acc:   0.00; ppl: 15151.81; xent: 9.63; lr: 0.00000004;   0/902 tok/s;     93 sec
[2020-03-29 03:22:39,439 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=90, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 03:22:39,439 INFO] Device ID 0
[2020-03-29 03:22:39,439 INFO] Device cuda
[2020-03-29 03:22:43,690 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:22:43,691 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:22:43,722 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:22:43,722 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:22:43,732 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:22:43,732 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:22:43,732 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 03:22:43,733 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 03:22:43,972 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:22:44,016 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:22:44,020 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:22:44,022 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 03:22:51,366 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 03:22:51,421 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:22:51,518 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:22:51,572 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:22:51,650 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 03:22:51,680 INFO] * number of parameters: 180222522
[2020-03-29 03:22:51,680 INFO] Start training...
[2020-03-29 03:22:51,872 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 03:24:21,402 INFO] Step 50/210000; acc:   0.00; ppl: 13502.64; xent: 9.51; lr: 0.00000004;   0/767 tok/s;     90 sec
[2020-03-29 03:25:30,787 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-29 03:25:51,436 INFO] Step 100/210000; acc:   0.60; ppl: 1810.48; xent: 7.50; lr: 0.00000007;   0/610 tok/s;    180 sec
[2020-03-29 03:27:19,892 INFO] Step 150/210000; acc:   1.56; ppl: 1059.72; xent: 6.97; lr: 0.00000011;   0/675 tok/s;    268 sec
[2020-03-29 03:28:11,398 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-29 03:28:50,666 INFO] Step 200/210000; acc:   2.42; ppl: 736.11; xent: 6.60; lr: 0.00000014;   0/874 tok/s;    359 sec
[2020-03-29 03:30:20,272 INFO] Step 250/210000; acc:   2.81; ppl: 703.52; xent: 6.56; lr: 0.00000018;   0/789 tok/s;    448 sec
[2020-03-29 03:30:51,333 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-29 03:31:50,032 INFO] Step 300/210000; acc:   3.94; ppl: 548.04; xent: 6.31; lr: 0.00000021;   0/1072 tok/s;    538 sec
[2020-03-29 03:33:20,204 INFO] Step 350/210000; acc:   6.35; ppl: 475.05; xent: 6.16; lr: 0.00000025;   0/1231 tok/s;    628 sec
[2020-03-29 03:33:31,533 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-29 03:34:51,363 INFO] Step 400/210000; acc:   6.98; ppl: 509.77; xent: 6.23; lr: 0.00000028;   0/570 tok/s;    719 sec
[2020-03-29 03:36:12,796 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-29 03:36:22,289 INFO] Step 450/210000; acc:   9.42; ppl: 310.28; xent: 5.74; lr: 0.00000032;   0/727 tok/s;    810 sec
[2020-03-29 03:37:51,979 INFO] Step 500/210000; acc:  10.24; ppl: 306.58; xent: 5.73; lr: 0.00000035;   0/774 tok/s;    900 sec
[2020-03-29 03:37:51,983 INFO] Saving checkpoint ../models/model_step_500.pt
[2020-03-29 03:38:56,691 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-29 03:39:25,728 INFO] Step 550/210000; acc:  12.15; ppl: 258.76; xent: 5.56; lr: 0.00000039;   0/973 tok/s;    994 sec
[2020-03-29 03:40:55,802 INFO] Step 600/210000; acc:  11.67; ppl: 257.47; xent: 5.55; lr: 0.00000042;   0/1016 tok/s;   1084 sec
[2020-03-29 03:41:37,420 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-29 03:42:26,748 INFO] Step 650/210000; acc:  12.41; ppl: 237.43; xent: 5.47; lr: 0.00000046;   0/589 tok/s;   1175 sec
[2020-03-29 03:43:56,790 INFO] Step 700/210000; acc:  12.52; ppl: 214.33; xent: 5.37; lr: 0.00000049;   0/601 tok/s;   1265 sec
[2020-03-29 03:44:18,108 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-29 03:45:26,880 INFO] Step 750/210000; acc:  13.66; ppl: 214.64; xent: 5.37; lr: 0.00000053;   0/727 tok/s;   1355 sec
[2020-03-29 03:46:56,791 INFO] Step 800/210000; acc:  12.97; ppl: 192.87; xent: 5.26; lr: 0.00000057;   0/795 tok/s;   1445 sec
[2020-03-29 04:02:48,459 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=80, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 04:02:48,460 INFO] Device ID 0
[2020-03-29 04:02:48,460 INFO] Device cuda
[2020-03-29 04:02:52,837 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 04:02:52,837 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 04:02:52,837 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 04:02:52,837 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 04:02:52,842 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 04:02:52,843 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 04:02:52,859 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 04:02:52,859 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 04:02:53,110 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 04:02:53,111 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 04:02:53,121 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 04:02:53,143 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 04:03:00,254 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 04:03:00,397 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 04:03:00,602 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 04:03:00,671 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 04:03:00,758 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 04:03:00,792 INFO] * number of parameters: 180222522
[2020-03-29 04:03:00,792 INFO] Start training...
[2020-03-29 04:03:00,977 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 04:04:27,554 INFO] Step 50/210000; acc:   0.09; ppl: 14489.29; xent: 9.58; lr: 0.00000004;   0/722 tok/s;     87 sec
[2020-03-29 04:05:42,045 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-29 04:05:54,249 INFO] Step 100/210000; acc:   0.61; ppl: 1501.54; xent: 7.31; lr: 0.00000007;   0/891 tok/s;    173 sec
[2020-03-29 04:07:20,635 INFO] Step 150/210000; acc:   1.57; ppl: 1184.80; xent: 7.08; lr: 0.00000011;   0/684 tok/s;    260 sec
[2020-03-29 04:08:23,347 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-29 04:08:47,940 INFO] Step 200/210000; acc:   1.78; ppl: 952.17; xent: 6.86; lr: 0.00000014;   0/864 tok/s;    347 sec
[2020-03-29 04:10:14,798 INFO] Step 250/210000; acc:   2.94; ppl: 806.16; xent: 6.69; lr: 0.00000018;   0/548 tok/s;    434 sec
[2020-03-29 04:11:06,599 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-29 04:11:41,667 INFO] Step 300/210000; acc:   5.26; ppl: 683.73; xent: 6.53; lr: 0.00000021;   0/686 tok/s;    521 sec
[2020-03-29 04:13:08,537 INFO] Step 350/210000; acc:   6.13; ppl: 513.55; xent: 6.24; lr: 0.00000025;   0/1168 tok/s;    608 sec
[2020-03-29 04:13:50,690 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-29 04:14:36,159 INFO] Step 400/210000; acc:   7.51; ppl: 484.65; xent: 6.18; lr: 0.00000028;   0/666 tok/s;    695 sec
[2020-03-29 04:16:03,255 INFO] Step 450/210000; acc:   9.75; ppl: 326.25; xent: 5.79; lr: 0.00000032;   0/894 tok/s;    782 sec
[2020-03-29 04:16:35,740 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-29 04:17:32,057 INFO] Step 500/210000; acc:  12.32; ppl: 372.84; xent: 5.92; lr: 0.00000035;   0/521 tok/s;    871 sec
[2020-03-29 04:17:32,061 INFO] Saving checkpoint ../models/model_step_500.pt
[2020-03-29 04:18:58,823 INFO] Step 550/210000; acc:  11.01; ppl: 277.67; xent: 5.63; lr: 0.00000039;   0/771 tok/s;    958 sec
[2020-03-29 04:19:18,533 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-29 04:20:27,425 INFO] Step 600/210000; acc:  11.40; ppl: 242.49; xent: 5.49; lr: 0.00000042;   0/1035 tok/s;   1046 sec
[2020-03-29 04:21:54,844 INFO] Step 650/210000; acc:  12.19; ppl: 248.18; xent: 5.51; lr: 0.00000046;   0/743 tok/s;   1134 sec
[2020-03-29 04:22:03,840 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-29 04:23:21,846 INFO] Step 700/210000; acc:  13.88; ppl: 229.21; xent: 5.43; lr: 0.00000049;   0/817 tok/s;   1221 sec
[2020-03-29 04:24:47,657 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-29 04:24:49,958 INFO] Step 750/210000; acc:  12.21; ppl: 214.10; xent: 5.37; lr: 0.00000053;   0/459 tok/s;   1309 sec
[2020-03-29 04:26:16,552 INFO] Step 800/210000; acc:  14.07; ppl: 178.85; xent: 5.19; lr: 0.00000057;   0/721 tok/s;   1396 sec
[2020-03-29 04:27:31,415 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-29 04:27:43,893 INFO] Step 850/210000; acc:  14.83; ppl: 193.50; xent: 5.27; lr: 0.00000060;   0/942 tok/s;   1483 sec
[2020-03-29 04:29:10,941 INFO] Step 900/210000; acc:  17.32; ppl: 126.41; xent: 4.84; lr: 0.00000064;   0/625 tok/s;   1570 sec
[2020-03-29 04:30:15,517 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-29 04:30:38,220 INFO] Step 950/210000; acc:  17.95; ppl: 147.89; xent: 5.00; lr: 0.00000067;   0/747 tok/s;   1657 sec
[2020-03-29 04:32:05,635 INFO] Step 1000/210000; acc:  14.73; ppl: 162.54; xent: 5.09; lr: 0.00000071;   0/579 tok/s;   1745 sec
[2020-03-29 04:32:05,639 INFO] Saving checkpoint ../models/model_step_1000.pt
[2020-03-29 04:33:02,002 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-29 04:33:35,655 INFO] Step 1050/210000; acc:  15.30; ppl: 135.75; xent: 4.91; lr: 0.00000074;   0/689 tok/s;   1835 sec
[2020-03-29 04:35:02,124 INFO] Step 1100/210000; acc:  15.23; ppl: 158.12; xent: 5.06; lr: 0.00000078;   0/989 tok/s;   1921 sec
[2020-03-29 04:35:46,714 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-29 04:36:30,717 INFO] Step 1150/210000; acc:  16.79; ppl: 153.05; xent: 5.03; lr: 0.00000081;   0/521 tok/s;   2010 sec
[2020-03-29 04:37:57,603 INFO] Step 1200/210000; acc:  19.54; ppl: 108.89; xent: 4.69; lr: 0.00000085;   0/704 tok/s;   2097 sec
[2020-03-29 04:38:30,981 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-29 04:39:24,934 INFO] Step 1250/210000; acc:  15.16; ppl: 138.46; xent: 4.93; lr: 0.00000088;   0/1246 tok/s;   2184 sec
[2020-03-29 04:40:52,795 INFO] Step 1300/210000; acc:  15.22; ppl: 133.53; xent: 4.89; lr: 0.00000092;   0/677 tok/s;   2272 sec
[2020-03-29 04:41:16,230 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-29 04:42:20,712 INFO] Step 1350/210000; acc:  19.63; ppl: 111.79; xent: 4.72; lr: 0.00000095;   0/780 tok/s;   2360 sec
[2020-03-29 05:32:08,756 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=100, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 05:32:08,756 INFO] Device ID 0
[2020-03-29 05:32:08,756 INFO] Device cuda
[2020-03-29 05:32:12,077 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 05:32:12,078 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 05:32:12,079 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 05:32:12,079 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 05:32:12,085 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 05:32:12,085 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 05:32:12,097 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 05:32:12,097 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 05:32:12,348 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 05:32:12,355 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 05:32:12,364 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 05:32:12,386 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 05:32:19,508 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 05:32:19,811 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 05:32:19,841 INFO] * number of parameters: 180222522
[2020-03-29 05:32:19,841 INFO] Start training...
[2020-03-29 05:32:19,970 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 05:32:20,019 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 05:32:20,019 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 05:32:20,057 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 05:33:52,679 INFO] Step 50/210000; acc:   0.00; ppl: 15151.81; xent: 9.63; lr: 0.00000004;   0/917 tok/s;     93 sec
[2020-03-29 05:37:19,928 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=80, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 05:37:19,929 INFO] Device ID 0
[2020-03-29 05:37:19,929 INFO] Device cuda
[2020-03-29 05:37:24,199 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 05:37:24,200 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 05:37:24,200 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 05:37:24,201 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 05:37:24,210 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 05:37:24,210 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 05:37:24,217 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 05:37:24,218 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 05:37:24,470 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 05:37:24,472 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 05:37:24,483 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 05:37:24,513 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 05:37:31,232 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 05:37:31,544 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 05:37:31,575 INFO] * number of parameters: 180222522
[2020-03-29 05:37:31,575 INFO] Start training...
[2020-03-29 05:37:31,761 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 05:37:31,764 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 05:37:31,785 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 05:37:32,042 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 05:38:58,996 INFO] Step 50/210000; acc:   0.09; ppl: 14489.29; xent: 9.58; lr: 0.00000004;   0/718 tok/s;     87 sec
[2020-03-29 05:40:13,531 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-29 05:40:25,871 INFO] Step 100/210000; acc:   0.61; ppl: 1501.54; xent: 7.31; lr: 0.00000007;   0/874 tok/s;    174 sec
[2020-03-29 05:41:52,515 INFO] Step 150/210000; acc:   1.57; ppl: 1184.80; xent: 7.08; lr: 0.00000011;   0/674 tok/s;    261 sec
[2020-03-29 05:42:55,366 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-29 05:43:19,942 INFO] Step 200/210000; acc:   1.78; ppl: 952.17; xent: 6.86; lr: 0.00000014;   0/869 tok/s;    348 sec
[2020-03-29 05:44:46,823 INFO] Step 250/210000; acc:   2.94; ppl: 806.16; xent: 6.69; lr: 0.00000018;   0/556 tok/s;    435 sec
[2020-03-29 05:45:38,789 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-29 05:46:13,753 INFO] Step 300/210000; acc:   5.26; ppl: 683.73; xent: 6.53; lr: 0.00000021;   0/697 tok/s;    522 sec
[2020-03-29 05:47:40,843 INFO] Step 350/210000; acc:   6.13; ppl: 513.55; xent: 6.24; lr: 0.00000025;   0/1168 tok/s;    609 sec
[2020-03-29 05:48:23,113 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-29 05:49:08,717 INFO] Step 400/210000; acc:   7.51; ppl: 484.65; xent: 6.18; lr: 0.00000028;   0/669 tok/s;    697 sec
[2020-03-29 05:50:35,766 INFO] Step 450/210000; acc:   9.75; ppl: 326.25; xent: 5.79; lr: 0.00000032;   0/896 tok/s;    784 sec
[2020-03-29 05:51:08,195 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-29 05:52:04,623 INFO] Step 500/210000; acc:  12.32; ppl: 372.84; xent: 5.92; lr: 0.00000035;   0/521 tok/s;    873 sec
[2020-03-29 05:52:04,626 INFO] Saving checkpoint ../models/model_step_500.pt
[2020-03-29 05:53:31,387 INFO] Step 550/210000; acc:  11.01; ppl: 277.67; xent: 5.63; lr: 0.00000039;   0/771 tok/s;    960 sec
[2020-03-29 05:53:51,119 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-29 05:55:00,079 INFO] Step 600/210000; acc:  11.40; ppl: 242.49; xent: 5.49; lr: 0.00000042;   0/1043 tok/s;   1048 sec
[2020-03-29 05:56:27,295 INFO] Step 650/210000; acc:  12.19; ppl: 248.18; xent: 5.51; lr: 0.00000046;   0/738 tok/s;   1136 sec
[2020-03-29 05:56:36,225 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-29 05:57:54,211 INFO] Step 700/210000; acc:  13.88; ppl: 229.21; xent: 5.43; lr: 0.00000049;   0/825 tok/s;   1222 sec
[2020-03-29 05:59:20,329 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-29 05:59:22,681 INFO] Step 750/210000; acc:  12.21; ppl: 214.10; xent: 5.37; lr: 0.00000053;   0/454 tok/s;   1311 sec
[2020-03-29 06:00:49,330 INFO] Step 800/210000; acc:  14.07; ppl: 178.85; xent: 5.19; lr: 0.00000057;   0/729 tok/s;   1398 sec
[2020-03-29 06:02:04,377 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-29 06:02:16,826 INFO] Step 850/210000; acc:  14.83; ppl: 193.50; xent: 5.27; lr: 0.00000060;   0/947 tok/s;   1485 sec
[2020-03-29 06:03:44,029 INFO] Step 900/210000; acc:  17.32; ppl: 126.41; xent: 4.84; lr: 0.00000064;   0/631 tok/s;   1572 sec
[2020-03-29 06:04:48,112 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-29 06:05:10,772 INFO] Step 950/210000; acc:  17.95; ppl: 147.89; xent: 5.00; lr: 0.00000067;   0/749 tok/s;   1659 sec
[2020-03-29 06:06:37,984 INFO] Step 1000/210000; acc:  14.73; ppl: 162.54; xent: 5.09; lr: 0.00000071;   0/582 tok/s;   1746 sec
[2020-03-29 06:06:37,987 INFO] Saving checkpoint ../models/model_step_1000.pt
[2020-03-29 06:07:31,805 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-29 06:08:05,490 INFO] Step 1050/210000; acc:  15.30; ppl: 135.75; xent: 4.91; lr: 0.00000074;   0/672 tok/s;   1834 sec
[2020-03-29 06:09:31,824 INFO] Step 1100/210000; acc:  15.23; ppl: 158.12; xent: 5.06; lr: 0.00000078;   0/991 tok/s;   1920 sec
[2020-03-29 06:10:15,973 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-29 06:10:59,984 INFO] Step 1150/210000; acc:  16.79; ppl: 153.05; xent: 5.03; lr: 0.00000081;   0/526 tok/s;   2008 sec
[2020-03-29 06:12:26,789 INFO] Step 1200/210000; acc:  19.54; ppl: 108.89; xent: 4.69; lr: 0.00000085;   0/712 tok/s;   2095 sec
[2020-03-29 06:12:59,849 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-29 06:13:53,650 INFO] Step 1250/210000; acc:  15.16; ppl: 138.46; xent: 4.93; lr: 0.00000088;   0/1239 tok/s;   2182 sec
[2020-03-29 06:15:21,438 INFO] Step 1300/210000; acc:  15.22; ppl: 133.53; xent: 4.89; lr: 0.00000092;   0/666 tok/s;   2270 sec
[2020-03-29 06:15:44,608 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-29 06:16:48,958 INFO] Step 1350/210000; acc:  19.63; ppl: 111.79; xent: 4.72; lr: 0.00000095;   0/778 tok/s;   2357 sec
[2020-03-29 06:18:19,564 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=140, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=1)
[2020-03-29 06:18:19,564 INFO] Device ID 0
[2020-03-29 06:18:19,564 INFO] Device cuda
[2020-03-29 06:18:24,650 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:18:24,650 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:18:24,928 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:18:31,267 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 06:18:31,539 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 06:18:31,567 INFO] * number of parameters: 180222522
[2020-03-29 06:18:31,567 INFO] Start training...
[2020-03-29 06:18:31,754 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 06:24:16,867 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=140, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=1)
[2020-03-29 06:24:16,870 INFO] Device ID 0
[2020-03-29 06:24:16,870 INFO] Device cuda
[2020-03-29 06:24:22,009 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:24:22,012 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:24:22,310 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:24:33,002 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 06:24:33,346 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 06:24:33,388 INFO] * number of parameters: 180222522
[2020-03-29 06:24:33,389 INFO] Start training...
[2020-03-29 06:24:33,848 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 06:25:08,514 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=140, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 06:25:08,514 INFO] Device ID 0
[2020-03-29 06:25:08,514 INFO] Device cuda
[2020-03-29 06:26:24,989 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=140, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 06:26:24,990 INFO] Device ID 0
[2020-03-29 06:26:24,990 INFO] Device cuda
[2020-03-29 06:26:27,173 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:26:27,174 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:26:27,181 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:26:27,181 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:26:27,181 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:26:27,182 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:26:27,182 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:26:27,182 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:26:27,444 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:26:27,448 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:26:27,451 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:26:27,490 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:27:11,951 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=100, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 06:27:11,951 INFO] Device ID 0
[2020-03-29 06:27:11,951 INFO] Device cuda
[2020-03-29 06:27:14,250 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:27:14,251 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:27:14,255 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:27:14,255 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:27:14,268 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:27:14,269 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:27:14,276 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:27:14,277 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:27:14,523 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:27:14,547 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:27:14,555 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:27:14,575 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:27:55,243 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=80, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 06:27:55,244 INFO] Device ID 0
[2020-03-29 06:27:55,244 INFO] Device cuda
[2020-03-29 06:27:57,279 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:27:57,280 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:27:57,289 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:27:57,290 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:27:57,291 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:27:57,292 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:27:57,292 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:27:57,292 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:27:57,551 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:27:57,557 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:27:57,587 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:27:57,635 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:30:23,358 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=80, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 06:30:23,361 INFO] Device ID 0
[2020-03-29 06:30:23,361 INFO] Device cuda
[2020-03-29 06:30:27,419 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:30:27,420 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:30:27,422 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:30:27,422 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:30:27,425 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:30:27,426 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:30:27,441 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 06:30:27,442 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 06:30:27,701 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:30:27,735 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:30:27,754 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:30:27,756 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 06:30:46,255 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 06:30:46,580 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 06:30:46,625 INFO] * number of parameters: 180222522
[2020-03-29 06:30:46,625 INFO] Start training...
[2020-03-29 06:30:46,636 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 06:30:46,705 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 06:30:47,186 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 06:30:47,253 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 06:32:14,137 INFO] Step 50/210000; acc:   0.09; ppl: 14489.29; xent: 9.58; lr: 0.00000004;   0/719 tok/s;     87 sec
[2020-03-29 06:33:28,856 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-29 06:33:40,999 INFO] Step 100/210000; acc:   0.61; ppl: 1501.54; xent: 7.31; lr: 0.00000007;   0/886 tok/s;    174 sec
[2020-03-29 06:35:07,379 INFO] Step 150/210000; acc:   1.57; ppl: 1184.80; xent: 7.08; lr: 0.00000011;   0/679 tok/s;    260 sec
[2020-03-29 06:36:10,392 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-29 06:36:34,924 INFO] Step 200/210000; acc:   1.78; ppl: 952.17; xent: 6.86; lr: 0.00000014;   0/861 tok/s;    348 sec
[2020-03-29 06:38:01,767 INFO] Step 250/210000; acc:   2.94; ppl: 806.16; xent: 6.69; lr: 0.00000018;   0/552 tok/s;    435 sec
[2020-03-29 06:38:54,038 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-29 06:39:28,980 INFO] Step 300/210000; acc:   5.26; ppl: 683.73; xent: 6.53; lr: 0.00000021;   0/698 tok/s;    522 sec
[2020-03-29 06:40:55,791 INFO] Step 350/210000; acc:   6.13; ppl: 513.55; xent: 6.24; lr: 0.00000025;   0/1183 tok/s;    609 sec
[2020-03-29 06:41:38,181 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-29 06:42:23,682 INFO] Step 400/210000; acc:   7.51; ppl: 484.65; xent: 6.18; lr: 0.00000028;   0/667 tok/s;    696 sec
[2020-03-29 06:43:50,535 INFO] Step 450/210000; acc:   9.75; ppl: 326.25; xent: 5.79; lr: 0.00000032;   0/907 tok/s;    783 sec
[2020-03-29 06:44:23,138 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-29 06:45:19,644 INFO] Step 500/210000; acc:  12.32; ppl: 372.84; xent: 5.92; lr: 0.00000035;   0/516 tok/s;    872 sec
[2020-03-29 06:45:19,649 INFO] Saving checkpoint ../models/model_step_500.pt
[2020-03-29 06:46:46,432 INFO] Step 550/210000; acc:  11.01; ppl: 277.67; xent: 5.63; lr: 0.00000039;   0/775 tok/s;    959 sec
[2020-03-29 06:47:06,534 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-29 06:48:15,384 INFO] Step 600/210000; acc:  11.40; ppl: 242.49; xent: 5.49; lr: 0.00000042;   0/1044 tok/s;   1048 sec
[2020-03-29 06:49:42,725 INFO] Step 650/210000; acc:  12.19; ppl: 248.18; xent: 5.51; lr: 0.00000046;   0/744 tok/s;   1136 sec
[2020-03-29 06:49:52,105 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-29 06:51:09,862 INFO] Step 700/210000; acc:  13.88; ppl: 229.21; xent: 5.43; lr: 0.00000049;   0/833 tok/s;   1223 sec
[2020-03-29 06:52:36,080 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-29 06:52:38,396 INFO] Step 750/210000; acc:  12.21; ppl: 214.10; xent: 5.37; lr: 0.00000053;   0/396 tok/s;   1311 sec
[2020-03-29 06:54:04,720 INFO] Step 800/210000; acc:  14.07; ppl: 178.85; xent: 5.19; lr: 0.00000057;   0/723 tok/s;   1398 sec
[2020-03-29 06:55:19,897 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-29 06:55:32,329 INFO] Step 850/210000; acc:  14.83; ppl: 193.50; xent: 5.27; lr: 0.00000060;   0/948 tok/s;   1485 sec
[2020-03-29 06:56:59,405 INFO] Step 900/210000; acc:  17.32; ppl: 126.41; xent: 4.84; lr: 0.00000064;   0/633 tok/s;   1572 sec
[2020-03-29 06:58:03,834 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-29 06:58:26,413 INFO] Step 950/210000; acc:  17.95; ppl: 147.89; xent: 5.00; lr: 0.00000067;   0/753 tok/s;   1659 sec
[2020-03-29 06:59:53,826 INFO] Step 1000/210000; acc:  14.73; ppl: 162.54; xent: 5.09; lr: 0.00000071;   0/585 tok/s;   1747 sec
[2020-03-29 06:59:53,829 INFO] Saving checkpoint ../models/model_step_1000.pt
[2020-03-29 07:00:48,148 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-29 07:01:21,827 INFO] Step 1050/210000; acc:  15.30; ppl: 135.75; xent: 4.91; lr: 0.00000074;   0/679 tok/s;   1835 sec
[2020-03-29 07:02:48,453 INFO] Step 1100/210000; acc:  15.23; ppl: 158.12; xent: 5.06; lr: 0.00000078;   0/920 tok/s;   1921 sec
[2020-03-29 07:03:33,105 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-29 07:04:17,288 INFO] Step 1150/210000; acc:  16.79; ppl: 153.05; xent: 5.03; lr: 0.00000081;   0/522 tok/s;   2010 sec
[2020-03-29 07:05:44,195 INFO] Step 1200/210000; acc:  19.54; ppl: 108.89; xent: 4.69; lr: 0.00000085;   0/707 tok/s;   2097 sec
[2020-03-29 07:06:17,644 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-29 07:07:11,628 INFO] Step 1250/210000; acc:  15.16; ppl: 138.46; xent: 4.93; lr: 0.00000088;   0/1243 tok/s;   2184 sec
[2020-03-29 07:08:39,513 INFO] Step 1300/210000; acc:  15.22; ppl: 133.53; xent: 4.89; lr: 0.00000092;   0/672 tok/s;   2272 sec
[2020-03-29 07:09:02,972 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-29 07:10:07,480 INFO] Step 1350/210000; acc:  19.63; ppl: 111.79; xent: 4.72; lr: 0.00000095;   0/773 tok/s;   2360 sec
[2020-03-29 07:12:15,647 INFO] Namespace(accum_count=2, alpha=0.6, batch_size=80, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=4)
[2020-03-29 07:12:15,647 INFO] Device ID 0
[2020-03-29 07:12:15,647 INFO] Device cuda
[2020-03-29 07:12:18,694 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 07:12:18,695 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 07:12:18,695 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 07:12:18,695 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 07:12:18,704 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 07:12:18,704 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 07:12:18,705 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 07:12:18,706 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 07:12:18,959 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 07:12:18,972 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 07:12:18,974 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 07:12:18,995 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 07:12:26,351 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 07:12:26,537 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 07:12:26,639 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 07:12:26,641 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 07:12:26,671 INFO] * number of parameters: 180222522
[2020-03-29 07:12:26,671 INFO] Start training...
[2020-03-29 07:12:26,851 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 07:12:26,912 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 07:13:20,677 INFO] Step 50/210000; acc:   0.00; ppl: 19601.20; xent: 9.88; lr: 0.00000004;   0/512 tok/s;     54 sec
[2020-03-29 07:14:14,441 INFO] Step 100/210000; acc:   1.10; ppl: 2343.71; xent: 7.76; lr: 0.00000007;   0/359 tok/s;    108 sec
[2020-03-29 07:15:07,184 INFO] Step 150/210000; acc:   0.54; ppl: 1105.64; xent: 7.01; lr: 0.00000011;   0/526 tok/s;    160 sec
[2020-03-29 07:16:00,768 INFO] Step 200/210000; acc:   1.22; ppl: 920.39; xent: 6.82; lr: 0.00000014;   0/404 tok/s;    214 sec
[2020-03-29 07:16:38,016 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-29 07:16:55,250 INFO] Step 250/210000; acc:   2.29; ppl: 633.75; xent: 6.45; lr: 0.00000018;   0/587 tok/s;    268 sec
[2020-03-29 07:17:48,706 INFO] Step 300/210000; acc:   4.13; ppl: 610.92; xent: 6.41; lr: 0.00000021;   0/446 tok/s;    322 sec
[2020-03-29 07:18:42,649 INFO] Step 350/210000; acc:   4.86; ppl: 675.35; xent: 6.52; lr: 0.00000025;   0/405 tok/s;    376 sec
[2020-03-29 07:19:35,371 INFO] Step 400/210000; acc:   7.23; ppl: 536.38; xent: 6.28; lr: 0.00000028;   0/543 tok/s;    429 sec
[2020-03-29 07:20:29,749 INFO] Step 450/210000; acc:   7.31; ppl: 409.83; xent: 6.02; lr: 0.00000032;   0/412 tok/s;    483 sec
[2020-03-29 07:20:47,639 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-29 07:21:24,144 INFO] Step 500/210000; acc:   9.74; ppl: 413.92; xent: 6.03; lr: 0.00000035;   0/600 tok/s;    537 sec
[2020-03-29 07:21:24,171 INFO] Saving checkpoint ../models/model_step_500.pt
[2020-03-29 07:22:17,969 INFO] Step 550/210000; acc:  10.67; ppl: 278.78; xent: 5.63; lr: 0.00000039;   0/416 tok/s;    591 sec
[2020-03-29 07:23:10,985 INFO] Step 600/210000; acc:  10.78; ppl: 295.58; xent: 5.69; lr: 0.00000042;   0/736 tok/s;    644 sec
[2020-03-29 07:24:04,574 INFO] Step 650/210000; acc:  10.54; ppl: 299.19; xent: 5.70; lr: 0.00000046;   0/468 tok/s;    698 sec
[2020-03-29 07:24:57,773 INFO] Step 700/210000; acc:  14.72; ppl: 173.41; xent: 5.16; lr: 0.00000049;   0/552 tok/s;    751 sec
[2020-03-29 07:24:59,742 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-29 07:25:52,377 INFO] Step 750/210000; acc:  10.88; ppl: 245.23; xent: 5.50; lr: 0.00000053;   0/465 tok/s;    806 sec
[2020-03-29 07:26:46,316 INFO] Step 800/210000; acc:   8.47; ppl: 338.88; xent: 5.83; lr: 0.00000057;   0/343 tok/s;    859 sec
[2020-03-29 07:27:39,347 INFO] Step 850/210000; acc:  11.98; ppl: 246.17; xent: 5.51; lr: 0.00000060;   0/530 tok/s;    912 sec
[2020-03-29 07:28:33,171 INFO] Step 900/210000; acc:  11.59; ppl: 203.54; xent: 5.32; lr: 0.00000064;   0/367 tok/s;    966 sec
[2020-03-29 07:29:10,492 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-29 07:29:26,788 INFO] Step 950/210000; acc:  14.38; ppl: 190.68; xent: 5.25; lr: 0.00000067;   0/606 tok/s;   1020 sec
[2020-03-29 07:30:20,166 INFO] Step 1000/210000; acc:  12.05; ppl: 239.61; xent: 5.48; lr: 0.00000071;   0/436 tok/s;   1073 sec
[2020-03-29 07:30:20,169 INFO] Saving checkpoint ../models/model_step_1000.pt
[2020-03-29 07:31:13,063 INFO] Step 1050/210000; acc:  14.22; ppl: 190.77; xent: 5.25; lr: 0.00000074;   0/782 tok/s;   1126 sec
[2020-03-29 07:32:06,963 INFO] Step 1100/210000; acc:  11.34; ppl: 206.56; xent: 5.33; lr: 0.00000078;   0/467 tok/s;   1180 sec
[2020-03-29 07:33:01,065 INFO] Step 1150/210000; acc:  12.50; ppl: 212.49; xent: 5.36; lr: 0.00000081;   0/315 tok/s;   1234 sec
[2020-03-29 07:33:22,944 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-29 07:33:55,447 INFO] Step 1200/210000; acc:  15.81; ppl: 170.91; xent: 5.14; lr: 0.00000085;   0/492 tok/s;   1289 sec
[2020-03-29 07:34:49,291 INFO] Step 1250/210000; acc:  14.89; ppl: 231.46; xent: 5.44; lr: 0.00000088;   0/344 tok/s;   1342 sec
[2020-03-29 07:35:42,493 INFO] Step 1300/210000; acc:  19.55; ppl: 123.27; xent: 4.81; lr: 0.00000092;   0/556 tok/s;   1396 sec
[2020-03-29 07:36:36,469 INFO] Step 1350/210000; acc:  13.25; ppl: 190.91; xent: 5.25; lr: 0.00000095;   0/367 tok/s;   1450 sec
[2020-03-29 07:37:29,934 INFO] Step 1400/210000; acc:  15.46; ppl: 133.70; xent: 4.90; lr: 0.00000099;   0/866 tok/s;   1503 sec
[2020-03-29 07:37:34,610 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-29 07:38:24,585 INFO] Step 1450/210000; acc:  13.84; ppl: 151.56; xent: 5.02; lr: 0.00000103;   0/436 tok/s;   1558 sec
[2020-03-29 07:39:18,382 INFO] Step 1500/210000; acc:  11.27; ppl: 202.11; xent: 5.31; lr: 0.00000106;   0/527 tok/s;   1612 sec
[2020-03-29 07:39:18,385 INFO] Saving checkpoint ../models/model_step_1500.pt
[2020-03-29 07:40:14,360 INFO] Step 1550/210000; acc:  14.67; ppl: 202.87; xent: 5.31; lr: 0.00000110;   0/500 tok/s;   1668 sec
[2020-03-29 07:41:08,058 INFO] Step 1600/210000; acc:  19.02; ppl: 108.43; xent: 4.69; lr: 0.00000113;   0/378 tok/s;   1721 sec
[2020-03-29 07:41:47,771 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-29 07:42:01,857 INFO] Step 1650/210000; acc:  19.55; ppl: 113.34; xent: 4.73; lr: 0.00000117;   0/517 tok/s;   1775 sec
[2020-03-29 07:42:55,663 INFO] Step 1700/210000; acc:  16.43; ppl: 146.33; xent: 4.99; lr: 0.00000120;   0/351 tok/s;   1829 sec
[2020-03-29 07:43:48,543 INFO] Step 1750/210000; acc:  16.52; ppl: 136.03; xent: 4.91; lr: 0.00000124;   0/558 tok/s;   1882 sec
[2020-03-29 07:44:42,609 INFO] Step 1800/210000; acc:  16.53; ppl: 102.73; xent: 4.63; lr: 0.00000127;   0/360 tok/s;   1936 sec
[2020-03-29 07:45:35,942 INFO] Step 1850/210000; acc:  19.02; ppl: 112.86; xent: 4.73; lr: 0.00000131;   0/600 tok/s;   1989 sec
[2020-03-29 07:46:02,261 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-29 07:46:30,310 INFO] Step 1900/210000; acc:  15.85; ppl: 158.29; xent: 5.06; lr: 0.00000134;   0/399 tok/s;   2043 sec
[2020-03-29 07:47:23,622 INFO] Step 1950/210000; acc:  15.80; ppl: 131.80; xent: 4.88; lr: 0.00000138;   0/869 tok/s;   2097 sec
[2020-03-29 07:48:16,869 INFO] Step 2000/210000; acc:  18.00; ppl: 119.71; xent: 4.79; lr: 0.00000141;   0/479 tok/s;   2150 sec
[2020-03-29 07:48:16,873 INFO] Saving checkpoint ../models/model_step_2000.pt
[2020-03-29 07:49:12,747 INFO] Step 2050/210000; acc:  19.44; ppl: 104.97; xent: 4.65; lr: 0.00000145;   0/335 tok/s;   2206 sec
[2020-03-29 07:50:05,649 INFO] Step 2100/210000; acc:  19.28; ppl: 94.51; xent: 4.55; lr: 0.00000148;   0/528 tok/s;   2259 sec
[2020-03-29 07:50:16,323 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-29 07:51:00,376 INFO] Step 2150/210000; acc:  16.90; ppl: 121.13; xent: 4.80; lr: 0.00000152;   0/357 tok/s;   2314 sec
[2020-03-29 07:51:53,651 INFO] Step 2200/210000; acc:  18.27; ppl: 123.24; xent: 4.81; lr: 0.00000156;   0/587 tok/s;   2367 sec
[2020-03-29 07:52:47,190 INFO] Step 2250/210000; acc:  19.24; ppl: 90.43; xent: 4.50; lr: 0.00000159;   0/385 tok/s;   2420 sec
[2020-03-29 07:53:40,540 INFO] Step 2300/210000; acc:  18.93; ppl: 91.07; xent: 4.51; lr: 0.00000163;   0/594 tok/s;   2474 sec
[2020-03-29 07:54:26,463 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-29 07:54:34,331 INFO] Step 2350/210000; acc:  18.51; ppl: 108.44; xent: 4.69; lr: 0.00000166;   0/404 tok/s;   2527 sec
[2020-03-29 07:55:27,454 INFO] Step 2400/210000; acc:  16.71; ppl: 127.93; xent: 4.85; lr: 0.00000170;   0/698 tok/s;   2581 sec
[2020-03-29 07:56:20,922 INFO] Step 2450/210000; acc:  19.03; ppl: 111.14; xent: 4.71; lr: 0.00000173;   0/461 tok/s;   2634 sec
[2020-03-29 07:57:14,748 INFO] Step 2500/210000; acc:  17.42; ppl: 97.63; xent: 4.58; lr: 0.00000177;   0/326 tok/s;   2688 sec
[2020-03-29 07:57:14,751 INFO] Saving checkpoint ../models/model_step_2500.pt
[2020-03-29 07:58:10,068 INFO] Step 2550/210000; acc:  20.49; ppl: 99.53; xent: 4.60; lr: 0.00000180;   0/538 tok/s;   2743 sec
[2020-03-29 07:58:40,139 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-29 07:59:04,450 INFO] Step 2600/210000; acc:  20.67; ppl: 77.41; xent: 4.35; lr: 0.00000184;   0/321 tok/s;   2798 sec
[2020-03-29 07:59:57,454 INFO] Step 2650/210000; acc:  20.33; ppl: 102.45; xent: 4.63; lr: 0.00000187;   0/581 tok/s;   2851 sec
[2020-03-29 08:00:50,932 INFO] Step 2700/210000; acc:  21.52; ppl: 96.55; xent: 4.57; lr: 0.00000191;   0/398 tok/s;   2904 sec
[2020-03-29 08:01:44,009 INFO] Step 2750/210000; acc:  17.38; ppl: 127.56; xent: 4.85; lr: 0.00000194;   0/695 tok/s;   2957 sec
[2020-03-29 08:02:37,918 INFO] Step 2800/210000; acc:  21.96; ppl: 115.06; xent: 4.75; lr: 0.00000198;   0/457 tok/s;   3011 sec
[2020-03-29 08:02:51,966 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-29 08:03:31,557 INFO] Step 2850/210000; acc:  18.27; ppl: 111.58; xent: 4.71; lr: 0.00000202;   0/696 tok/s;   3065 sec
[2020-03-29 08:04:25,257 INFO] Step 2900/210000; acc:  21.33; ppl: 91.56; xent: 4.52; lr: 0.00000205;   0/433 tok/s;   3118 sec
[2020-03-29 08:05:18,599 INFO] Step 2950/210000; acc:  23.57; ppl: 62.40; xent: 4.13; lr: 0.00000209;   0/615 tok/s;   3172 sec
[2020-03-29 08:06:12,015 INFO] Step 3000/210000; acc:  23.14; ppl: 67.39; xent: 4.21; lr: 0.00000212;   0/471 tok/s;   3225 sec
[2020-03-29 08:06:12,018 INFO] Saving checkpoint ../models/model_step_3000.pt
[2020-03-29 08:07:05,814 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-29 08:07:08,541 INFO] Step 3050/210000; acc:  24.38; ppl: 71.93; xent: 4.28; lr: 0.00000216;   0/340 tok/s;   3282 sec
[2020-03-29 08:08:01,106 INFO] Step 3100/210000; acc:  26.31; ppl: 67.37; xent: 4.21; lr: 0.00000219;   0/516 tok/s;   3334 sec
[2020-03-29 08:08:55,413 INFO] Step 3150/210000; acc:  27.07; ppl: 67.73; xent: 4.22; lr: 0.00000223;   0/385 tok/s;   3389 sec
[2020-03-29 08:09:48,735 INFO] Step 3200/210000; acc:  21.49; ppl: 72.28; xent: 4.28; lr: 0.00000226;   0/629 tok/s;   3442 sec
[2020-03-29 08:10:42,482 INFO] Step 3250/210000; acc:  27.55; ppl: 59.47; xent: 4.09; lr: 0.00000230;   0/446 tok/s;   3496 sec
[2020-03-29 08:11:16,743 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-29 08:11:35,932 INFO] Step 3300/210000; acc:  27.96; ppl: 62.84; xent: 4.14; lr: 0.00000233;   0/670 tok/s;   3549 sec
[2020-03-29 08:12:29,762 INFO] Step 3350/210000; acc:  26.09; ppl: 65.38; xent: 4.18; lr: 0.00000237;   0/423 tok/s;   3603 sec
[2020-03-29 16:14:09,938 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=80, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=4)
[2020-03-29 16:14:09,941 INFO] Device ID 0
[2020-03-29 16:14:09,941 INFO] Device cuda
[2020-03-29 16:14:12,884 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 16:14:12,887 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 16:14:12,887 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 16:14:12,888 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 16:14:12,890 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 16:14:12,891 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 16:14:12,900 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 16:14:12,901 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 16:14:13,164 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 16:14:13,172 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 16:14:13,176 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 16:14:13,177 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 16:14:34,946 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 16:14:35,176 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 16:14:35,235 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 16:14:35,264 INFO] * number of parameters: 180222522
[2020-03-29 16:14:35,264 INFO] Start training...
[2020-03-29 16:14:35,359 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 16:14:35,402 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 16:14:35,802 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 16:16:02,664 INFO] Step 50/210000; acc:   0.09; ppl: 6074.81; xent: 8.71; lr: 0.00000014;   0/706 tok/s;     87 sec
[2020-03-29 16:17:17,325 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-29 16:17:29,525 INFO] Step 100/210000; acc:   1.62; ppl: 1228.50; xent: 7.11; lr: 0.00000028;   0/890 tok/s;    174 sec
[2020-03-29 16:18:55,975 INFO] Step 150/210000; acc:   2.04; ppl: 997.95; xent: 6.91; lr: 0.00000042;   0/679 tok/s;    260 sec
[2020-03-29 16:19:59,217 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-29 16:20:23,820 INFO] Step 200/210000; acc:   2.19; ppl: 831.29; xent: 6.72; lr: 0.00000056;   0/855 tok/s;    348 sec
[2020-03-29 16:21:50,528 INFO] Step 250/210000; acc:   4.75; ppl: 740.80; xent: 6.61; lr: 0.00000070;   0/548 tok/s;    435 sec
[2020-03-29 16:22:42,637 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-29 16:23:17,624 INFO] Step 300/210000; acc:   6.84; ppl: 582.47; xent: 6.37; lr: 0.00000084;   0/698 tok/s;    522 sec
[2020-03-29 16:24:44,597 INFO] Step 350/210000; acc:   8.20; ppl: 398.15; xent: 5.99; lr: 0.00000098;   0/1168 tok/s;    609 sec
[2020-03-29 16:25:27,101 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-29 16:26:12,510 INFO] Step 400/210000; acc:  10.48; ppl: 379.38; xent: 5.94; lr: 0.00000112;   0/667 tok/s;    697 sec
[2020-03-29 16:27:39,672 INFO] Step 450/210000; acc:  11.73; ppl: 268.58; xent: 5.59; lr: 0.00000126;   0/889 tok/s;    784 sec
[2020-03-29 16:28:12,385 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-29 16:29:08,702 INFO] Step 500/210000; acc:  12.74; ppl: 311.70; xent: 5.74; lr: 0.00000140;   0/522 tok/s;    873 sec
[2020-03-29 16:29:08,705 INFO] Saving checkpoint ../models/model_step_500.pt
[2020-03-29 16:30:35,457 INFO] Step 550/210000; acc:  11.87; ppl: 239.57; xent: 5.48; lr: 0.00000154;   0/770 tok/s;    960 sec
[2020-03-29 16:30:55,594 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-29 16:32:04,345 INFO] Step 600/210000; acc:  12.62; ppl: 212.60; xent: 5.36; lr: 0.00000168;   0/1040 tok/s;   1049 sec
[2020-03-29 16:33:31,823 INFO] Step 650/210000; acc:  13.76; ppl: 210.56; xent: 5.35; lr: 0.00000182;   0/743 tok/s;   1136 sec
[2020-03-29 16:33:41,203 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-29 16:34:59,092 INFO] Step 700/210000; acc:  14.52; ppl: 201.13; xent: 5.30; lr: 0.00000196;   0/828 tok/s;   1223 sec
[2020-03-29 16:36:25,180 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-29 16:36:27,495 INFO] Step 750/210000; acc:  12.95; ppl: 183.15; xent: 5.21; lr: 0.00000210;   0/402 tok/s;   1312 sec
[2020-03-29 16:37:54,032 INFO] Step 800/210000; acc:  14.98; ppl: 155.49; xent: 5.05; lr: 0.00000224;   0/732 tok/s;   1398 sec
[2020-03-29 16:39:09,327 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-29 16:39:21,749 INFO] Step 850/210000; acc:  14.89; ppl: 168.82; xent: 5.13; lr: 0.00000238;   0/946 tok/s;   1486 sec
[2020-03-29 16:40:48,770 INFO] Step 900/210000; acc:  19.50; ppl: 108.34; xent: 4.69; lr: 0.00000252;   0/636 tok/s;   1573 sec
[2020-03-29 16:41:53,255 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-29 16:42:15,907 INFO] Step 950/210000; acc:  19.32; ppl: 129.82; xent: 4.87; lr: 0.00000266;   0/738 tok/s;   1660 sec
[2020-03-29 16:43:43,190 INFO] Step 1000/210000; acc:  15.64; ppl: 140.51; xent: 4.95; lr: 0.00000280;   0/586 tok/s;   1747 sec
[2020-03-29 16:43:43,193 INFO] Saving checkpoint ../models/model_step_1000.pt
[2020-03-29 16:44:37,457 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-29 16:45:11,034 INFO] Step 1050/210000; acc:  16.82; ppl: 118.87; xent: 4.78; lr: 0.00000293;   0/676 tok/s;   1835 sec
[2020-03-29 16:46:37,377 INFO] Step 1100/210000; acc:  16.79; ppl: 140.53; xent: 4.95; lr: 0.00000307;   0/986 tok/s;   1922 sec
[2020-03-29 16:47:21,879 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-29 16:48:05,854 INFO] Step 1150/210000; acc:  18.23; ppl: 131.98; xent: 4.88; lr: 0.00000321;   0/529 tok/s;   2010 sec
[2020-03-29 16:49:32,678 INFO] Step 1200/210000; acc:  19.71; ppl: 93.74; xent: 4.54; lr: 0.00000335;   0/703 tok/s;   2097 sec
[2020-03-29 16:50:06,021 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-29 16:51:00,017 INFO] Step 1250/210000; acc:  15.90; ppl: 119.11; xent: 4.78; lr: 0.00000349;   0/1245 tok/s;   2184 sec
[2020-03-29 16:52:27,959 INFO] Step 1300/210000; acc:  16.65; ppl: 107.91; xent: 4.68; lr: 0.00000363;   0/669 tok/s;   2272 sec
[2020-03-29 16:52:51,367 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-29 16:53:55,851 INFO] Step 1350/210000; acc:  19.71; ppl: 94.66; xent: 4.55; lr: 0.00000377;   0/782 tok/s;   2360 sec
[2020-03-29 17:35:36,553 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=80, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=100, use_bert_emb=True, use_interval=True, visible_gpus='-1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)
[2020-03-29 17:35:36,553 INFO] Device ID -1
[2020-03-29 17:35:36,578 INFO] Device cpu
[2020-03-29 17:35:36,857 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 17:35:36,857 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 17:35:37,127 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 17:35:41,914 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 17:35:42,213 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 17:35:42,244 INFO] * number of parameters: 180222522
[2020-03-29 17:35:42,244 INFO] Start training...
[2020-03-29 17:35:42,426 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 17:40:54,764 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=80, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=100, use_bert_emb=True, use_interval=True, visible_gpus='-1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)
[2020-03-29 17:40:54,764 INFO] Device ID -1
[2020-03-29 17:40:54,764 INFO] Device cpu
[2020-03-29 17:40:55,043 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpw7oob7ns
[2020-03-29 17:40:55,323 INFO] copying /tmp/tmpw7oob7ns to cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 17:40:55,324 INFO] creating metadata file for ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 17:40:55,324 INFO] removing temp file /tmp/tmpw7oob7ns
[2020-03-29 17:40:55,324 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 17:40:55,324 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 17:40:55,610 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpk9pt7v8x
[2020-03-29 17:41:05,928 INFO] copying /tmp/tmpk9pt7v8x to cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 17:41:06,381 INFO] creating metadata file for ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 17:41:06,382 INFO] removing temp file /tmp/tmpk9pt7v8x
[2020-03-29 17:41:06,448 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 17:41:11,103 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 17:41:11,398 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp1u0j257l
[2020-03-29 17:41:11,923 INFO] copying /tmp/tmp1u0j257l to cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 17:41:11,924 INFO] creating metadata file for ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 17:41:11,924 INFO] removing temp file /tmp/tmp1u0j257l
[2020-03-29 17:41:11,925 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 17:41:11,954 INFO] * number of parameters: 180222522
[2020-03-29 17:41:11,955 INFO] Start training...
[2020-03-29 17:41:12,136 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 17:45:35,605 INFO] Step 50/  100; acc:   0.00; ppl: 5879.61; xent: 8.68; lr: 0.00000014;   0/ 63 tok/s;    263 sec
[2020-03-29 17:49:44,912 INFO] Step 100/  100; acc:   2.45; ppl: 1334.58; xent: 7.20; lr: 0.00000028;   0/ 51 tok/s;    513 sec
[2020-03-29 17:49:45,226 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 18:36:24,312 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=80, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=4)
[2020-03-29 18:36:24,312 INFO] Device ID 0
[2020-03-29 18:36:24,313 INFO] Device cuda
[2020-03-29 18:36:28,551 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 18:36:28,551 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 18:36:28,552 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 18:36:28,553 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 18:36:28,557 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 18:36:28,558 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 18:36:28,642 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 18:36:28,643 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 18:36:28,821 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 18:36:28,831 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 18:36:28,850 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 18:36:28,911 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 18:36:35,948 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 18:36:35,985 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 18:36:36,270 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 18:36:36,299 INFO] * number of parameters: 180222522
[2020-03-29 18:36:36,299 INFO] Start training...
[2020-03-29 18:36:36,322 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 18:36:36,352 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 18:36:36,480 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 18:38:02,958 INFO] Step 50/210000; acc:   0.09; ppl: 6074.81; xent: 8.71; lr: 0.00000014;   0/714 tok/s;     86 sec
[2020-03-29 18:39:17,470 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-29 18:39:29,706 INFO] Step 100/210000; acc:   1.62; ppl: 1228.50; xent: 7.11; lr: 0.00000028;   0/879 tok/s;    173 sec
[2020-03-29 18:40:56,201 INFO] Step 150/210000; acc:   2.04; ppl: 997.95; xent: 6.91; lr: 0.00000042;   0/674 tok/s;    260 sec
[2020-03-29 18:41:58,913 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-29 18:42:23,507 INFO] Step 200/210000; acc:   2.19; ppl: 831.29; xent: 6.72; lr: 0.00000056;   0/852 tok/s;    347 sec
[2020-03-29 18:43:50,360 INFO] Step 250/210000; acc:   4.75; ppl: 740.80; xent: 6.61; lr: 0.00000070;   0/554 tok/s;    434 sec
[2020-03-29 18:44:42,320 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-29 18:45:17,288 INFO] Step 300/210000; acc:   6.84; ppl: 582.47; xent: 6.37; lr: 0.00000084;   0/692 tok/s;    521 sec
[2020-03-29 18:46:44,244 INFO] Step 350/210000; acc:   8.20; ppl: 398.15; xent: 5.99; lr: 0.00000098;   0/1181 tok/s;    608 sec
[2020-03-29 18:47:26,310 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-29 18:48:11,908 INFO] Step 400/210000; acc:  10.48; ppl: 379.38; xent: 5.94; lr: 0.00000112;   0/671 tok/s;    695 sec
[2020-03-29 18:49:38,876 INFO] Step 450/210000; acc:  11.73; ppl: 268.58; xent: 5.59; lr: 0.00000126;   0/910 tok/s;    782 sec
[2020-03-29 18:50:11,282 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-29 18:51:07,839 INFO] Step 500/210000; acc:  12.74; ppl: 311.70; xent: 5.74; lr: 0.00000140;   0/512 tok/s;    871 sec
[2020-03-29 18:51:07,843 INFO] Saving checkpoint ../models/model_step_500.pt
[2020-03-29 18:52:34,634 INFO] Step 550/210000; acc:  11.87; ppl: 239.57; xent: 5.48; lr: 0.00000154;   0/780 tok/s;    958 sec
[2020-03-29 18:52:54,396 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-29 18:54:03,497 INFO] Step 600/210000; acc:  12.62; ppl: 212.60; xent: 5.36; lr: 0.00000168;   0/1044 tok/s;   1047 sec
[2020-03-29 18:55:30,752 INFO] Step 650/210000; acc:  13.76; ppl: 210.56; xent: 5.35; lr: 0.00000182;   0/749 tok/s;   1134 sec
[2020-03-29 18:55:39,780 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-29 18:56:57,767 INFO] Step 700/210000; acc:  14.52; ppl: 201.13; xent: 5.30; lr: 0.00000196;   0/830 tok/s;   1221 sec
[2020-03-29 18:58:23,522 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-29 18:58:25,811 INFO] Step 750/210000; acc:  12.95; ppl: 183.15; xent: 5.21; lr: 0.00000210;   0/461 tok/s;   1309 sec
[2020-03-29 18:59:52,492 INFO] Step 800/210000; acc:  14.98; ppl: 155.49; xent: 5.05; lr: 0.00000224;   0/721 tok/s;   1396 sec
[2020-03-29 19:01:07,544 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-29 19:01:20,022 INFO] Step 850/210000; acc:  14.89; ppl: 168.82; xent: 5.13; lr: 0.00000238;   0/934 tok/s;   1484 sec
[2020-03-29 19:02:47,437 INFO] Step 900/210000; acc:  19.50; ppl: 108.34; xent: 4.69; lr: 0.00000252;   0/631 tok/s;   1571 sec
[2020-03-29 19:03:51,474 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-29 19:04:14,117 INFO] Step 950/210000; acc:  19.32; ppl: 129.82; xent: 4.87; lr: 0.00000266;   0/754 tok/s;   1658 sec
[2020-03-29 19:05:41,520 INFO] Step 1000/210000; acc:  15.64; ppl: 140.51; xent: 4.95; lr: 0.00000280;   0/576 tok/s;   1745 sec
[2020-03-29 19:05:41,523 INFO] Saving checkpoint ../models/model_step_1000.pt
[2020-03-29 19:06:35,463 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-29 19:07:09,216 INFO] Step 1050/210000; acc:  16.82; ppl: 118.87; xent: 4.78; lr: 0.00000293;   0/684 tok/s;   1833 sec
[2020-03-29 19:08:35,727 INFO] Step 1100/210000; acc:  16.79; ppl: 140.53; xent: 4.95; lr: 0.00000307;   0/974 tok/s;   1919 sec
[2020-03-29 19:09:20,068 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-29 19:10:04,166 INFO] Step 1150/210000; acc:  18.23; ppl: 131.98; xent: 4.88; lr: 0.00000321;   0/524 tok/s;   2008 sec
[2020-03-29 19:11:31,175 INFO] Step 1200/210000; acc:  19.71; ppl: 93.74; xent: 4.54; lr: 0.00000335;   0/705 tok/s;   2095 sec
[2020-03-29 19:12:04,310 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-29 19:12:58,292 INFO] Step 1250/210000; acc:  15.90; ppl: 119.11; xent: 4.78; lr: 0.00000349;   0/1254 tok/s;   2182 sec
[2020-03-29 19:14:26,197 INFO] Step 1300/210000; acc:  16.65; ppl: 107.91; xent: 4.68; lr: 0.00000363;   0/684 tok/s;   2270 sec
[2020-03-29 19:14:49,413 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-29 19:15:54,035 INFO] Step 1350/210000; acc:  19.71; ppl: 94.66; xent: 4.55; lr: 0.00000377;   0/772 tok/s;   2358 sec
[2020-03-29 19:37:20,388 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=140, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=150000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=4)
[2020-03-29 19:37:20,389 INFO] Device ID 0
[2020-03-29 19:37:20,389 INFO] Device cuda
[2020-03-29 19:37:24,728 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 19:37:24,729 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 19:37:24,737 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 19:37:24,737 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 19:37:24,741 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 19:37:24,743 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 19:37:24,853 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 19:37:24,853 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 19:37:25,005 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 19:37:25,016 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 19:37:25,028 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 19:37:25,175 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 19:37:32,118 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 19:37:32,180 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 19:37:32,269 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 19:37:32,468 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 19:37:32,756 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 19:37:32,785 INFO] * number of parameters: 180222522
[2020-03-29 19:37:32,785 INFO] Start training...
[2020-03-29 19:37:32,963 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 19:39:13,708 INFO] Namespace(accum_count=5, alpha=0.6, batch_size=50, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2, 3], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/abs_bert_cnndmwikihow2', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=150, max_pos=512, max_tgt_len=140, min_length=15, mode='train', model_path='../models', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../results/cnndm', save_checkpoint_steps=500, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=200, test_from='', test_start_from=-1, train_from='', train_steps=210000, use_bert_emb=True, use_interval=True, visible_gpus='0,1,2,3', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=4)
[2020-03-29 19:39:13,709 INFO] Device ID 0
[2020-03-29 19:39:13,709 INFO] Device cuda
[2020-03-29 19:39:17,064 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 19:39:17,064 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 19:39:17,066 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 19:39:17,067 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 19:39:17,068 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 19:39:17,068 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 19:39:17,073 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
[2020-03-29 19:39:17,074 INFO] Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2020-03-29 19:39:17,351 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 19:39:17,353 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 19:39:17,357 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 19:39:17,549 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2020-03-29 19:39:24,666 INFO] AbsSummarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (pos_emb): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.2, inplace=False)
          (dropout_2): Dropout(p=0.2, inplace=False)
        )
        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.2, inplace=False)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=30522, bias=True)
    (1): LogSoftmax()
  )
)
[2020-03-29 19:39:24,800 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 19:39:24,935 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 19:39:24,959 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 19:39:24,962 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2020-03-29 19:39:24,993 INFO] * number of parameters: 180222522
[2020-03-29 19:39:24,994 INFO] Start training...
[2020-03-29 19:39:25,178 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-29 19:40:47,949 INFO] Step 50/210000; acc:   0.00; ppl: 5260.76; xent: 8.57; lr: 0.00000014;   0/859 tok/s;     83 sec
[2020-03-29 19:42:10,121 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-29 19:42:11,829 INFO] Step 100/210000; acc:   0.70; ppl: 1477.79; xent: 7.30; lr: 0.00000028;   0/404 tok/s;    167 sec
[2020-03-29 19:43:34,786 INFO] Step 150/210000; acc:   1.69; ppl: 1068.74; xent: 6.97; lr: 0.00000042;   0/468 tok/s;    250 sec
[2020-03-29 19:44:54,757 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-29 19:44:58,127 INFO] Step 200/210000; acc:   2.66; ppl: 740.33; xent: 6.61; lr: 0.00000056;   0/530 tok/s;    333 sec
[2020-03-29 19:46:21,787 INFO] Step 250/210000; acc:   3.93; ppl: 637.85; xent: 6.46; lr: 0.00000070;   0/547 tok/s;    417 sec
[2020-03-29 19:47:42,120 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-29 19:47:45,416 INFO] Step 300/210000; acc:   6.87; ppl: 499.04; xent: 6.21; lr: 0.00000084;   0/596 tok/s;    500 sec
[2020-03-29 19:49:09,431 INFO] Step 350/210000; acc:  10.71; ppl: 382.06; xent: 5.95; lr: 0.00000098;   0/764 tok/s;    584 sec
[2020-03-29 19:50:28,781 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-29 19:50:34,011 INFO] Step 400/210000; acc:   9.72; ppl: 312.54; xent: 5.74; lr: 0.00000112;   0/748 tok/s;    669 sec
[2020-03-29 19:51:57,813 INFO] Step 450/210000; acc:  11.05; ppl: 279.72; xent: 5.63; lr: 0.00000126;   0/642 tok/s;    753 sec
[2020-03-29 19:53:15,989 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-29 19:53:22,694 INFO] Step 500/210000; acc:  10.78; ppl: 238.35; xent: 5.47; lr: 0.00000140;   0/774 tok/s;    838 sec
[2020-03-29 19:53:22,696 INFO] Saving checkpoint ../models/model_step_500.pt
[2020-03-29 19:54:47,376 INFO] Step 550/210000; acc:  13.50; ppl: 233.79; xent: 5.45; lr: 0.00000154;   0/742 tok/s;    922 sec
[2020-03-29 19:56:03,506 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-29 19:56:11,913 INFO] Step 600/210000; acc:  13.10; ppl: 209.35; xent: 5.34; lr: 0.00000168;   0/889 tok/s;   1007 sec
[2020-03-29 19:57:36,002 INFO] Step 650/210000; acc:  14.65; ppl: 212.23; xent: 5.36; lr: 0.00000182;   0/752 tok/s;   1091 sec
[2020-03-29 19:58:52,027 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-29 19:59:00,577 INFO] Step 700/210000; acc:  14.39; ppl: 172.29; xent: 5.15; lr: 0.00000196;   0/1054 tok/s;   1175 sec
[2020-03-29 20:00:24,723 INFO] Step 750/210000; acc:  15.64; ppl: 160.37; xent: 5.08; lr: 0.00000210;   0/987 tok/s;   1260 sec
[2020-03-29 20:01:38,828 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-29 20:01:49,052 INFO] Step 800/210000; acc:  14.07; ppl: 161.72; xent: 5.09; lr: 0.00000224;   0/774 tok/s;   1344 sec
[2020-03-29 20:03:13,298 INFO] Step 850/210000; acc:  16.30; ppl: 162.97; xent: 5.09; lr: 0.00000238;   0/477 tok/s;   1428 sec
[2020-03-29 20:04:27,932 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-29 20:04:38,060 INFO] Step 900/210000; acc:  14.42; ppl: 148.34; xent: 5.00; lr: 0.00000252;   0/507 tok/s;   1513 sec
[2020-03-29 20:06:02,190 INFO] Step 950/210000; acc:  16.92; ppl: 136.18; xent: 4.91; lr: 0.00000266;   0/501 tok/s;   1597 sec
[2020-03-29 20:07:14,953 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-29 20:07:26,661 INFO] Step 1000/210000; acc:  17.14; ppl: 133.21; xent: 4.89; lr: 0.00000280;   0/597 tok/s;   1681 sec
[2020-03-29 20:07:26,664 INFO] Saving checkpoint ../models/model_step_1000.pt
[2020-03-29 20:08:50,709 INFO] Step 1050/210000; acc:  18.22; ppl: 133.69; xent: 4.90; lr: 0.00000293;   0/601 tok/s;   1766 sec
[2020-03-29 20:10:01,474 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-29 20:10:15,083 INFO] Step 1100/210000; acc:  17.02; ppl: 141.42; xent: 4.95; lr: 0.00000307;   0/650 tok/s;   1850 sec
[2020-03-29 20:11:39,066 INFO] Step 1150/210000; acc:  17.92; ppl: 119.76; xent: 4.79; lr: 0.00000321;   0/667 tok/s;   1934 sec
[2020-03-29 20:12:48,489 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-29 20:13:03,696 INFO] Step 1200/210000; acc:  20.71; ppl: 97.37; xent: 4.58; lr: 0.00000335;   0/732 tok/s;   2019 sec
[2020-03-29 20:14:27,739 INFO] Step 1250/210000; acc:  19.46; ppl: 82.49; xent: 4.41; lr: 0.00000349;   0/607 tok/s;   2103 sec
[2020-03-29 20:15:35,612 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-29 20:15:52,680 INFO] Step 1300/210000; acc:  17.76; ppl: 112.48; xent: 4.72; lr: 0.00000363;   0/908 tok/s;   2188 sec
[2020-03-29 20:17:17,096 INFO] Step 1350/210000; acc:  17.41; ppl: 112.32; xent: 4.72; lr: 0.00000377;   0/990 tok/s;   2272 sec
[2020-03-29 20:18:24,966 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-29 20:18:41,676 INFO] Step 1400/210000; acc:  21.12; ppl: 87.02; xent: 4.47; lr: 0.00000391;   0/904 tok/s;   2356 sec
[2020-03-29 20:20:06,084 INFO] Step 1450/210000; acc:  20.77; ppl: 87.69; xent: 4.47; lr: 0.00000405;   0/783 tok/s;   2441 sec
[2020-03-29 20:21:12,522 INFO] Loading train dataset from ../bert_data/cnndm.train.103.bert.pt, number of examples: 2000
[2020-03-29 20:21:31,150 INFO] Step 1500/210000; acc:  19.72; ppl: 93.82; xent: 4.54; lr: 0.00000419;   0/646 tok/s;   2526 sec
[2020-03-29 20:21:31,154 INFO] Saving checkpoint ../models/model_step_1500.pt
[2020-03-29 20:22:55,344 INFO] Step 1550/210000; acc:  22.04; ppl: 79.04; xent: 4.37; lr: 0.00000433;   0/1059 tok/s;   2610 sec
[2020-03-29 20:23:59,828 INFO] Loading train dataset from ../bert_data/cnndm.train.104.bert.pt, number of examples: 2000
[2020-03-29 20:24:20,052 INFO] Step 1600/210000; acc:  22.34; ppl: 75.40; xent: 4.32; lr: 0.00000447;   0/543 tok/s;   2695 sec
[2020-03-29 20:25:44,483 INFO] Step 1650/210000; acc:  24.22; ppl: 62.03; xent: 4.13; lr: 0.00000461;   0/450 tok/s;   2779 sec
[2020-03-29 20:26:47,464 INFO] Loading train dataset from ../bert_data/cnndm.train.105.bert.pt, number of examples: 2001
[2020-03-29 20:27:09,368 INFO] Step 1700/210000; acc:  25.02; ppl: 66.79; xent: 4.20; lr: 0.00000475;   0/678 tok/s;   2864 sec
[2020-03-29 20:28:33,354 INFO] Step 1750/210000; acc:  24.06; ppl: 65.48; xent: 4.18; lr: 0.00000489;   0/660 tok/s;   2948 sec
[2020-03-29 20:29:37,009 INFO] Loading train dataset from ../bert_data/cnndm.train.106.bert.pt, number of examples: 1999
[2020-03-29 20:29:58,953 INFO] Step 1800/210000; acc:  27.11; ppl: 51.39; xent: 3.94; lr: 0.00000503;   0/703 tok/s;   3034 sec
[2020-03-29 20:31:23,151 INFO] Step 1850/210000; acc:  28.56; ppl: 46.16; xent: 3.83; lr: 0.00000517;   0/631 tok/s;   3118 sec
[2020-03-29 20:32:24,794 INFO] Loading train dataset from ../bert_data/cnndm.train.107.bert.pt, number of examples: 1999
[2020-03-29 20:32:48,510 INFO] Step 1900/210000; acc:  27.91; ppl: 52.02; xent: 3.95; lr: 0.00000531;   0/924 tok/s;   3203 sec
[2020-03-29 20:34:12,795 INFO] Step 1950/210000; acc:  31.24; ppl: 45.38; xent: 3.82; lr: 0.00000545;   0/768 tok/s;   3288 sec
[2020-03-29 20:35:13,145 INFO] Loading train dataset from ../bert_data/cnndm.train.108.bert.pt, number of examples: 2000
[2020-03-29 20:35:38,152 INFO] Step 2000/210000; acc:  31.28; ppl: 41.09; xent: 3.72; lr: 0.00000559;   0/774 tok/s;   3373 sec
[2020-03-29 20:35:38,156 INFO] Saving checkpoint ../models/model_step_2000.pt
[2020-03-29 20:37:02,034 INFO] Step 2050/210000; acc:  31.98; ppl: 38.44; xent: 3.65; lr: 0.00000573;   0/713 tok/s;   3457 sec
[2020-03-29 20:37:59,850 INFO] Loading train dataset from ../bert_data/cnndm.train.109.bert.pt, number of examples: 2000
[2020-03-29 20:38:26,900 INFO] Step 2100/210000; acc:  31.43; ppl: 38.86; xent: 3.66; lr: 0.00000587;   0/830 tok/s;   3542 sec
[2020-03-29 20:39:51,066 INFO] Step 2150/210000; acc:  32.86; ppl: 44.60; xent: 3.80; lr: 0.00000601;   0/1070 tok/s;   3626 sec
[2020-03-29 20:40:47,719 INFO] Loading train dataset from ../bert_data/cnndm.train.11.bert.pt, number of examples: 1999
[2020-03-29 20:41:16,505 INFO] Step 2200/210000; acc:  37.22; ppl: 35.80; xent: 3.58; lr: 0.00000615;   0/537 tok/s;   3711 sec
[2020-03-29 20:42:40,857 INFO] Step 2250/210000; acc:  35.73; ppl: 32.56; xent: 3.48; lr: 0.00000629;   0/624 tok/s;   3796 sec
[2020-03-29 20:43:35,609 INFO] Loading train dataset from ../bert_data/cnndm.train.110.bert.pt, number of examples: 2000
[2020-03-29 20:44:05,993 INFO] Step 2300/210000; acc:  32.64; ppl: 45.02; xent: 3.81; lr: 0.00000643;   0/552 tok/s;   3881 sec
[2020-03-29 20:45:30,431 INFO] Step 2350/210000; acc:  33.84; ppl: 39.76; xent: 3.68; lr: 0.00000657;   0/551 tok/s;   3965 sec
[2020-03-29 20:46:23,097 INFO] Loading train dataset from ../bert_data/cnndm.train.111.bert.pt, number of examples: 2000
[2020-03-29 20:46:55,068 INFO] Step 2400/210000; acc:  36.34; ppl: 34.03; xent: 3.53; lr: 0.00000671;   0/769 tok/s;   4050 sec
[2020-03-29 20:48:18,976 INFO] Step 2450/210000; acc:  36.83; ppl: 36.24; xent: 3.59; lr: 0.00000685;   0/654 tok/s;   4134 sec
[2020-03-29 20:49:12,254 INFO] Loading train dataset from ../bert_data/cnndm.train.112.bert.pt, number of examples: 1999
[2020-03-29 20:49:43,856 INFO] Step 2500/210000; acc:  37.02; ppl: 34.45; xent: 3.54; lr: 0.00000699;   0/822 tok/s;   4219 sec
[2020-03-29 20:49:43,859 INFO] Saving checkpoint ../models/model_step_2500.pt
[2020-03-29 20:51:07,958 INFO] Step 2550/210000; acc:  32.42; ppl: 48.40; xent: 3.88; lr: 0.00000713;   0/918 tok/s;   4303 sec
[2020-03-29 20:51:59,413 INFO] Loading train dataset from ../bert_data/cnndm.train.113.bert.pt, number of examples: 1999
[2020-03-29 20:52:33,447 INFO] Step 2600/210000; acc:  32.97; ppl: 43.54; xent: 3.77; lr: 0.00000727;   0/866 tok/s;   4388 sec
[2020-03-29 20:53:57,454 INFO] Step 2650/210000; acc:  35.59; ppl: 35.43; xent: 3.57; lr: 0.00000741;   0/899 tok/s;   4472 sec
[2020-03-29 20:54:46,897 INFO] Loading train dataset from ../bert_data/cnndm.train.114.bert.pt, number of examples: 1998
[2020-03-29 20:55:22,218 INFO] Step 2700/210000; acc:  31.79; ppl: 48.40; xent: 3.88; lr: 0.00000755;   0/540 tok/s;   4557 sec
[2020-03-29 20:56:46,052 INFO] Step 2750/210000; acc:  33.21; ppl: 48.03; xent: 3.87; lr: 0.00000769;   0/774 tok/s;   4641 sec
[2020-03-29 20:57:34,024 INFO] Loading train dataset from ../bert_data/cnndm.train.115.bert.pt, number of examples: 2000
[2020-03-29 20:58:11,019 INFO] Step 2800/210000; acc:  36.00; ppl: 37.83; xent: 3.63; lr: 0.00000783;   0/512 tok/s;   4726 sec
[2020-03-29 20:59:35,439 INFO] Step 2850/210000; acc:  37.57; ppl: 31.70; xent: 3.46; lr: 0.00000797;   0/545 tok/s;   4810 sec
[2020-03-29 21:00:21,368 INFO] Loading train dataset from ../bert_data/cnndm.train.116.bert.pt, number of examples: 2000
[2020-03-29 21:01:00,172 INFO] Step 2900/210000; acc:  35.41; ppl: 35.13; xent: 3.56; lr: 0.00000811;   0/615 tok/s;   4895 sec
[2020-03-29 21:02:24,287 INFO] Step 2950/210000; acc:  39.01; ppl: 28.75; xent: 3.36; lr: 0.00000825;   0/664 tok/s;   4979 sec
[2020-03-29 21:03:08,806 INFO] Loading train dataset from ../bert_data/cnndm.train.117.bert.pt, number of examples: 2000
[2020-03-29 21:03:49,168 INFO] Step 3000/210000; acc:  32.86; ppl: 40.19; xent: 3.69; lr: 0.00000839;   0/762 tok/s;   5064 sec
[2020-03-29 21:03:49,171 INFO] Saving checkpoint ../models/model_step_3000.pt
[2020-03-29 21:05:13,664 INFO] Step 3050/210000; acc:  38.16; ppl: 33.48; xent: 3.51; lr: 0.00000853;   0/759 tok/s;   5148 sec
[2020-03-29 21:05:56,434 INFO] Loading train dataset from ../bert_data/cnndm.train.118.bert.pt, number of examples: 2001
[2020-03-29 21:06:38,607 INFO] Step 3100/210000; acc:  35.70; ppl: 33.04; xent: 3.50; lr: 0.00000866;   0/938 tok/s;   5233 sec
[2020-03-29 21:08:02,795 INFO] Step 3150/210000; acc:  34.97; ppl: 43.84; xent: 3.78; lr: 0.00000880;   0/829 tok/s;   5318 sec
[2020-03-29 21:08:45,718 INFO] Loading train dataset from ../bert_data/cnndm.train.119.bert.pt, number of examples: 2000
[2020-03-29 21:09:27,767 INFO] Step 3200/210000; acc:  41.79; ppl: 23.97; xent: 3.18; lr: 0.00000894;   0/965 tok/s;   5403 sec
[2020-03-29 21:10:51,912 INFO] Step 3250/210000; acc:  34.72; ppl: 43.23; xent: 3.77; lr: 0.00000908;   0/859 tok/s;   5487 sec
[2020-03-29 21:11:33,104 INFO] Loading train dataset from ../bert_data/cnndm.train.12.bert.pt, number of examples: 2001
[2020-03-29 21:12:16,764 INFO] Step 3300/210000; acc:  34.82; ppl: 34.25; xent: 3.53; lr: 0.00000922;   0/1246 tok/s;   5572 sec
[2020-03-29 21:13:41,156 INFO] Step 3350/210000; acc:  38.49; ppl: 30.52; xent: 3.42; lr: 0.00000936;   0/993 tok/s;   5656 sec
[2020-03-29 21:14:22,446 INFO] Loading train dataset from ../bert_data/cnndm.train.120.bert.pt, number of examples: 2001
[2020-03-29 21:15:06,255 INFO] Step 3400/210000; acc:  35.48; ppl: 37.02; xent: 3.61; lr: 0.00000950;   0/528 tok/s;   5741 sec
[2020-03-29 21:16:30,881 INFO] Step 3450/210000; acc:  32.38; ppl: 45.15; xent: 3.81; lr: 0.00000964;   0/620 tok/s;   5826 sec
[2020-03-29 21:17:10,024 INFO] Loading train dataset from ../bert_data/cnndm.train.121.bert.pt, number of examples: 2001
[2020-03-29 21:17:56,136 INFO] Step 3500/210000; acc:  41.17; ppl: 28.88; xent: 3.36; lr: 0.00000978;   0/515 tok/s;   5911 sec
[2020-03-29 21:17:56,139 INFO] Saving checkpoint ../models/model_step_3500.pt
[2020-03-29 21:19:22,541 INFO] Step 3550/210000; acc:  43.15; ppl: 27.52; xent: 3.31; lr: 0.00000992;   0/545 tok/s;   5997 sec
[2020-03-29 21:20:00,377 INFO] Loading train dataset from ../bert_data/cnndm.train.122.bert.pt, number of examples: 2000
[2020-03-29 21:20:47,769 INFO] Step 3600/210000; acc:  34.05; ppl: 41.82; xent: 3.73; lr: 0.00001006;   0/722 tok/s;   6083 sec
[2020-03-29 21:22:11,874 INFO] Step 3650/210000; acc:  39.12; ppl: 26.57; xent: 3.28; lr: 0.00001020;   0/797 tok/s;   6167 sec
[2020-03-29 21:22:48,060 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001
[2020-03-29 21:23:36,576 INFO] Step 3700/210000; acc:  40.01; ppl: 25.81; xent: 3.25; lr: 0.00001034;   0/870 tok/s;   6251 sec
[2020-03-29 21:25:00,649 INFO] Step 3750/210000; acc:  38.97; ppl: 27.61; xent: 3.32; lr: 0.00001048;   0/849 tok/s;   6335 sec
[2020-03-29 21:25:35,038 INFO] Loading train dataset from ../bert_data/cnndm.train.124.bert.pt, number of examples: 2001
[2020-03-29 21:26:25,881 INFO] Step 3800/210000; acc:  42.10; ppl: 22.32; xent: 3.11; lr: 0.00001062;   0/874 tok/s;   6421 sec
[2020-03-29 21:27:50,091 INFO] Step 3850/210000; acc:  38.42; ppl: 30.77; xent: 3.43; lr: 0.00001076;   0/770 tok/s;   6505 sec
[2020-03-29 21:28:24,929 INFO] Loading train dataset from ../bert_data/cnndm.train.125.bert.pt, number of examples: 2000
[2020-03-29 21:29:15,663 INFO] Step 3900/210000; acc:  35.71; ppl: 38.03; xent: 3.64; lr: 0.00001090;   0/862 tok/s;   6590 sec
[2020-03-29 21:30:40,255 INFO] Step 3950/210000; acc:  35.88; ppl: 30.77; xent: 3.43; lr: 0.00001104;   0/763 tok/s;   6675 sec
[2020-03-29 21:31:10,934 INFO] Loading train dataset from ../bert_data/cnndm.train.126.bert.pt, number of examples: 1999
[2020-03-29 21:32:04,770 INFO] Step 4000/210000; acc:  42.74; ppl: 22.37; xent: 3.11; lr: 0.00001118;   0/558 tok/s;   6760 sec
[2020-03-29 21:32:04,774 INFO] Saving checkpoint ../models/model_step_4000.pt
[2020-03-29 21:33:31,166 INFO] Step 4050/210000; acc:  31.72; ppl: 46.25; xent: 3.83; lr: 0.00001132;   0/526 tok/s;   6846 sec
[2020-03-29 21:34:00,589 INFO] Loading train dataset from ../bert_data/cnndm.train.127.bert.pt, number of examples: 2000
[2020-03-29 21:34:56,043 INFO] Step 4100/210000; acc:  38.44; ppl: 33.34; xent: 3.51; lr: 0.00001146;   0/635 tok/s;   6931 sec
[2020-03-29 21:36:20,336 INFO] Step 4150/210000; acc:  35.94; ppl: 32.66; xent: 3.49; lr: 0.00001160;   0/652 tok/s;   7015 sec
[2020-03-29 21:36:49,746 INFO] Loading train dataset from ../bert_data/cnndm.train.128.bert.pt, number of examples: 2001
[2020-03-29 21:37:45,387 INFO] Step 4200/210000; acc:  38.83; ppl: 26.84; xent: 3.29; lr: 0.00001174;   0/640 tok/s;   7100 sec
[2020-03-29 21:39:09,527 INFO] Step 4250/210000; acc:  41.91; ppl: 25.63; xent: 3.24; lr: 0.00001188;   0/612 tok/s;   7184 sec
[2020-03-29 21:39:37,055 INFO] Loading train dataset from ../bert_data/cnndm.train.129.bert.pt, number of examples: 2000
[2020-03-29 21:40:34,474 INFO] Step 4300/210000; acc:  38.35; ppl: 30.19; xent: 3.41; lr: 0.00001202;   0/803 tok/s;   7269 sec
[2020-03-29 21:41:58,190 INFO] Step 4350/210000; acc:  39.47; ppl: 27.29; xent: 3.31; lr: 0.00001216;   0/899 tok/s;   7353 sec
[2020-03-29 21:42:24,324 INFO] Loading train dataset from ../bert_data/cnndm.train.13.bert.pt, number of examples: 2001
[2020-03-29 21:43:23,435 INFO] Step 4400/210000; acc:  39.46; ppl: 26.84; xent: 3.29; lr: 0.00001230;   0/1241 tok/s;   7438 sec
[2020-03-29 21:44:47,684 INFO] Step 4450/210000; acc:  37.62; ppl: 32.90; xent: 3.49; lr: 0.00001244;   0/861 tok/s;   7523 sec
[2020-03-29 21:45:12,116 INFO] Loading train dataset from ../bert_data/cnndm.train.130.bert.pt, number of examples: 2000
[2020-03-29 21:46:12,415 INFO] Step 4500/210000; acc:  39.94; ppl: 27.15; xent: 3.30; lr: 0.00001258;   0/835 tok/s;   7607 sec
[2020-03-29 21:46:12,419 INFO] Saving checkpoint ../models/model_step_4500.pt
[2020-03-29 21:47:38,243 INFO] Step 4550/210000; acc:  38.98; ppl: 29.39; xent: 3.38; lr: 0.00001272;   0/1028 tok/s;   7693 sec
[2020-03-29 21:48:02,315 INFO] Loading train dataset from ../bert_data/cnndm.train.131.bert.pt, number of examples: 2001
[2020-03-29 21:49:03,147 INFO] Step 4600/210000; acc:  36.33; ppl: 38.13; xent: 3.64; lr: 0.00001286;   0/481 tok/s;   7778 sec
[2020-03-29 21:50:27,188 INFO] Step 4650/210000; acc:  37.34; ppl: 32.68; xent: 3.49; lr: 0.00001300;   0/741 tok/s;   7862 sec
[2020-03-29 21:50:49,618 INFO] Loading train dataset from ../bert_data/cnndm.train.132.bert.pt, number of examples: 2001
[2020-03-29 21:51:51,676 INFO] Step 4700/210000; acc:  39.22; ppl: 31.45; xent: 3.45; lr: 0.00001314;   0/538 tok/s;   7946 sec
[2020-03-29 21:53:15,657 INFO] Step 4750/210000; acc:  34.75; ppl: 33.25; xent: 3.50; lr: 0.00001328;   0/1020 tok/s;   8030 sec
[2020-03-29 21:53:38,725 INFO] Loading train dataset from ../bert_data/cnndm.train.133.bert.pt, number of examples: 1999
[2020-03-29 21:54:40,865 INFO] Step 4800/210000; acc:  38.78; ppl: 28.14; xent: 3.34; lr: 0.00001342;   0/619 tok/s;   8116 sec
[2020-03-29 21:56:05,086 INFO] Step 4850/210000; acc:  40.76; ppl: 27.55; xent: 3.32; lr: 0.00001356;   0/658 tok/s;   8200 sec
[2020-03-29 21:56:25,937 INFO] Loading train dataset from ../bert_data/cnndm.train.134.bert.pt, number of examples: 2001
[2020-03-29 21:57:30,103 INFO] Step 4900/210000; acc:  36.85; ppl: 35.81; xent: 3.58; lr: 0.00001370;   0/757 tok/s;   8285 sec
[2020-03-29 21:58:54,227 INFO] Step 4950/210000; acc:  40.88; ppl: 26.13; xent: 3.26; lr: 0.00001384;   0/724 tok/s;   8369 sec
[2020-03-29 21:59:13,667 INFO] Loading train dataset from ../bert_data/cnndm.train.135.bert.pt, number of examples: 1999
[2020-03-29 22:00:18,930 INFO] Step 5000/210000; acc:  42.42; ppl: 23.32; xent: 3.15; lr: 0.00001398;   0/785 tok/s;   8454 sec
[2020-03-29 22:00:18,934 INFO] Saving checkpoint ../models/model_step_5000.pt
[2020-03-29 22:01:44,702 INFO] Step 5050/210000; acc:  40.27; ppl: 22.77; xent: 3.13; lr: 0.00001412;   0/842 tok/s;   8540 sec
[2020-03-29 22:02:02,326 INFO] Loading train dataset from ../bert_data/cnndm.train.136.bert.pt, number of examples: 2001
[2020-03-29 22:03:09,415 INFO] Step 5100/210000; acc:  39.06; ppl: 25.36; xent: 3.23; lr: 0.00001425;   0/1040 tok/s;   8624 sec
[2020-03-29 22:04:33,417 INFO] Step 5150/210000; acc:  42.45; ppl: 23.68; xent: 3.16; lr: 0.00001439;   0/804 tok/s;   8708 sec
[2020-03-29 22:04:51,256 INFO] Loading train dataset from ../bert_data/cnndm.train.137.bert.pt, number of examples: 2000
[2020-03-29 22:05:58,551 INFO] Step 5200/210000; acc:  39.03; ppl: 26.37; xent: 3.27; lr: 0.00001453;   0/1161 tok/s;   8793 sec
[2020-03-29 22:07:21,995 INFO] Step 5250/210000; acc:  36.59; ppl: 36.06; xent: 3.59; lr: 0.00001467;   0/1094 tok/s;   8877 sec
[2020-03-29 22:07:38,017 INFO] Loading train dataset from ../bert_data/cnndm.train.138.bert.pt, number of examples: 2000
[2020-03-29 22:08:47,172 INFO] Step 5300/210000; acc:  42.54; ppl: 20.54; xent: 3.02; lr: 0.00001481;   0/1022 tok/s;   8962 sec
[2020-03-29 22:10:11,248 INFO] Step 5350/210000; acc:  40.97; ppl: 22.90; xent: 3.13; lr: 0.00001495;   0/1175 tok/s;   9046 sec
[2020-03-29 22:10:25,254 INFO] Loading train dataset from ../bert_data/cnndm.train.139.bert.pt, number of examples: 2001
[2020-03-29 22:11:35,447 INFO] Step 5400/210000; acc:  42.41; ppl: 22.21; xent: 3.10; lr: 0.00001509;   0/531 tok/s;   9130 sec
[2020-03-29 22:12:59,680 INFO] Step 5450/210000; acc:  39.51; ppl: 27.35; xent: 3.31; lr: 0.00001523;   0/505 tok/s;   9215 sec
[2020-03-29 22:13:13,862 INFO] Loading train dataset from ../bert_data/cnndm.train.14.bert.pt, number of examples: 1998
[2020-03-29 22:14:24,907 INFO] Step 5500/210000; acc:  36.85; ppl: 30.55; xent: 3.42; lr: 0.00001537;   0/495 tok/s;   9300 sec
[2020-03-29 22:14:24,931 INFO] Saving checkpoint ../models/model_step_5500.pt
[2020-03-29 22:15:51,482 INFO] Step 5550/210000; acc:  37.38; ppl: 36.08; xent: 3.59; lr: 0.00001551;   0/552 tok/s;   9386 sec
[2020-03-29 22:16:03,921 INFO] Loading train dataset from ../bert_data/cnndm.train.140.bert.pt, number of examples: 2000
[2020-03-29 22:17:16,410 INFO] Step 5600/210000; acc:  40.71; ppl: 22.55; xent: 3.12; lr: 0.00001565;   0/693 tok/s;   9471 sec
[2020-03-29 22:18:40,561 INFO] Step 5650/210000; acc:  39.61; ppl: 28.43; xent: 3.35; lr: 0.00001579;   0/648 tok/s;   9555 sec
[2020-03-29 22:18:51,531 INFO] Loading train dataset from ../bert_data/cnndm.train.141.bert.pt, number of examples: 1999
[2020-03-29 22:20:05,528 INFO] Step 5700/210000; acc:  39.57; ppl: 30.09; xent: 3.40; lr: 0.00001593;   0/809 tok/s;   9640 sec
[2020-03-29 22:21:29,224 INFO] Step 5750/210000; acc:  43.09; ppl: 20.96; xent: 3.04; lr: 0.00001607;   0/639 tok/s;   9724 sec
[2020-03-29 22:21:38,368 INFO] Loading train dataset from ../bert_data/cnndm.train.142.bert.pt, number of examples: 2000
[2020-03-29 22:22:54,061 INFO] Step 5800/210000; acc:  39.25; ppl: 25.46; xent: 3.24; lr: 0.00001621;   0/723 tok/s;   9809 sec
[2020-03-29 22:24:18,373 INFO] Step 5850/210000; acc:  47.97; ppl: 16.59; xent: 2.81; lr: 0.00001635;   0/631 tok/s;   9893 sec
[2020-03-29 22:24:27,553 INFO] Loading train dataset from ../bert_data/cnndm.train.143.bert.pt, number of examples: 1084
[2020-03-29 22:25:43,239 INFO] Step 5900/210000; acc:  38.57; ppl: 30.64; xent: 3.42; lr: 0.00001649;   0/812 tok/s;   9978 sec
[2020-03-29 22:25:59,283 INFO] Loading train dataset from ../bert_data/cnndm.train.15.bert.pt, number of examples: 1999
[2020-03-29 22:27:07,914 INFO] Step 5950/210000; acc:  36.09; ppl: 30.53; xent: 3.42; lr: 0.00001663;   0/1037 tok/s;  10063 sec
[2020-03-29 22:28:31,949 INFO] Step 6000/210000; acc:  41.34; ppl: 25.73; xent: 3.25; lr: 0.00001677;   0/930 tok/s;  10147 sec
[2020-03-29 22:28:31,971 INFO] Saving checkpoint ../models/model_step_6000.pt
[2020-03-29 22:28:47,875 INFO] Loading train dataset from ../bert_data/cnndm.train.150.bert.pt, number of examples: 1985
[2020-03-29 22:29:48,804 INFO] Step 6050/210000; acc:  37.86; ppl: 26.87; xent: 3.29; lr: 0.00001691;   0/523 tok/s;  10224 sec
[2020-03-29 22:31:01,124 INFO] Step 6100/210000; acc:  39.92; ppl: 23.82; xent: 3.17; lr: 0.00001705;   0/712 tok/s;  10296 sec
[2020-03-29 22:31:04,545 INFO] Loading train dataset from ../bert_data/cnndm.train.151.bert.pt, number of examples: 1990
[2020-03-29 22:32:14,720 INFO] Step 6150/210000; acc:  44.62; ppl: 17.46; xent: 2.86; lr: 0.00001719;   0/457 tok/s;  10370 sec
[2020-03-29 22:33:24,170 INFO] Loading train dataset from ../bert_data/cnndm.train.152.bert.pt, number of examples: 993
[2020-03-29 22:33:28,502 INFO] Step 6200/210000; acc:  46.47; ppl: 14.05; xent: 2.64; lr: 0.00001733;   0/467 tok/s;  10443 sec
[2020-03-29 22:34:34,848 INFO] Loading train dataset from ../bert_data/cnndm.train.153.bert.pt, number of examples: 1982
[2020-03-29 22:34:43,020 INFO] Step 6250/210000; acc:  27.48; ppl: 70.48; xent: 4.26; lr: 0.00001747;   0/1335 tok/s;  10518 sec
[2020-03-29 22:36:05,548 INFO] Step 6300/210000; acc:  34.72; ppl: 37.73; xent: 3.63; lr: 0.00001761;   0/676 tok/s;  10600 sec
[2020-03-29 22:37:10,447 INFO] Loading train dataset from ../bert_data/cnndm.train.154.bert.pt, number of examples: 1978
[2020-03-29 22:37:28,587 INFO] Step 6350/210000; acc:  37.76; ppl: 22.69; xent: 3.12; lr: 0.00001775;   0/449 tok/s;  10683 sec
[2020-03-29 22:38:51,124 INFO] Step 6400/210000; acc:  33.66; ppl: 35.74; xent: 3.58; lr: 0.00001789;   0/971 tok/s;  10766 sec
[2020-03-29 22:39:46,576 INFO] Loading train dataset from ../bert_data/cnndm.train.155.bert.pt, number of examples: 1988
[2020-03-29 22:40:14,626 INFO] Step 6450/210000; acc:  38.66; ppl: 24.06; xent: 3.18; lr: 0.00001803;   0/528 tok/s;  10849 sec
[2020-03-29 22:41:36,565 INFO] Step 6500/210000; acc:  33.86; ppl: 33.58; xent: 3.51; lr: 0.00001817;   0/1355 tok/s;  10931 sec
[2020-03-29 22:41:36,568 INFO] Saving checkpoint ../models/model_step_6500.pt
[2020-03-29 22:42:30,224 INFO] Loading train dataset from ../bert_data/cnndm.train.156.bert.pt, number of examples: 1993
[2020-03-29 22:43:06,320 INFO] Step 6550/210000; acc:  39.11; ppl: 21.86; xent: 3.08; lr: 0.00001831;   0/634 tok/s;  11021 sec
[2020-03-29 22:44:28,857 INFO] Step 6600/210000; acc:  34.77; ppl: 30.12; xent: 3.41; lr: 0.00001845;   0/1136 tok/s;  11104 sec
[2020-03-29 22:45:07,046 INFO] Loading train dataset from ../bert_data/cnndm.train.157.bert.pt, number of examples: 1981
[2020-03-29 22:45:51,829 INFO] Step 6650/210000; acc:  40.13; ppl: 21.03; xent: 3.05; lr: 0.00001859;   0/773 tok/s;  11187 sec
[2020-03-29 22:47:15,505 INFO] Step 6700/210000; acc:  35.69; ppl: 25.88; xent: 3.25; lr: 0.00001873;   0/895 tok/s;  11270 sec
[2020-03-29 22:47:44,171 INFO] Loading train dataset from ../bert_data/cnndm.train.158.bert.pt, number of examples: 1987
[2020-03-29 22:48:38,042 INFO] Step 6750/210000; acc:  38.31; ppl: 20.68; xent: 3.03; lr: 0.00001887;   0/930 tok/s;  11353 sec
[2020-03-29 22:50:00,762 INFO] Step 6800/210000; acc:  44.93; ppl: 13.41; xent: 2.60; lr: 0.00001901;   0/418 tok/s;  11436 sec
[2020-03-29 22:50:20,962 INFO] Loading train dataset from ../bert_data/cnndm.train.159.bert.pt, number of examples: 1986
[2020-03-29 22:51:23,378 INFO] Step 6850/210000; acc:  37.83; ppl: 28.27; xent: 3.34; lr: 0.00001915;   0/1275 tok/s;  11518 sec
[2020-03-29 22:52:45,771 INFO] Step 6900/210000; acc:  44.62; ppl: 15.82; xent: 2.76; lr: 0.00001929;   0/522 tok/s;  11601 sec
[2020-03-29 22:52:57,655 INFO] Loading train dataset from ../bert_data/cnndm.train.16.bert.pt, number of examples: 2001
[2020-03-29 22:54:09,823 INFO] Step 6950/210000; acc:  34.42; ppl: 38.31; xent: 3.65; lr: 0.00001943;   0/687 tok/s;  11685 sec
[2020-03-29 22:55:33,774 INFO] Step 7000/210000; acc:  42.44; ppl: 26.47; xent: 3.28; lr: 0.00001957;   0/564 tok/s;  11769 sec
[2020-03-29 22:55:33,778 INFO] Saving checkpoint ../models/model_step_7000.pt
[2020-03-29 22:55:46,698 INFO] Loading train dataset from ../bert_data/cnndm.train.160.bert.pt, number of examples: 1983
[2020-03-29 22:56:59,755 INFO] Step 7050/210000; acc:  32.04; ppl: 34.21; xent: 3.53; lr: 0.00001971;   0/985 tok/s;  11855 sec
[2020-03-29 22:58:22,487 INFO] Step 7100/210000; acc:  39.86; ppl: 20.94; xent: 3.04; lr: 0.00001985;   0/602 tok/s;  11937 sec
[2020-03-29 22:58:24,776 INFO] Loading train dataset from ../bert_data/cnndm.train.161.bert.pt, number of examples: 1990
[2020-03-29 22:59:45,336 INFO] Step 7150/210000; acc:  40.00; ppl: 19.61; xent: 2.98; lr: 0.00001998;   0/565 tok/s;  12020 sec
[2020-03-29 23:01:03,236 INFO] Loading train dataset from ../bert_data/cnndm.train.162.bert.pt, number of examples: 1986
[2020-03-29 23:01:07,872 INFO] Step 7200/210000; acc:  34.35; ppl: 31.33; xent: 3.44; lr: 0.00002012;   0/881 tok/s;  12103 sec
[2020-03-29 23:02:31,353 INFO] Step 7250/210000; acc:  36.17; ppl: 26.47; xent: 3.28; lr: 0.00002026;   0/651 tok/s;  12186 sec
[2020-03-29 23:03:39,223 INFO] Loading train dataset from ../bert_data/cnndm.train.163.bert.pt, number of examples: 1982
[2020-03-29 23:03:53,837 INFO] Step 7300/210000; acc:  36.87; ppl: 28.07; xent: 3.33; lr: 0.00002040;   0/868 tok/s;  12269 sec
[2020-03-29 23:05:16,160 INFO] Step 7350/210000; acc:  45.36; ppl: 12.66; xent: 2.54; lr: 0.00002054;   0/488 tok/s;  12351 sec
[2020-03-29 23:06:16,048 INFO] Loading train dataset from ../bert_data/cnndm.train.164.bert.pt, number of examples: 1984
[2020-03-29 23:06:38,509 INFO] Step 7400/210000; acc:  34.08; ppl: 28.38; xent: 3.35; lr: 0.00002068;   0/1458 tok/s;  12433 sec
[2020-03-29 23:08:01,558 INFO] Step 7450/210000; acc:  44.22; ppl: 15.75; xent: 2.76; lr: 0.00002082;   0/503 tok/s;  12516 sec
[2020-03-29 23:08:51,620 INFO] Loading train dataset from ../bert_data/cnndm.train.165.bert.pt, number of examples: 1984
[2020-03-29 23:09:24,631 INFO] Step 7500/210000; acc:  33.99; ppl: 30.65; xent: 3.42; lr: 0.00002096;   0/1364 tok/s;  12599 sec
[2020-03-29 23:09:24,634 INFO] Saving checkpoint ../models/model_step_7500.pt
[2020-03-29 23:10:48,847 INFO] Step 7550/210000; acc:  43.30; ppl: 17.95; xent: 2.89; lr: 0.00002110;   0/626 tok/s;  12684 sec
[2020-03-29 23:11:30,146 INFO] Loading train dataset from ../bert_data/cnndm.train.166.bert.pt, number of examples: 1370
[2020-03-29 23:12:11,411 INFO] Step 7600/210000; acc:  39.07; ppl: 21.80; xent: 3.08; lr: 0.00002124;   0/695 tok/s;  12766 sec
[2020-03-29 23:13:20,770 INFO] Loading train dataset from ../bert_data/cnndm.train.167.bert.pt, number of examples: 1089
[2020-03-29 23:13:35,107 INFO] Step 7650/210000; acc:  40.77; ppl: 19.38; xent: 2.96; lr: 0.00002138;   0/1053 tok/s;  12850 sec
[2020-03-29 23:14:43,495 INFO] Loading train dataset from ../bert_data/cnndm.train.168.bert.pt, number of examples: 1991
[2020-03-29 23:14:54,867 INFO] Step 7700/210000; acc:  43.64; ppl: 16.71; xent: 2.82; lr: 0.00002152;   0/501 tok/s;  12930 sec
[2020-03-29 23:16:14,750 INFO] Step 7750/210000; acc:  43.81; ppl: 14.74; xent: 2.69; lr: 0.00002166;   0/997 tok/s;  13010 sec
[2020-03-29 23:17:18,133 INFO] Loading train dataset from ../bert_data/cnndm.train.169.bert.pt, number of examples: 1995
[2020-03-29 23:17:36,176 INFO] Step 7800/210000; acc:  46.10; ppl: 11.72; xent: 2.46; lr: 0.00002180;   0/420 tok/s;  13091 sec
[2020-03-29 23:18:56,478 INFO] Step 7850/210000; acc:  39.95; ppl: 18.87; xent: 2.94; lr: 0.00002194;   0/1015 tok/s;  13171 sec
[2020-03-29 23:19:52,062 INFO] Loading train dataset from ../bert_data/cnndm.train.17.bert.pt, number of examples: 2000
[2020-03-29 23:20:18,973 INFO] Step 7900/210000; acc:  33.44; ppl: 42.17; xent: 3.74; lr: 0.00002208;   0/560 tok/s;  13254 sec
[2020-03-29 23:21:43,125 INFO] Step 7950/210000; acc:  36.54; ppl: 40.78; xent: 3.71; lr: 0.00002222;   0/473 tok/s;  13338 sec
[2020-03-29 23:22:39,027 INFO] Loading train dataset from ../bert_data/cnndm.train.170.bert.pt, number of examples: 1981
[2020-03-29 23:23:06,405 INFO] Step 8000/210000; acc:  39.43; ppl: 18.46; xent: 2.92; lr: 0.00002236;   0/601 tok/s;  13421 sec
[2020-03-29 23:23:06,408 INFO] Saving checkpoint ../models/model_step_8000.pt
[2020-03-29 23:24:29,338 INFO] Step 8050/210000; acc:  32.84; ppl: 32.42; xent: 3.48; lr: 0.00002229;   0/1478 tok/s;  13504 sec
[2020-03-29 23:25:13,752 INFO] Loading train dataset from ../bert_data/cnndm.train.171.bert.pt, number of examples: 1990
[2020-03-29 23:25:50,992 INFO] Step 8100/210000; acc:  41.65; ppl: 16.08; xent: 2.78; lr: 0.00002222;   0/877 tok/s;  13586 sec
[2020-03-29 23:27:11,798 INFO] Step 8150/210000; acc:  49.68; ppl: 10.66; xent: 2.37; lr: 0.00002215;   0/358 tok/s;  13667 sec
[2020-03-29 23:27:47,835 INFO] Loading train dataset from ../bert_data/cnndm.train.172.bert.pt, number of examples: 1983
[2020-03-29 23:28:32,497 INFO] Step 8200/210000; acc:  38.31; ppl: 19.26; xent: 2.96; lr: 0.00002209;   0/947 tok/s;  13747 sec
[2020-03-29 23:29:53,502 INFO] Step 8250/210000; acc:  43.43; ppl: 13.77; xent: 2.62; lr: 0.00002202;   0/349 tok/s;  13828 sec
[2020-03-29 23:30:21,680 INFO] Loading train dataset from ../bert_data/cnndm.train.18.bert.pt, number of examples: 1998
[2020-03-29 23:31:17,054 INFO] Step 8300/210000; acc:  37.95; ppl: 32.03; xent: 3.47; lr: 0.00002195;   0/697 tok/s;  13912 sec
[2020-03-29 23:32:41,469 INFO] Step 8350/210000; acc:  37.98; ppl: 33.28; xent: 3.51; lr: 0.00002189;   0/643 tok/s;  13996 sec
[2020-03-29 23:33:09,147 INFO] Loading train dataset from ../bert_data/cnndm.train.19.bert.pt, number of examples: 2000
[2020-03-29 23:34:06,278 INFO] Step 8400/210000; acc:  40.20; ppl: 28.41; xent: 3.35; lr: 0.00002182;   0/650 tok/s;  14081 sec
[2020-03-29 23:35:30,370 INFO] Step 8450/210000; acc:  43.97; ppl: 23.85; xent: 3.17; lr: 0.00002176;   0/511 tok/s;  14165 sec
[2020-03-29 23:35:56,609 INFO] Loading train dataset from ../bert_data/cnndm.train.2.bert.pt, number of examples: 2001
[2020-03-29 23:36:55,489 INFO] Step 8500/210000; acc:  38.70; ppl: 33.51; xent: 3.51; lr: 0.00002169;   0/849 tok/s;  14250 sec
[2020-03-29 23:36:55,493 INFO] Saving checkpoint ../models/model_step_8500.pt
[2020-03-29 23:38:21,664 INFO] Step 8550/210000; acc:  43.51; ppl: 21.83; xent: 3.08; lr: 0.00002163;   0/713 tok/s;  14336 sec
[2020-03-29 23:38:46,189 INFO] Loading train dataset from ../bert_data/cnndm.train.20.bert.pt, number of examples: 2000
[2020-03-29 23:39:47,046 INFO] Step 8600/210000; acc:  38.75; ppl: 29.04; xent: 3.37; lr: 0.00002157;   0/972 tok/s;  14422 sec
[2020-03-29 23:41:11,330 INFO] Step 8650/210000; acc:  41.54; ppl: 26.57; xent: 3.28; lr: 0.00002150;   0/803 tok/s;  14506 sec
[2020-03-29 23:41:33,938 INFO] Loading train dataset from ../bert_data/cnndm.train.21.bert.pt, number of examples: 2001
[2020-03-29 23:42:36,368 INFO] Step 8700/210000; acc:  35.10; ppl: 35.11; xent: 3.56; lr: 0.00002144;   0/999 tok/s;  14591 sec
[2020-03-29 23:44:00,280 INFO] Step 8750/210000; acc:  43.84; ppl: 20.72; xent: 3.03; lr: 0.00002138;   0/881 tok/s;  14675 sec
[2020-03-29 23:44:22,934 INFO] Loading train dataset from ../bert_data/cnndm.train.22.bert.pt, number of examples: 1999
[2020-03-29 23:45:25,031 INFO] Step 8800/210000; acc:  37.47; ppl: 30.96; xent: 3.43; lr: 0.00002132;   0/452 tok/s;  14760 sec
[2020-03-29 23:46:48,942 INFO] Step 8850/210000; acc:  37.65; ppl: 28.99; xent: 3.37; lr: 0.00002126;   0/1177 tok/s;  14844 sec
[2020-03-29 23:47:09,934 INFO] Loading train dataset from ../bert_data/cnndm.train.23.bert.pt, number of examples: 2001
[2020-03-29 23:48:13,809 INFO] Step 8900/210000; acc:  40.42; ppl: 25.11; xent: 3.22; lr: 0.00002120;   0/566 tok/s;  14929 sec
[2020-03-29 23:49:38,037 INFO] Step 8950/210000; acc:  36.91; ppl: 31.52; xent: 3.45; lr: 0.00002114;   0/869 tok/s;  15013 sec
[2020-03-29 23:49:59,447 INFO] Loading train dataset from ../bert_data/cnndm.train.24.bert.pt, number of examples: 2001
[2020-03-29 23:51:03,218 INFO] Step 9000/210000; acc:  43.23; ppl: 21.71; xent: 3.08; lr: 0.00002108;   0/712 tok/s;  15098 sec
[2020-03-29 23:51:03,222 INFO] Saving checkpoint ../models/model_step_9000.pt
[2020-03-29 23:52:29,655 INFO] Step 9050/210000; acc:  41.69; ppl: 25.14; xent: 3.22; lr: 0.00002102;   0/540 tok/s;  15184 sec
[2020-03-29 23:52:48,847 INFO] Loading train dataset from ../bert_data/cnndm.train.25.bert.pt, number of examples: 2001
[2020-03-29 23:53:54,764 INFO] Step 9100/210000; acc:  39.95; ppl: 27.52; xent: 3.32; lr: 0.00002097;   0/789 tok/s;  15270 sec
[2020-03-29 23:55:18,741 INFO] Step 9150/210000; acc:  38.27; ppl: 31.37; xent: 3.45; lr: 0.00002091;   0/742 tok/s;  15354 sec
[2020-03-29 23:55:38,377 INFO] Loading train dataset from ../bert_data/cnndm.train.26.bert.pt, number of examples: 2000
[2020-03-29 23:56:43,844 INFO] Step 9200/210000; acc:  45.52; ppl: 19.53; xent: 2.97; lr: 0.00002085;   0/717 tok/s;  15439 sec
[2020-03-29 23:58:07,960 INFO] Step 9250/210000; acc:  42.28; ppl: 24.28; xent: 3.19; lr: 0.00002080;   0/744 tok/s;  15523 sec
[2020-03-29 23:58:25,540 INFO] Loading train dataset from ../bert_data/cnndm.train.27.bert.pt, number of examples: 2001
[2020-03-29 23:59:32,945 INFO] Step 9300/210000; acc:  43.07; ppl: 21.90; xent: 3.09; lr: 0.00002074;   0/1110 tok/s;  15608 sec
[2020-03-30 00:00:57,494 INFO] Step 9350/210000; acc:  38.77; ppl: 27.92; xent: 3.33; lr: 0.00002068;   0/1046 tok/s;  15692 sec
[2020-03-30 00:01:13,477 INFO] Loading train dataset from ../bert_data/cnndm.train.28.bert.pt, number of examples: 2000
[2020-03-30 00:02:22,875 INFO] Step 9400/210000; acc:  38.84; ppl: 24.48; xent: 3.20; lr: 0.00002063;   0/761 tok/s;  15778 sec
[2020-03-30 00:03:46,846 INFO] Step 9450/210000; acc:  40.09; ppl: 24.78; xent: 3.21; lr: 0.00002057;   0/1055 tok/s;  15862 sec
[2020-03-30 00:04:00,945 INFO] Loading train dataset from ../bert_data/cnndm.train.29.bert.pt, number of examples: 1999
[2020-03-30 00:05:11,573 INFO] Step 9500/210000; acc:  41.84; ppl: 23.65; xent: 3.16; lr: 0.00002052;   0/940 tok/s;  15946 sec
[2020-03-30 00:05:11,600 INFO] Saving checkpoint ../models/model_step_9500.pt
[2020-03-30 00:06:38,047 INFO] Step 9550/210000; acc:  43.71; ppl: 22.32; xent: 3.11; lr: 0.00002047;   0/1113 tok/s;  16033 sec
[2020-03-30 00:06:50,382 INFO] Loading train dataset from ../bert_data/cnndm.train.3.bert.pt, number of examples: 2001
[2020-03-30 00:08:02,530 INFO] Step 9600/210000; acc:  49.58; ppl: 14.99; xent: 2.71; lr: 0.00002041;   0/421 tok/s;  16117 sec
[2020-03-30 00:09:26,455 INFO] Step 9650/210000; acc:  45.02; ppl: 18.42; xent: 2.91; lr: 0.00002036;   0/494 tok/s;  16201 sec
[2020-03-30 00:09:38,878 INFO] Loading train dataset from ../bert_data/cnndm.train.30.bert.pt, number of examples: 1996
[2020-03-30 00:10:50,989 INFO] Step 9700/210000; acc:  44.53; ppl: 23.56; xent: 3.16; lr: 0.00002031;   0/640 tok/s;  16286 sec
[2020-03-30 00:12:15,671 INFO] Step 9750/210000; acc:  48.95; ppl: 16.25; xent: 2.79; lr: 0.00002025;   0/689 tok/s;  16370 sec
[2020-03-30 00:12:26,562 INFO] Loading train dataset from ../bert_data/cnndm.train.31.bert.pt, number of examples: 2000
[2020-03-30 00:13:40,751 INFO] Step 9800/210000; acc:  42.96; ppl: 19.03; xent: 2.95; lr: 0.00002020;   0/809 tok/s;  16456 sec
[2020-03-30 00:15:04,870 INFO] Step 9850/210000; acc:  44.45; ppl: 21.42; xent: 3.06; lr: 0.00002015;   0/704 tok/s;  16540 sec
[2020-03-30 00:15:14,001 INFO] Loading train dataset from ../bert_data/cnndm.train.32.bert.pt, number of examples: 1998
[2020-03-30 00:16:29,490 INFO] Step 9900/210000; acc:  43.36; ppl: 22.77; xent: 3.13; lr: 0.00002010;   0/833 tok/s;  16624 sec
[2020-03-30 00:17:53,439 INFO] Step 9950/210000; acc:  49.28; ppl: 16.51; xent: 2.80; lr: 0.00002005;   0/868 tok/s;  16708 sec
[2020-03-30 00:18:01,031 INFO] Loading train dataset from ../bert_data/cnndm.train.33.bert.pt, number of examples: 1999
[2020-03-30 00:19:18,371 INFO] Step 10000/210000; acc:  50.91; ppl: 13.58; xent: 2.61; lr: 0.00002000;   0/1056 tok/s;  16793 sec
[2020-03-30 00:19:18,375 INFO] Saving checkpoint ../models/model_step_10000.pt
[2020-03-30 00:20:44,130 INFO] Step 10050/210000; acc:  45.40; ppl: 23.36; xent: 3.15; lr: 0.00001995;   0/811 tok/s;  16879 sec
[2020-03-30 00:20:51,599 INFO] Loading train dataset from ../bert_data/cnndm.train.34.bert.pt, number of examples: 2000
[2020-03-30 00:22:08,836 INFO] Step 10100/210000; acc:  43.38; ppl: 20.51; xent: 3.02; lr: 0.00001990;   0/1020 tok/s;  16964 sec
[2020-03-30 00:23:32,700 INFO] Step 10150/210000; acc:  43.28; ppl: 22.03; xent: 3.09; lr: 0.00001985;   0/1046 tok/s;  17048 sec
[2020-03-30 00:23:38,559 INFO] Loading train dataset from ../bert_data/cnndm.train.35.bert.pt, number of examples: 1998
[2020-03-30 00:24:57,665 INFO] Step 10200/210000; acc:  41.89; ppl: 20.46; xent: 3.02; lr: 0.00001980;   0/494 tok/s;  17132 sec
[2020-03-30 00:26:21,425 INFO] Step 10250/210000; acc:  42.17; ppl: 21.03; xent: 3.05; lr: 0.00001975;   0/939 tok/s;  17216 sec
[2020-03-30 00:26:25,522 INFO] Loading train dataset from ../bert_data/cnndm.train.36.bert.pt, number of examples: 2000
[2020-03-30 00:27:46,118 INFO] Step 10300/210000; acc:  44.01; ppl: 23.15; xent: 3.14; lr: 0.00001971;   0/771 tok/s;  17301 sec
[2020-03-30 00:29:10,366 INFO] Step 10350/210000; acc:  42.66; ppl: 24.03; xent: 3.18; lr: 0.00001966;   0/732 tok/s;  17385 sec
[2020-03-30 00:29:12,761 INFO] Loading train dataset from ../bert_data/cnndm.train.37.bert.pt, number of examples: 1999
[2020-03-30 00:30:35,013 INFO] Step 10400/210000; acc:  46.96; ppl: 17.10; xent: 2.84; lr: 0.00001961;   0/751 tok/s;  17470 sec
[2020-03-30 00:31:59,057 INFO] Step 10450/210000; acc:  45.60; ppl: 16.75; xent: 2.82; lr: 0.00001956;   0/698 tok/s;  17554 sec
[2020-03-30 00:32:01,583 INFO] Loading train dataset from ../bert_data/cnndm.train.38.bert.pt, number of examples: 2001
[2020-03-30 00:33:24,080 INFO] Step 10500/210000; acc:  42.34; ppl: 21.46; xent: 3.07; lr: 0.00001952;   0/853 tok/s;  17639 sec
[2020-03-30 00:33:24,084 INFO] Saving checkpoint ../models/model_step_10500.pt
[2020-03-30 00:34:50,297 INFO] Step 10550/210000; acc:  39.40; ppl: 23.62; xent: 3.16; lr: 0.00001947;   0/1000 tok/s;  17725 sec
[2020-03-30 00:34:51,154 INFO] Loading train dataset from ../bert_data/cnndm.train.39.bert.pt, number of examples: 2000
[2020-03-30 00:36:15,275 INFO] Step 10600/210000; acc:  51.20; ppl: 12.98; xent: 2.56; lr: 0.00001943;   0/1030 tok/s;  17810 sec
[2020-03-30 00:37:38,724 INFO] Loading train dataset from ../bert_data/cnndm.train.4.bert.pt, number of examples: 2001
[2020-03-30 00:37:40,429 INFO] Step 10650/210000; acc:  45.51; ppl: 17.22; xent: 2.85; lr: 0.00001938;   0/545 tok/s;  17895 sec
[2020-03-30 00:39:04,657 INFO] Step 10700/210000; acc:  44.87; ppl: 18.21; xent: 2.90; lr: 0.00001933;   0/1066 tok/s;  17979 sec
[2020-03-30 00:40:27,479 INFO] Loading train dataset from ../bert_data/cnndm.train.40.bert.pt, number of examples: 1999
[2020-03-30 00:40:29,262 INFO] Step 10750/210000; acc:  45.72; ppl: 17.13; xent: 2.84; lr: 0.00001929;   0/340 tok/s;  18064 sec
[2020-03-30 00:41:53,125 INFO] Step 10800/210000; acc:  41.31; ppl: 23.25; xent: 3.15; lr: 0.00001925;   0/516 tok/s;  18148 sec
[2020-03-30 00:43:14,539 INFO] Loading train dataset from ../bert_data/cnndm.train.41.bert.pt, number of examples: 2000
[2020-03-30 00:43:18,002 INFO] Step 10850/210000; acc:  48.69; ppl: 13.96; xent: 2.64; lr: 0.00001920;   0/579 tok/s;  18233 sec
[2020-03-30 00:44:41,720 INFO] Step 10900/210000; acc:  44.87; ppl: 17.78; xent: 2.88; lr: 0.00001916;   0/676 tok/s;  18317 sec
[2020-03-30 00:46:01,568 INFO] Loading train dataset from ../bert_data/cnndm.train.42.bert.pt, number of examples: 2000
[2020-03-30 00:46:06,647 INFO] Step 10950/210000; acc:  44.59; ppl: 18.74; xent: 2.93; lr: 0.00001911;   0/658 tok/s;  18401 sec
[2020-03-30 00:47:30,758 INFO] Step 11000/210000; acc:  49.87; ppl: 14.57; xent: 2.68; lr: 0.00001907;   0/701 tok/s;  18486 sec
[2020-03-30 00:47:30,762 INFO] Saving checkpoint ../models/model_step_11000.pt
[2020-03-30 00:48:51,501 INFO] Loading train dataset from ../bert_data/cnndm.train.43.bert.pt, number of examples: 2001
[2020-03-30 00:48:58,202 INFO] Step 11050/210000; acc:  44.23; ppl: 18.35; xent: 2.91; lr: 0.00001903;   0/722 tok/s;  18573 sec
[2020-03-30 00:50:22,382 INFO] Step 11100/210000; acc:  47.13; ppl: 17.57; xent: 2.87; lr: 0.00001898;   0/505 tok/s;  18657 sec
[2020-03-30 00:51:40,541 INFO] Loading train dataset from ../bert_data/cnndm.train.44.bert.pt, number of examples: 1998
[2020-03-30 00:51:47,295 INFO] Step 11150/210000; acc:  47.11; ppl: 16.21; xent: 2.79; lr: 0.00001894;   0/835 tok/s;  18742 sec
[2020-03-30 00:53:11,905 INFO] Step 11200/210000; acc:  46.63; ppl: 18.89; xent: 2.94; lr: 0.00001890;   0/833 tok/s;  18827 sec
[2020-03-30 00:54:29,459 INFO] Loading train dataset from ../bert_data/cnndm.train.45.bert.pt, number of examples: 2001
[2020-03-30 00:54:38,047 INFO] Step 11250/210000; acc:  44.49; ppl: 21.36; xent: 3.06; lr: 0.00001886;   0/1016 tok/s;  18913 sec
[2020-03-30 00:56:01,964 INFO] Step 11300/210000; acc:  44.61; ppl: 18.88; xent: 2.94; lr: 0.00001881;   0/946 tok/s;  18997 sec
[2020-03-30 00:57:16,899 INFO] Loading train dataset from ../bert_data/cnndm.train.46.bert.pt, number of examples: 2001
[2020-03-30 00:57:27,020 INFO] Step 11350/210000; acc:  41.18; ppl: 23.56; xent: 3.16; lr: 0.00001877;   0/646 tok/s;  19082 sec
[2020-03-30 00:58:50,436 INFO] Step 11400/210000; acc:  42.96; ppl: 19.56; xent: 2.97; lr: 0.00001873;   0/816 tok/s;  19165 sec
[2020-03-30 01:00:05,155 INFO] Loading train dataset from ../bert_data/cnndm.train.47.bert.pt, number of examples: 2000
[2020-03-30 01:00:15,376 INFO] Step 11450/210000; acc:  48.92; ppl: 15.06; xent: 2.71; lr: 0.00001869;   0/507 tok/s;  19250 sec
[2020-03-30 01:01:39,710 INFO] Step 11500/210000; acc:  42.62; ppl: 21.06; xent: 3.05; lr: 0.00001865;   0/1010 tok/s;  19335 sec
[2020-03-30 01:01:39,736 INFO] Saving checkpoint ../models/model_step_11500.pt
[2020-03-30 01:02:54,548 INFO] Loading train dataset from ../bert_data/cnndm.train.48.bert.pt, number of examples: 2000
[2020-03-30 01:03:06,329 INFO] Step 11550/210000; acc:  51.21; ppl: 12.97; xent: 2.56; lr: 0.00001861;   0/551 tok/s;  19421 sec
[2020-03-30 01:04:30,532 INFO] Step 11600/210000; acc:  48.54; ppl: 13.76; xent: 2.62; lr: 0.00001857;   0/478 tok/s;  19505 sec
[2020-03-30 01:05:42,139 INFO] Loading train dataset from ../bert_data/cnndm.train.49.bert.pt, number of examples: 2001
[2020-03-30 01:05:55,593 INFO] Step 11650/210000; acc:  43.39; ppl: 20.04; xent: 3.00; lr: 0.00001853;   0/705 tok/s;  19590 sec
[2020-03-30 01:07:19,564 INFO] Step 11700/210000; acc:  45.28; ppl: 18.35; xent: 2.91; lr: 0.00001849;   0/570 tok/s;  19674 sec
[2020-03-30 01:08:31,037 INFO] Loading train dataset from ../bert_data/cnndm.train.5.bert.pt, number of examples: 2001
[2020-03-30 01:08:44,349 INFO] Step 11750/210000; acc:  43.76; ppl: 21.76; xent: 3.08; lr: 0.00001845;   0/759 tok/s;  19759 sec
[2020-03-30 01:10:08,247 INFO] Step 11800/210000; acc:  48.36; ppl: 12.72; xent: 2.54; lr: 0.00001841;   0/737 tok/s;  19843 sec
[2020-03-30 01:11:17,890 INFO] Loading train dataset from ../bert_data/cnndm.train.50.bert.pt, number of examples: 2001
[2020-03-30 01:11:32,936 INFO] Step 11850/210000; acc:  41.99; ppl: 19.04; xent: 2.95; lr: 0.00001837;   0/732 tok/s;  19928 sec
[2020-03-30 01:12:56,920 INFO] Step 11900/210000; acc:  41.87; ppl: 23.61; xent: 3.16; lr: 0.00001833;   0/664 tok/s;  20012 sec
[2020-03-30 01:14:06,407 INFO] Loading train dataset from ../bert_data/cnndm.train.51.bert.pt, number of examples: 2000
[2020-03-30 01:14:21,588 INFO] Step 11950/210000; acc:  45.07; ppl: 18.17; xent: 2.90; lr: 0.00001830;   0/814 tok/s;  20096 sec
[2020-03-30 01:15:45,603 INFO] Step 12000/210000; acc:  47.41; ppl: 17.40; xent: 2.86; lr: 0.00001826;   0/765 tok/s;  20180 sec
[2020-03-30 01:15:45,629 INFO] Saving checkpoint ../models/model_step_12000.pt
[2020-03-30 01:16:55,563 INFO] Loading train dataset from ../bert_data/cnndm.train.52.bert.pt, number of examples: 2001
[2020-03-30 01:17:12,426 INFO] Step 12050/210000; acc:  43.18; ppl: 18.80; xent: 2.93; lr: 0.00001822;   0/976 tok/s;  20267 sec
[2020-03-30 01:18:36,464 INFO] Step 12100/210000; acc:  42.87; ppl: 20.24; xent: 3.01; lr: 0.00001818;   0/1004 tok/s;  20351 sec
[2020-03-30 01:19:42,730 INFO] Loading train dataset from ../bert_data/cnndm.train.53.bert.pt, number of examples: 1999
[2020-03-30 01:20:01,233 INFO] Step 12150/210000; acc:  41.52; ppl: 23.14; xent: 3.14; lr: 0.00001814;   0/1108 tok/s;  20436 sec
[2020-03-30 01:21:24,916 INFO] Step 12200/210000; acc:  45.74; ppl: 16.10; xent: 2.78; lr: 0.00001811;   0/958 tok/s;  20520 sec
[2020-03-30 01:22:32,057 INFO] Loading train dataset from ../bert_data/cnndm.train.54.bert.pt, number of examples: 2001
[2020-03-30 01:22:50,345 INFO] Step 12250/210000; acc:  39.00; ppl: 23.27; xent: 3.15; lr: 0.00001807;   0/622 tok/s;  20605 sec
[2020-03-30 01:24:14,571 INFO] Step 12300/210000; acc:  45.57; ppl: 17.62; xent: 2.87; lr: 0.00001803;   0/1234 tok/s;  20689 sec
[2020-03-30 01:25:19,110 INFO] Loading train dataset from ../bert_data/cnndm.train.55.bert.pt, number of examples: 1999
[2020-03-30 01:25:39,509 INFO] Step 12350/210000; acc:  45.42; ppl: 19.79; xent: 2.99; lr: 0.00001800;   0/558 tok/s;  20774 sec
[2020-03-30 01:27:03,677 INFO] Step 12400/210000; acc:  48.05; ppl: 15.34; xent: 2.73; lr: 0.00001796;   0/445 tok/s;  20858 sec
[2020-03-30 01:28:06,618 INFO] Loading train dataset from ../bert_data/cnndm.train.56.bert.pt, number of examples: 2001
[2020-03-30 01:28:28,365 INFO] Step 12450/210000; acc:  44.17; ppl: 18.05; xent: 2.89; lr: 0.00001792;   0/582 tok/s;  20943 sec
[2020-03-30 01:29:52,735 INFO] Step 12500/210000; acc:  42.07; ppl: 20.92; xent: 3.04; lr: 0.00001789;   0/648 tok/s;  21028 sec
[2020-03-30 01:29:52,738 INFO] Saving checkpoint ../models/model_step_12500.pt
[2020-03-30 01:30:56,249 INFO] Loading train dataset from ../bert_data/cnndm.train.57.bert.pt, number of examples: 1999
[2020-03-30 01:31:19,616 INFO] Step 12550/210000; acc:  42.79; ppl: 20.49; xent: 3.02; lr: 0.00001785;   0/748 tok/s;  21114 sec
[2020-03-30 01:32:43,998 INFO] Step 12600/210000; acc:  49.76; ppl: 14.18; xent: 2.65; lr: 0.00001782;   0/738 tok/s;  21199 sec
[2020-03-30 01:33:45,609 INFO] Loading train dataset from ../bert_data/cnndm.train.58.bert.pt, number of examples: 2000
[2020-03-30 01:34:09,033 INFO] Step 12650/210000; acc:  48.65; ppl: 15.52; xent: 2.74; lr: 0.00001778;   0/793 tok/s;  21284 sec
[2020-03-30 01:35:32,958 INFO] Step 12700/210000; acc:  46.35; ppl: 16.43; xent: 2.80; lr: 0.00001775;   0/774 tok/s;  21368 sec
[2020-03-30 01:36:32,888 INFO] Loading train dataset from ../bert_data/cnndm.train.59.bert.pt, number of examples: 2000
[2020-03-30 01:36:58,272 INFO] Step 12750/210000; acc:  42.95; ppl: 19.99; xent: 3.00; lr: 0.00001771;   0/943 tok/s;  21453 sec
[2020-03-30 01:38:22,636 INFO] Step 12800/210000; acc:  44.36; ppl: 18.84; xent: 2.94; lr: 0.00001768;   0/943 tok/s;  21537 sec
[2020-03-30 01:39:20,511 INFO] Loading train dataset from ../bert_data/cnndm.train.6.bert.pt, number of examples: 2001
[2020-03-30 01:39:47,674 INFO] Step 12850/210000; acc:  36.41; ppl: 33.98; xent: 3.53; lr: 0.00001764;   0/442 tok/s;  21622 sec
[2020-03-30 01:41:12,361 INFO] Step 12900/210000; acc:  49.07; ppl: 15.88; xent: 2.77; lr: 0.00001761;   0/519 tok/s;  21707 sec
[2020-03-30 01:42:08,761 INFO] Loading train dataset from ../bert_data/cnndm.train.60.bert.pt, number of examples: 2000
[2020-03-30 01:42:37,185 INFO] Step 12950/210000; acc:  52.23; ppl: 10.82; xent: 2.38; lr: 0.00001757;   0/569 tok/s;  21792 sec
[2020-03-30 01:44:01,669 INFO] Step 13000/210000; acc:  45.48; ppl: 16.15; xent: 2.78; lr: 0.00001754;   0/452 tok/s;  21876 sec
[2020-03-30 01:44:01,694 INFO] Saving checkpoint ../models/model_step_13000.pt
[2020-03-30 01:44:59,731 INFO] Loading train dataset from ../bert_data/cnndm.train.61.bert.pt, number of examples: 2001
[2020-03-30 01:45:30,191 INFO] Step 13050/210000; acc:  51.15; ppl: 12.37; xent: 2.51; lr: 0.00001751;   0/660 tok/s;  21965 sec
[2020-03-30 01:46:54,269 INFO] Step 13100/210000; acc:  48.08; ppl: 13.79; xent: 2.62; lr: 0.00001747;   0/689 tok/s;  22049 sec
[2020-03-30 01:47:46,985 INFO] Loading train dataset from ../bert_data/cnndm.train.62.bert.pt, number of examples: 2001
[2020-03-30 01:48:18,842 INFO] Step 13150/210000; acc:  46.72; ppl: 14.87; xent: 2.70; lr: 0.00001744;   0/724 tok/s;  22134 sec
[2020-03-30 01:49:43,247 INFO] Step 13200/210000; acc:  47.81; ppl: 16.37; xent: 2.80; lr: 0.00001741;   0/725 tok/s;  22218 sec
[2020-03-30 01:50:36,126 INFO] Loading train dataset from ../bert_data/cnndm.train.63.bert.pt, number of examples: 2001
[2020-03-30 01:51:08,206 INFO] Step 13250/210000; acc:  48.76; ppl: 14.29; xent: 2.66; lr: 0.00001737;   0/752 tok/s;  22303 sec
[2020-03-30 01:52:32,651 INFO] Step 13300/210000; acc:  45.68; ppl: 16.41; xent: 2.80; lr: 0.00001734;   0/489 tok/s;  22387 sec
[2020-03-30 01:53:23,767 INFO] Loading train dataset from ../bert_data/cnndm.train.64.bert.pt, number of examples: 2001
[2020-03-30 01:53:57,406 INFO] Step 13350/210000; acc:  41.95; ppl: 22.96; xent: 3.13; lr: 0.00001731;   0/830 tok/s;  22472 sec
[2020-03-30 01:55:21,636 INFO] Step 13400/210000; acc:  45.20; ppl: 19.01; xent: 2.94; lr: 0.00001728;   0/597 tok/s;  22556 sec
[2020-03-30 01:56:11,300 INFO] Loading train dataset from ../bert_data/cnndm.train.65.bert.pt, number of examples: 2000
[2020-03-30 01:56:46,712 INFO] Step 13450/210000; acc:  44.23; ppl: 20.32; xent: 3.01; lr: 0.00001725;   0/924 tok/s;  22642 sec
[2020-03-30 01:58:10,885 INFO] Step 13500/210000; acc:  47.42; ppl: 15.59; xent: 2.75; lr: 0.00001721;   0/1002 tok/s;  22726 sec
[2020-03-30 01:58:10,910 INFO] Saving checkpoint ../models/model_step_13500.pt
[2020-03-30 01:59:04,245 INFO] Loading train dataset from ../bert_data/cnndm.train.66.bert.pt, number of examples: 2001
[2020-03-30 01:59:39,663 INFO] Step 13550/210000; acc:  45.42; ppl: 16.14; xent: 2.78; lr: 0.00001718;   0/643 tok/s;  22814 sec
[2020-03-30 02:01:03,637 INFO] Step 13600/210000; acc:  47.71; ppl: 16.41; xent: 2.80; lr: 0.00001715;   0/1052 tok/s;  22898 sec
[2020-03-30 02:01:51,660 INFO] Loading train dataset from ../bert_data/cnndm.train.67.bert.pt, number of examples: 1999
[2020-03-30 02:02:28,917 INFO] Step 13650/210000; acc:  47.85; ppl: 12.62; xent: 2.54; lr: 0.00001712;   0/503 tok/s;  22984 sec
[2020-03-30 02:03:53,083 INFO] Step 13700/210000; acc:  40.49; ppl: 24.20; xent: 3.19; lr: 0.00001709;   0/1171 tok/s;  23068 sec
[2020-03-30 02:04:39,120 INFO] Loading train dataset from ../bert_data/cnndm.train.68.bert.pt, number of examples: 1999
[2020-03-30 02:05:17,538 INFO] Step 13750/210000; acc:  49.50; ppl: 13.95; xent: 2.64; lr: 0.00001706;   0/596 tok/s;  23152 sec
[2020-03-30 02:06:42,054 INFO] Step 13800/210000; acc:  46.25; ppl: 15.98; xent: 2.77; lr: 0.00001703;   0/490 tok/s;  23237 sec
[2020-03-30 02:07:26,610 INFO] Loading train dataset from ../bert_data/cnndm.train.69.bert.pt, number of examples: 2000
[2020-03-30 02:08:06,879 INFO] Step 13850/210000; acc:  46.44; ppl: 17.95; xent: 2.89; lr: 0.00001699;   0/765 tok/s;  23322 sec
[2020-03-30 02:09:30,952 INFO] Step 13900/210000; acc:  40.69; ppl: 24.19; xent: 3.19; lr: 0.00001696;   0/835 tok/s;  23406 sec
[2020-03-30 02:10:15,325 INFO] Loading train dataset from ../bert_data/cnndm.train.7.bert.pt, number of examples: 2001
[2020-03-30 02:10:55,598 INFO] Step 13950/210000; acc:  47.96; ppl: 14.36; xent: 2.66; lr: 0.00001693;   0/877 tok/s;  23490 sec
[2020-03-30 02:12:19,447 INFO] Step 14000/210000; acc:  45.69; ppl: 17.37; xent: 2.85; lr: 0.00001690;   0/842 tok/s;  23574 sec
[2020-03-30 02:12:19,472 INFO] Saving checkpoint ../models/model_step_14000.pt
[2020-03-30 02:13:09,571 INFO] Loading train dataset from ../bert_data/cnndm.train.70.bert.pt, number of examples: 2000
[2020-03-30 02:13:51,820 INFO] Step 14050/210000; acc:  43.36; ppl: 19.85; xent: 2.99; lr: 0.00001687;   0/1080 tok/s;  23667 sec
[2020-03-30 02:15:15,647 INFO] Step 14100/210000; acc:  45.97; ppl: 15.49; xent: 2.74; lr: 0.00001684;   0/1134 tok/s;  23750 sec
[2020-03-30 02:15:58,439 INFO] Loading train dataset from ../bert_data/cnndm.train.71.bert.pt, number of examples: 1999
[2020-03-30 02:16:40,359 INFO] Step 14150/210000; acc:  53.52; ppl: 10.91; xent: 2.39; lr: 0.00001681;   0/991 tok/s;  23835 sec
[2020-03-30 02:18:04,431 INFO] Step 14200/210000; acc:  42.74; ppl: 20.10; xent: 3.00; lr: 0.00001678;   0/1022 tok/s;  23919 sec
[2020-03-30 02:18:45,522 INFO] Loading train dataset from ../bert_data/cnndm.train.72.bert.pt, number of examples: 2000
[2020-03-30 02:19:29,121 INFO] Step 14250/210000; acc:  47.13; ppl: 15.80; xent: 2.76; lr: 0.00001675;   0/1007 tok/s;  24004 sec
[2020-03-30 02:20:53,481 INFO] Step 14300/210000; acc:  44.79; ppl: 17.19; xent: 2.84; lr: 0.00001672;   0/1140 tok/s;  24088 sec
[2020-03-30 02:21:32,841 INFO] Loading train dataset from ../bert_data/cnndm.train.73.bert.pt, number of examples: 2000
[2020-03-30 02:22:18,157 INFO] Step 14350/210000; acc:  53.37; ppl: 10.49; xent: 2.35; lr: 0.00001670;   0/593 tok/s;  24173 sec
[2020-03-30 02:23:41,970 INFO] Step 14400/210000; acc:  39.92; ppl: 21.83; xent: 3.08; lr: 0.00001667;   0/466 tok/s;  24257 sec
[2020-03-30 02:24:19,825 INFO] Loading train dataset from ../bert_data/cnndm.train.74.bert.pt, number of examples: 2000
[2020-03-30 02:25:06,905 INFO] Step 14450/210000; acc:  49.55; ppl: 12.25; xent: 2.51; lr: 0.00001664;   0/548 tok/s;  24342 sec
[2020-03-30 02:26:31,220 INFO] Step 14500/210000; acc:  40.70; ppl: 18.29; xent: 2.91; lr: 0.00001661;   0/750 tok/s;  24426 sec
[2020-03-30 02:26:31,244 INFO] Saving checkpoint ../models/model_step_14500.pt
[2020-03-30 02:27:13,364 INFO] Loading train dataset from ../bert_data/cnndm.train.75.bert.pt, number of examples: 1999
[2020-03-30 02:28:02,404 INFO] Step 14550/210000; acc:  48.83; ppl: 13.09; xent: 2.57; lr: 0.00001658;   0/595 tok/s;  24517 sec
[2020-03-30 02:29:26,521 INFO] Step 14600/210000; acc:  49.35; ppl: 13.27; xent: 2.59; lr: 0.00001655;   0/557 tok/s;  24601 sec
[2020-03-30 02:30:03,011 INFO] Loading train dataset from ../bert_data/cnndm.train.76.bert.pt, number of examples: 1999
[2020-03-30 02:30:51,716 INFO] Step 14650/210000; acc:  45.71; ppl: 16.13; xent: 2.78; lr: 0.00001652;   0/836 tok/s;  24687 sec
[2020-03-30 02:32:15,784 INFO] Step 14700/210000; acc:  50.24; ppl: 12.27; xent: 2.51; lr: 0.00001650;   0/747 tok/s;  24771 sec
[2020-03-30 02:32:50,045 INFO] Loading train dataset from ../bert_data/cnndm.train.77.bert.pt, number of examples: 2001
[2020-03-30 02:33:41,174 INFO] Step 14750/210000; acc:  44.49; ppl: 17.87; xent: 2.88; lr: 0.00001647;   0/921 tok/s;  24856 sec
[2020-03-30 02:35:05,295 INFO] Step 14800/210000; acc:  50.43; ppl: 12.08; xent: 2.49; lr: 0.00001644;   0/972 tok/s;  24940 sec
[2020-03-30 02:35:38,008 INFO] Loading train dataset from ../bert_data/cnndm.train.78.bert.pt, number of examples: 2001
[2020-03-30 02:36:29,906 INFO] Step 14850/210000; acc:  42.46; ppl: 19.18; xent: 2.95; lr: 0.00001641;   0/769 tok/s;  25025 sec
[2020-03-30 02:37:53,888 INFO] Step 14900/210000; acc:  47.32; ppl: 15.75; xent: 2.76; lr: 0.00001638;   0/738 tok/s;  25109 sec
[2020-03-30 02:38:26,912 INFO] Loading train dataset from ../bert_data/cnndm.train.79.bert.pt, number of examples: 1999
[2020-03-30 02:39:18,846 INFO] Step 14950/210000; acc:  42.78; ppl: 20.64; xent: 3.03; lr: 0.00001636;   0/641 tok/s;  25194 sec
[2020-03-30 02:40:43,336 INFO] Step 15000/210000; acc:  46.14; ppl: 13.90; xent: 2.63; lr: 0.00001633;   0/517 tok/s;  25278 sec
[2020-03-30 02:40:43,362 INFO] Saving checkpoint ../models/model_step_15000.pt
[2020-03-30 02:41:20,830 INFO] Loading train dataset from ../bert_data/cnndm.train.8.bert.pt, number of examples: 2000
[2020-03-30 02:42:14,839 INFO] Step 15050/210000; acc:  48.05; ppl: 13.62; xent: 2.61; lr: 0.00001630;   0/532 tok/s;  25370 sec
[2020-03-30 02:43:38,980 INFO] Step 15100/210000; acc:  49.51; ppl: 11.35; xent: 2.43; lr: 0.00001628;   0/562 tok/s;  25454 sec
[2020-03-30 02:44:08,415 INFO] Loading train dataset from ../bert_data/cnndm.train.80.bert.pt, number of examples: 1999
[2020-03-30 02:45:04,043 INFO] Step 15150/210000; acc:  46.46; ppl: 14.72; xent: 2.69; lr: 0.00001625;   0/763 tok/s;  25539 sec
[2020-03-30 02:46:28,052 INFO] Step 15200/210000; acc:  51.16; ppl: 11.23; xent: 2.42; lr: 0.00001622;   0/600 tok/s;  25623 sec
[2020-03-30 02:46:55,724 INFO] Loading train dataset from ../bert_data/cnndm.train.81.bert.pt, number of examples: 2000
[2020-03-30 02:47:53,219 INFO] Step 15250/210000; acc:  48.89; ppl: 14.40; xent: 2.67; lr: 0.00001620;   0/794 tok/s;  25708 sec
[2020-03-30 02:49:17,187 INFO] Step 15300/210000; acc:  45.45; ppl: 15.44; xent: 2.74; lr: 0.00001617;   0/712 tok/s;  25792 sec
[2020-03-30 02:49:43,026 INFO] Loading train dataset from ../bert_data/cnndm.train.82.bert.pt, number of examples: 2001
[2020-03-30 02:50:41,761 INFO] Step 15350/210000; acc:  47.22; ppl: 14.38; xent: 2.67; lr: 0.00001614;   0/942 tok/s;  25877 sec
[2020-03-30 02:52:06,049 INFO] Step 15400/210000; acc:  46.03; ppl: 15.25; xent: 2.72; lr: 0.00001612;   0/878 tok/s;  25961 sec
[2020-03-30 02:52:30,448 INFO] Loading train dataset from ../bert_data/cnndm.train.83.bert.pt, number of examples: 1999
[2020-03-30 02:53:31,447 INFO] Step 15450/210000; acc:  44.69; ppl: 17.13; xent: 2.84; lr: 0.00001609;   0/1022 tok/s;  26046 sec
[2020-03-30 02:54:55,646 INFO] Step 15500/210000; acc:  45.89; ppl: 16.21; xent: 2.79; lr: 0.00001606;   0/748 tok/s;  26130 sec
[2020-03-30 02:54:55,668 INFO] Saving checkpoint ../models/model_step_15500.pt
[2020-03-30 02:55:20,466 INFO] Loading train dataset from ../bert_data/cnndm.train.84.bert.pt, number of examples: 2000
[2020-03-30 02:56:22,734 INFO] Step 15550/210000; acc:  47.74; ppl: 12.91; xent: 2.56; lr: 0.00001604;   0/549 tok/s;  26218 sec
[2020-03-30 02:57:47,056 INFO] Step 15600/210000; acc:  47.39; ppl: 14.64; xent: 2.68; lr: 0.00001601;   0/573 tok/s;  26302 sec
[2020-03-30 02:58:08,141 INFO] Loading train dataset from ../bert_data/cnndm.train.85.bert.pt, number of examples: 2001
[2020-03-30 02:59:12,199 INFO] Step 15650/210000; acc:  48.65; ppl: 14.28; xent: 2.66; lr: 0.00001599;   0/567 tok/s;  26387 sec
[2020-03-30 03:00:36,120 INFO] Step 15700/210000; acc:  49.90; ppl: 12.72; xent: 2.54; lr: 0.00001596;   0/621 tok/s;  26471 sec
[2020-03-30 03:00:56,990 INFO] Loading train dataset from ../bert_data/cnndm.train.86.bert.pt, number of examples: 1999
[2020-03-30 03:02:01,207 INFO] Step 15750/210000; acc:  48.01; ppl: 15.52; xent: 2.74; lr: 0.00001594;   0/589 tok/s;  26556 sec
[2020-03-30 03:03:25,572 INFO] Step 15800/210000; acc:  50.45; ppl: 13.88; xent: 2.63; lr: 0.00001591;   0/602 tok/s;  26640 sec
[2020-03-30 03:03:45,519 INFO] Loading train dataset from ../bert_data/cnndm.train.87.bert.pt, number of examples: 1999
[2020-03-30 03:04:51,226 INFO] Step 15850/210000; acc:  47.74; ppl: 17.26; xent: 2.85; lr: 0.00001589;   0/840 tok/s;  26726 sec
[2020-03-30 03:06:15,813 INFO] Step 15900/210000; acc:  48.74; ppl: 13.80; xent: 2.62; lr: 0.00001586;   0/827 tok/s;  26811 sec
[2020-03-30 03:06:33,988 INFO] Loading train dataset from ../bert_data/cnndm.train.88.bert.pt, number of examples: 1999
[2020-03-30 03:07:41,175 INFO] Step 15950/210000; acc:  42.86; ppl: 19.32; xent: 2.96; lr: 0.00001584;   0/984 tok/s;  26896 sec
[2020-03-30 03:09:04,940 INFO] Step 16000/210000; acc:  48.03; ppl: 14.65; xent: 2.68; lr: 0.00001581;   0/990 tok/s;  26980 sec
[2020-03-30 03:09:04,963 INFO] Saving checkpoint ../models/model_step_16000.pt
[2020-03-30 03:09:22,965 INFO] Loading train dataset from ../bert_data/cnndm.train.89.bert.pt, number of examples: 2001
[2020-03-30 03:10:31,395 INFO] Step 16050/210000; acc:  47.08; ppl: 17.20; xent: 2.85; lr: 0.00001579;   0/1054 tok/s;  27066 sec
[2020-03-30 03:11:55,582 INFO] Step 16100/210000; acc:  45.78; ppl: 15.76; xent: 2.76; lr: 0.00001576;   0/1202 tok/s;  27150 sec
[2020-03-30 03:12:09,747 INFO] Loading train dataset from ../bert_data/cnndm.train.9.bert.pt, number of examples: 1999
[2020-03-30 03:13:20,592 INFO] Step 16150/210000; acc:  51.28; ppl: 13.14; xent: 2.58; lr: 0.00001574;   0/549 tok/s;  27235 sec
[2020-03-30 03:14:44,723 INFO] Step 16200/210000; acc:  50.17; ppl: 12.73; xent: 2.54; lr: 0.00001571;   0/567 tok/s;  27320 sec
[2020-03-30 03:14:58,979 INFO] Loading train dataset from ../bert_data/cnndm.train.90.bert.pt, number of examples: 2000
[2020-03-30 03:16:10,305 INFO] Step 16250/210000; acc:  50.61; ppl: 12.65; xent: 2.54; lr: 0.00001569;   0/636 tok/s;  27405 sec
[2020-03-30 03:17:35,069 INFO] Step 16300/210000; acc:  43.72; ppl: 18.40; xent: 2.91; lr: 0.00001567;   0/537 tok/s;  27490 sec
[2020-03-30 03:17:47,313 INFO] Loading train dataset from ../bert_data/cnndm.train.9000.bert.pt, number of examples: 1984
[2020-03-30 03:18:57,826 INFO] Step 16350/210000; acc:  33.62; ppl: 33.82; xent: 3.52; lr: 0.00001564;   0/1312 tok/s;  27573 sec
[2020-03-30 03:20:19,468 INFO] Step 16400/210000; acc:  38.80; ppl: 28.37; xent: 3.35; lr: 0.00001562;   0/861 tok/s;  27654 sec
[2020-03-30 03:20:21,784 INFO] Loading train dataset from ../bert_data/cnndm.train.9001.bert.pt, number of examples: 1983
[2020-03-30 03:21:42,203 INFO] Step 16450/210000; acc:  36.33; ppl: 29.22; xent: 3.37; lr: 0.00001559;   0/782 tok/s;  27737 sec
[2020-03-30 03:22:57,624 INFO] Loading train dataset from ../bert_data/cnndm.train.9002.bert.pt, number of examples: 1990
[2020-03-30 03:23:04,368 INFO] Step 16500/210000; acc:  41.33; ppl: 19.95; xent: 2.99; lr: 0.00001557;   0/899 tok/s;  27819 sec
[2020-03-30 03:23:04,370 INFO] Saving checkpoint ../models/model_step_16500.pt
[2020-03-30 03:24:28,235 INFO] Step 16550/210000; acc:  44.27; ppl: 14.79; xent: 2.69; lr: 0.00001555;   0/518 tok/s;  27903 sec
[2020-03-30 03:25:35,634 INFO] Loading train dataset from ../bert_data/cnndm.train.9003.bert.pt, number of examples: 1985
[2020-03-30 03:25:50,513 INFO] Step 16600/210000; acc:  35.07; ppl: 26.26; xent: 3.27; lr: 0.00001552;   0/1092 tok/s;  27985 sec
[2020-03-30 03:27:12,938 INFO] Step 16650/210000; acc:  44.48; ppl: 13.07; xent: 2.57; lr: 0.00001550;   0/402 tok/s;  28068 sec
[2020-03-30 03:28:12,520 INFO] Loading train dataset from ../bert_data/cnndm.train.9004.bert.pt, number of examples: 1981
[2020-03-30 03:28:35,557 INFO] Step 16700/210000; acc:  39.66; ppl: 22.11; xent: 3.10; lr: 0.00001548;   0/1200 tok/s;  28150 sec
[2020-03-30 03:29:57,253 INFO] Step 16750/210000; acc:  51.26; ppl:  9.65; xent: 2.27; lr: 0.00001545;   0/455 tok/s;  28232 sec
[2020-03-30 03:30:48,462 INFO] Loading train dataset from ../bert_data/cnndm.train.9005.bert.pt, number of examples: 1986
[2020-03-30 03:31:19,556 INFO] Step 16800/210000; acc:  38.60; ppl: 21.60; xent: 3.07; lr: 0.00001543;   0/1319 tok/s;  28314 sec
[2020-03-30 03:32:41,554 INFO] Step 16850/210000; acc:  43.62; ppl: 12.59; xent: 2.53; lr: 0.00001541;   0/518 tok/s;  28396 sec
[2020-03-30 03:33:25,033 INFO] Loading train dataset from ../bert_data/cnndm.train.9006.bert.pt, number of examples: 1993
[2020-03-30 03:34:04,314 INFO] Step 16900/210000; acc:  35.22; ppl: 28.09; xent: 3.34; lr: 0.00001538;   0/1463 tok/s;  28479 sec
[2020-03-30 03:35:26,519 INFO] Step 16950/210000; acc:  41.74; ppl: 16.98; xent: 2.83; lr: 0.00001536;   0/634 tok/s;  28561 sec
[2020-03-30 03:36:02,052 INFO] Loading train dataset from ../bert_data/cnndm.train.9007.bert.pt, number of examples: 1983
[2020-03-30 03:36:50,159 INFO] Step 17000/210000; acc:  32.97; ppl: 30.23; xent: 3.41; lr: 0.00001534;   0/1383 tok/s;  28645 sec
[2020-03-30 03:36:50,162 INFO] Saving checkpoint ../models/model_step_17000.pt
[2020-03-30 03:38:14,520 INFO] Step 17050/210000; acc:  43.99; ppl: 13.00; xent: 2.56; lr: 0.00001532;   0/614 tok/s;  28729 sec
[2020-03-30 03:38:42,035 INFO] Loading train dataset from ../bert_data/cnndm.train.9008.bert.pt, number of examples: 1990
[2020-03-30 03:39:37,675 INFO] Step 17100/210000; acc:  42.82; ppl: 17.37; xent: 2.86; lr: 0.00001529;   0/624 tok/s;  28812 sec
[2020-03-30 03:41:00,093 INFO] Step 17150/210000; acc:  43.66; ppl: 16.48; xent: 2.80; lr: 0.00001527;   0/775 tok/s;  28895 sec
[2020-03-30 03:41:19,327 INFO] Loading train dataset from ../bert_data/cnndm.train.9009.bert.pt, number of examples: 1988
[2020-03-30 03:42:23,734 INFO] Step 17200/210000; acc:  45.50; ppl: 13.15; xent: 2.58; lr: 0.00001525;   0/528 tok/s;  28979 sec
[2020-03-30 03:43:46,484 INFO] Step 17250/210000; acc:  40.97; ppl: 17.27; xent: 2.85; lr: 0.00001523;   0/972 tok/s;  29061 sec
[2020-03-30 03:43:55,455 INFO] Loading train dataset from ../bert_data/cnndm.train.9010.bert.pt, number of examples: 1984
[2020-03-30 03:45:09,364 INFO] Step 17300/210000; acc:  47.44; ppl: 11.12; xent: 2.41; lr: 0.00001521;   0/506 tok/s;  29144 sec
[2020-03-30 03:46:31,070 INFO] Step 17350/210000; acc:  36.90; ppl: 22.00; xent: 3.09; lr: 0.00001518;   0/1394 tok/s;  29226 sec
[2020-03-30 03:46:31,654 INFO] Loading train dataset from ../bert_data/cnndm.train.9011.bert.pt, number of examples: 1983
[2020-03-30 03:47:54,138 INFO] Step 17400/210000; acc:  47.72; ppl: 11.74; xent: 2.46; lr: 0.00001516;   0/542 tok/s;  29309 sec
[2020-03-30 03:49:07,928 INFO] Loading train dataset from ../bert_data/cnndm.train.9012.bert.pt, number of examples: 1985
[2020-03-30 03:49:16,161 INFO] Step 17450/210000; acc:  38.73; ppl: 20.45; xent: 3.02; lr: 0.00001514;   0/1378 tok/s;  29391 sec
[2020-03-30 03:50:38,205 INFO] Step 17500/210000; acc:  47.43; ppl: 12.06; xent: 2.49; lr: 0.00001512;   0/473 tok/s;  29473 sec
[2020-03-30 03:50:38,208 INFO] Saving checkpoint ../models/model_step_17500.pt
[2020-03-30 03:51:46,963 INFO] Loading train dataset from ../bert_data/cnndm.train.9013.bert.pt, number of examples: 1987
[2020-03-30 03:52:03,683 INFO] Step 17550/210000; acc:  39.31; ppl: 22.29; xent: 3.10; lr: 0.00001510;   0/1102 tok/s;  29559 sec
[2020-03-30 03:53:25,414 INFO] Step 17600/210000; acc:  43.76; ppl: 14.15; xent: 2.65; lr: 0.00001508;   0/657 tok/s;  29640 sec
[2020-03-30 03:54:22,995 INFO] Loading train dataset from ../bert_data/cnndm.train.9014.bert.pt, number of examples: 1976
[2020-03-30 03:54:47,666 INFO] Step 17650/210000; acc:  44.19; ppl: 14.61; xent: 2.68; lr: 0.00001505;   0/696 tok/s;  29722 sec
[2020-03-30 03:56:09,985 INFO] Step 17700/210000; acc:  43.83; ppl: 14.20; xent: 2.65; lr: 0.00001503;   0/821 tok/s;  29805 sec
[2020-03-30 03:56:56,825 INFO] Loading train dataset from ../bert_data/cnndm.train.9015.bert.pt, number of examples: 1982
[2020-03-30 03:57:33,478 INFO] Step 17750/210000; acc:  43.72; ppl: 14.14; xent: 2.65; lr: 0.00001501;   0/671 tok/s;  29888 sec
[2020-03-30 03:58:54,831 INFO] Step 17800/210000; acc:  36.81; ppl: 26.73; xent: 3.29; lr: 0.00001499;   0/1502 tok/s;  29970 sec
[2020-03-30 03:59:33,093 INFO] Loading train dataset from ../bert_data/cnndm.train.9016.bert.pt, number of examples: 1984
[2020-03-30 04:00:16,777 INFO] Step 17850/210000; acc:  48.45; ppl: 11.95; xent: 2.48; lr: 0.00001497;   0/665 tok/s;  30052 sec
[2020-03-30 04:01:38,563 INFO] Step 17900/210000; acc:  40.70; ppl: 17.41; xent: 2.86; lr: 0.00001495;   0/1405 tok/s;  30133 sec
[2020-03-30 04:02:08,623 INFO] Loading train dataset from ../bert_data/cnndm.train.9017.bert.pt, number of examples: 1984
[2020-03-30 04:03:01,350 INFO] Step 17950/210000; acc:  40.26; ppl: 16.35; xent: 2.79; lr: 0.00001493;   0/839 tok/s;  30216 sec
[2020-03-30 04:04:23,244 INFO] Step 18000/210000; acc:  37.04; ppl: 25.84; xent: 3.25; lr: 0.00001491;   0/1580 tok/s;  30298 sec
[2020-03-30 04:04:23,248 INFO] Saving checkpoint ../models/model_step_18000.pt
[2020-03-30 04:04:46,978 INFO] Loading train dataset from ../bert_data/cnndm.train.9018.bert.pt, number of examples: 1989
[2020-03-30 04:05:47,289 INFO] Step 18050/210000; acc:  45.36; ppl: 13.69; xent: 2.62; lr: 0.00001489;   0/782 tok/s;  30382 sec
[2020-03-30 04:07:09,311 INFO] Step 18100/210000; acc:  36.86; ppl: 24.50; xent: 3.20; lr: 0.00001487;   0/1092 tok/s;  30464 sec
[2020-03-30 04:07:22,828 INFO] Loading train dataset from ../bert_data/cnndm.train.9019.bert.pt, number of examples: 1986
[2020-03-30 04:08:31,411 INFO] Step 18150/210000; acc:  45.65; ppl: 13.87; xent: 2.63; lr: 0.00001485;   0/762 tok/s;  30546 sec
[2020-03-30 04:09:53,244 INFO] Step 18200/210000; acc:  43.77; ppl: 16.37; xent: 2.80; lr: 0.00001482;   0/763 tok/s;  30628 sec
[2020-03-30 04:09:58,598 INFO] Loading train dataset from ../bert_data/cnndm.train.9020.bert.pt, number of examples: 1992
[2020-03-30 04:11:14,941 INFO] Step 18250/210000; acc:  41.53; ppl: 16.69; xent: 2.81; lr: 0.00001480;   0/1095 tok/s;  30710 sec
[2020-03-30 04:12:33,879 INFO] Loading train dataset from ../bert_data/cnndm.train.9021.bert.pt, number of examples: 1988
[2020-03-30 04:12:37,162 INFO] Step 18300/210000; acc:  51.31; ppl:  9.25; xent: 2.22; lr: 0.00001478;   0/442 tok/s;  30792 sec
[2020-03-30 04:13:58,522 INFO] Step 18350/210000; acc:  48.96; ppl: 12.11; xent: 2.49; lr: 0.00001476;   0/718 tok/s;  30873 sec
[2020-03-30 04:15:11,226 INFO] Loading train dataset from ../bert_data/cnndm.train.9022.bert.pt, number of examples: 1991
[2020-03-30 04:15:21,109 INFO] Step 18400/210000; acc:  52.55; ppl:  8.68; xent: 2.16; lr: 0.00001474;   0/379 tok/s;  30956 sec
[2020-03-30 04:16:42,241 INFO] Step 18450/210000; acc:  44.77; ppl: 13.18; xent: 2.58; lr: 0.00001472;   0/720 tok/s;  31037 sec
[2020-03-30 04:17:46,261 INFO] Loading train dataset from ../bert_data/cnndm.train.9023.bert.pt, number of examples: 1986
[2020-03-30 04:18:04,023 INFO] Step 18500/210000; acc:  45.77; ppl: 12.45; xent: 2.52; lr: 0.00001470;   0/561 tok/s;  31119 sec
[2020-03-30 04:18:04,026 INFO] Saving checkpoint ../models/model_step_18500.pt
[2020-03-30 04:19:27,623 INFO] Step 18550/210000; acc:  44.35; ppl: 13.81; xent: 2.63; lr: 0.00001468;   0/807 tok/s;  31202 sec
[2020-03-30 04:20:24,522 INFO] Loading train dataset from ../bert_data/cnndm.train.9024.bert.pt, number of examples: 1989
[2020-03-30 04:20:51,000 INFO] Step 18600/210000; acc:  53.67; ppl:  7.90; xent: 2.07; lr: 0.00001466;   0/360 tok/s;  31286 sec
[2020-03-30 04:22:12,386 INFO] Step 18650/210000; acc:  42.13; ppl: 15.60; xent: 2.75; lr: 0.00001465;   0/876 tok/s;  31367 sec
[2020-03-30 04:23:00,359 INFO] Loading train dataset from ../bert_data/cnndm.train.9025.bert.pt, number of examples: 1984
[2020-03-30 04:23:34,728 INFO] Step 18700/210000; acc:  52.90; ppl:  9.06; xent: 2.20; lr: 0.00001463;   0/411 tok/s;  31450 sec
[2020-03-30 04:24:56,215 INFO] Step 18750/210000; acc:  41.46; ppl: 17.10; xent: 2.84; lr: 0.00001461;   0/1026 tok/s;  31531 sec
[2020-03-30 04:25:36,374 INFO] Loading train dataset from ../bert_data/cnndm.train.9026.bert.pt, number of examples: 1986
[2020-03-30 04:26:19,240 INFO] Step 18800/210000; acc:  48.85; ppl: 10.40; xent: 2.34; lr: 0.00001459;   0/559 tok/s;  31614 sec
[2020-03-30 04:27:41,264 INFO] Step 18850/210000; acc:  38.47; ppl: 21.68; xent: 3.08; lr: 0.00001457;   0/1119 tok/s;  31696 sec
[2020-03-30 04:28:13,909 INFO] Loading train dataset from ../bert_data/cnndm.train.9027.bert.pt, number of examples: 1989
[2020-03-30 04:29:04,319 INFO] Step 18900/210000; acc:  48.48; ppl: 11.17; xent: 2.41; lr: 0.00001455;   0/707 tok/s;  31779 sec
[2020-03-30 04:30:26,512 INFO] Step 18950/210000; acc:  39.26; ppl: 19.00; xent: 2.94; lr: 0.00001453;   0/1462 tok/s;  31861 sec
[2020-03-30 04:30:50,040 INFO] Loading train dataset from ../bert_data/cnndm.train.9028.bert.pt, number of examples: 1986
[2020-03-30 04:31:49,652 INFO] Step 19000/210000; acc:  47.93; ppl: 10.77; xent: 2.38; lr: 0.00001451;   0/599 tok/s;  31944 sec
[2020-03-30 04:31:49,656 INFO] Saving checkpoint ../models/model_step_19000.pt
[2020-03-30 04:33:13,471 INFO] Step 19050/210000; acc:  43.15; ppl: 15.85; xent: 2.76; lr: 0.00001449;   0/1255 tok/s;  32028 sec
[2020-03-30 04:33:28,549 INFO] Loading train dataset from ../bert_data/cnndm.train.9029.bert.pt, number of examples: 1981
[2020-03-30 04:34:35,309 INFO] Step 19100/210000; acc:  50.05; ppl: 10.06; xent: 2.31; lr: 0.00001447;   0/666 tok/s;  32110 sec
[2020-03-30 04:35:58,175 INFO] Step 19150/210000; acc:  36.48; ppl: 24.18; xent: 3.19; lr: 0.00001445;   0/1485 tok/s;  32193 sec
[2020-03-30 04:36:05,826 INFO] Loading train dataset from ../bert_data/cnndm.train.9030.bert.pt, number of examples: 1984
[2020-03-30 04:37:20,820 INFO] Step 19200/210000; acc:  49.74; ppl:  9.85; xent: 2.29; lr: 0.00001443;   0/732 tok/s;  32276 sec
[2020-03-30 04:38:40,455 INFO] Loading train dataset from ../bert_data/cnndm.train.9031.bert.pt, number of examples: 1975
[2020-03-30 04:38:43,861 INFO] Step 19250/210000; acc:  51.82; ppl:  8.20; xent: 2.10; lr: 0.00001441;   0/440 tok/s;  32359 sec
[2020-03-30 04:40:05,612 INFO] Step 19300/210000; acc:  46.13; ppl: 13.52; xent: 2.60; lr: 0.00001440;   0/965 tok/s;  32440 sec
[2020-03-30 04:41:15,878 INFO] Loading train dataset from ../bert_data/cnndm.train.9032.bert.pt, number of examples: 1986
[2020-03-30 04:41:28,712 INFO] Step 19350/210000; acc:  44.73; ppl: 14.15; xent: 2.65; lr: 0.00001438;   0/658 tok/s;  32524 sec
[2020-03-30 04:42:50,825 INFO] Step 19400/210000; acc:  37.10; ppl: 21.27; xent: 3.06; lr: 0.00001436;   0/975 tok/s;  32606 sec
[2020-03-30 04:43:52,049 INFO] Loading train dataset from ../bert_data/cnndm.train.9033.bert.pt, number of examples: 1986
[2020-03-30 04:44:13,433 INFO] Step 19450/210000; acc:  44.28; ppl: 13.31; xent: 2.59; lr: 0.00001434;   0/902 tok/s;  32688 sec
[2020-03-30 04:45:34,890 INFO] Step 19500/210000; acc:  46.10; ppl: 13.08; xent: 2.57; lr: 0.00001432;   0/794 tok/s;  32770 sec
[2020-03-30 04:45:34,893 INFO] Saving checkpoint ../models/model_step_19500.pt
[2020-03-30 04:46:30,459 INFO] Loading train dataset from ../bert_data/cnndm.train.9034.bert.pt, number of examples: 1989
[2020-03-30 04:46:59,156 INFO] Step 19550/210000; acc:  41.30; ppl: 15.70; xent: 2.75; lr: 0.00001430;   0/1007 tok/s;  32854 sec
[2020-03-30 04:48:21,225 INFO] Step 19600/210000; acc:  48.23; ppl:  9.49; xent: 2.25; lr: 0.00001429;   0/382 tok/s;  32936 sec
[2020-03-30 04:49:06,225 INFO] Loading train dataset from ../bert_data/cnndm.train.9035.bert.pt, number of examples: 1988
[2020-03-30 04:49:43,531 INFO] Step 19650/210000; acc:  42.78; ppl: 15.85; xent: 2.76; lr: 0.00001427;   0/1155 tok/s;  33018 sec
[2020-03-30 04:51:04,677 INFO] Step 19700/210000; acc:  49.39; ppl:  8.71; xent: 2.16; lr: 0.00001425;   0/436 tok/s;  33099 sec
[2020-03-30 04:51:41,380 INFO] Loading train dataset from ../bert_data/cnndm.train.9036.bert.pt, number of examples: 1984
[2020-03-30 04:52:27,402 INFO] Step 19750/210000; acc:  39.48; ppl: 20.03; xent: 3.00; lr: 0.00001423;   0/1492 tok/s;  33182 sec
[2020-03-30 04:53:50,312 INFO] Step 19800/210000; acc:  48.11; ppl:  9.55; xent: 2.26; lr: 0.00001421;   0/555 tok/s;  33265 sec
[2020-03-30 04:54:15,359 INFO] Loading train dataset from ../bert_data/cnndm.train.9037.bert.pt, number of examples: 1983
[2020-03-30 04:55:13,225 INFO] Step 19850/210000; acc:  53.61; ppl:  8.46; xent: 2.14; lr: 0.00001420;   0/424 tok/s;  33348 sec
[2020-03-30 04:56:34,843 INFO] Step 19900/210000; acc:  44.86; ppl: 13.77; xent: 2.62; lr: 0.00001418;   0/1113 tok/s;  33430 sec
[2020-03-30 04:56:51,522 INFO] Loading train dataset from ../bert_data/cnndm.train.9038.bert.pt, number of examples: 1982
[2020-03-30 04:57:57,420 INFO] Step 19950/210000; acc:  49.02; ppl:  9.85; xent: 2.29; lr: 0.00001416;   0/442 tok/s;  33512 sec
[2020-03-30 04:59:20,213 INFO] Step 20000/210000; acc:  40.04; ppl: 16.96; xent: 2.83; lr: 0.00001414;   0/937 tok/s;  33595 sec
[2020-03-30 04:59:20,217 INFO] Saving checkpoint ../models/model_step_20000.pt
[2020-03-30 04:59:31,023 INFO] Loading train dataset from ../bert_data/cnndm.train.9039.bert.pt, number of examples: 1983
[2020-03-30 05:00:43,947 INFO] Step 20050/210000; acc:  49.34; ppl: 10.13; xent: 2.32; lr: 0.00001412;   0/572 tok/s;  33679 sec
[2020-03-30 05:02:03,906 INFO] Loading train dataset from ../bert_data/cnndm.train.9040.bert.pt, number of examples: 1982
[2020-03-30 05:02:05,989 INFO] Step 20100/210000; acc:  38.70; ppl: 22.50; xent: 3.11; lr: 0.00001411;   0/783 tok/s;  33761 sec
[2020-03-30 05:03:28,188 INFO] Step 20150/210000; acc:  44.93; ppl: 13.00; xent: 2.56; lr: 0.00001409;   0/797 tok/s;  33843 sec
[2020-03-30 05:04:38,423 INFO] Loading train dataset from ../bert_data/cnndm.train.9041.bert.pt, number of examples: 1986
[2020-03-30 05:04:50,377 INFO] Step 20200/210000; acc:  52.11; ppl:  8.01; xent: 2.08; lr: 0.00001407;   0/508 tok/s;  33925 sec
[2020-03-30 05:06:11,761 INFO] Step 20250/210000; acc:  39.06; ppl: 17.44; xent: 2.86; lr: 0.00001405;   0/1083 tok/s;  34007 sec
[2020-03-30 05:07:15,543 INFO] Loading train dataset from ../bert_data/cnndm.train.9042.bert.pt, number of examples: 1984
[2020-03-30 05:07:33,376 INFO] Step 20300/210000; acc:  53.46; ppl:  8.05; xent: 2.09; lr: 0.00001404;   0/438 tok/s;  34088 sec
[2020-03-30 05:08:54,279 INFO] Step 20350/210000; acc:  42.60; ppl: 15.39; xent: 2.73; lr: 0.00001402;   0/1013 tok/s;  34169 sec
[2020-03-30 05:09:50,558 INFO] Loading train dataset from ../bert_data/cnndm.train.9043.bert.pt, number of examples: 1984
[2020-03-30 05:10:17,060 INFO] Step 20400/210000; acc:  50.39; ppl:  8.57; xent: 2.15; lr: 0.00001400;   0/425 tok/s;  34252 sec
[2020-03-30 05:11:39,373 INFO] Step 20450/210000; acc:  42.73; ppl: 15.21; xent: 2.72; lr: 0.00001399;   0/1181 tok/s;  34334 sec
[2020-03-30 05:12:28,303 INFO] Loading train dataset from ../bert_data/cnndm.train.9044.bert.pt, number of examples: 1988
[2020-03-30 05:13:02,431 INFO] Step 20500/210000; acc:  48.69; ppl: 10.55; xent: 2.36; lr: 0.00001397;   0/487 tok/s;  34417 sec
[2020-03-30 05:13:02,435 INFO] Saving checkpoint ../models/model_step_20500.pt
[2020-03-30 05:14:26,563 INFO] Step 20550/210000; acc:  47.24; ppl: 11.87; xent: 2.47; lr: 0.00001395;   0/1389 tok/s;  34501 sec
[2020-03-30 05:15:05,189 INFO] Loading train dataset from ../bert_data/cnndm.train.9045.bert.pt, number of examples: 1983
[2020-03-30 05:15:49,433 INFO] Step 20600/210000; acc:  47.04; ppl: 10.77; xent: 2.38; lr: 0.00001393;   0/622 tok/s;  34584 sec
[2020-03-30 05:17:11,314 INFO] Step 20650/210000; acc:  41.08; ppl: 17.74; xent: 2.88; lr: 0.00001392;   0/1421 tok/s;  34666 sec
[2020-03-30 05:17:41,314 INFO] Loading train dataset from ../bert_data/cnndm.train.9046.bert.pt, number of examples: 1983
[2020-03-30 05:18:33,944 INFO] Step 20700/210000; acc:  43.47; ppl: 15.17; xent: 2.72; lr: 0.00001390;   0/817 tok/s;  34749 sec
[2020-03-30 05:19:56,483 INFO] Step 20750/210000; acc:  46.89; ppl: 13.20; xent: 2.58; lr: 0.00001388;   0/618 tok/s;  34831 sec
[2020-03-30 05:20:16,943 INFO] Loading train dataset from ../bert_data/cnndm.train.9047.bert.pt, number of examples: 1983
[2020-03-30 05:21:19,604 INFO] Step 20800/210000; acc:  47.62; ppl: 11.45; xent: 2.44; lr: 0.00001387;   0/902 tok/s;  34914 sec
[2020-03-30 05:22:41,741 INFO] Step 20850/210000; acc:  51.90; ppl:  8.53; xent: 2.14; lr: 0.00001385;   0/428 tok/s;  34997 sec
[2020-03-30 05:22:53,875 INFO] Loading train dataset from ../bert_data/cnndm.train.9048.bert.pt, number of examples: 1984
[2020-03-30 05:24:04,546 INFO] Step 20900/210000; acc:  43.25; ppl: 13.64; xent: 2.61; lr: 0.00001383;   0/956 tok/s;  35079 sec
[2020-03-30 05:25:26,937 INFO] Step 20950/210000; acc:  51.44; ppl:  9.24; xent: 2.22; lr: 0.00001382;   0/473 tok/s;  35162 sec
[2020-03-30 05:25:30,835 INFO] Loading train dataset from ../bert_data/cnndm.train.9049.bert.pt, number of examples: 1978
[2020-03-30 05:26:50,568 INFO] Step 21000/210000; acc:  41.20; ppl: 15.55; xent: 2.74; lr: 0.00001380;   0/1388 tok/s;  35245 sec
[2020-03-30 05:26:50,571 INFO] Saving checkpoint ../models/model_step_21000.pt
[2020-03-30 05:28:09,954 INFO] Loading train dataset from ../bert_data/cnndm.train.9050.bert.pt, number of examples: 1978
[2020-03-30 05:28:14,588 INFO] Step 21050/210000; acc:  42.33; ppl: 15.42; xent: 2.74; lr: 0.00001378;   0/1016 tok/s;  35329 sec
[2020-03-30 05:29:36,707 INFO] Step 21100/210000; acc:  38.61; ppl: 18.96; xent: 2.94; lr: 0.00001377;   0/1073 tok/s;  35412 sec
[2020-03-30 05:30:44,776 INFO] Loading train dataset from ../bert_data/cnndm.train.9051.bert.pt, number of examples: 1981
[2020-03-30 05:30:59,572 INFO] Step 21150/210000; acc:  45.91; ppl: 11.27; xent: 2.42; lr: 0.00001375;   0/893 tok/s;  35494 sec
[2020-03-30 05:32:21,330 INFO] Step 21200/210000; acc:  52.20; ppl:  9.27; xent: 2.23; lr: 0.00001374;   0/413 tok/s;  35576 sec
[2020-03-30 05:33:20,324 INFO] Loading train dataset from ../bert_data/cnndm.train.9052.bert.pt, number of examples: 1981
[2020-03-30 05:33:43,130 INFO] Step 21250/210000; acc:  42.94; ppl: 16.29; xent: 2.79; lr: 0.00001372;   0/1226 tok/s;  35658 sec
[2020-03-30 05:35:05,866 INFO] Step 21300/210000; acc:  49.53; ppl: 10.42; xent: 2.34; lr: 0.00001370;   0/691 tok/s;  35741 sec
[2020-03-30 05:35:55,174 INFO] Loading train dataset from ../bert_data/cnndm.train.9053.bert.pt, number of examples: 1981
[2020-03-30 05:36:27,664 INFO] Step 21350/210000; acc:  42.06; ppl: 16.11; xent: 2.78; lr: 0.00001369;   0/713 tok/s;  35822 sec
[2020-03-30 05:37:49,625 INFO] Step 21400/210000; acc:  44.29; ppl: 13.46; xent: 2.60; lr: 0.00001367;   0/701 tok/s;  35904 sec
[2020-03-30 05:38:29,485 INFO] Loading train dataset from ../bert_data/cnndm.train.9054.bert.pt, number of examples: 1987
[2020-03-30 05:39:12,110 INFO] Step 21450/210000; acc:  48.34; ppl: 10.01; xent: 2.30; lr: 0.00001366;   0/411 tok/s;  35987 sec
[2020-03-30 05:40:33,666 INFO] Step 21500/210000; acc:  46.86; ppl: 11.00; xent: 2.40; lr: 0.00001364;   0/793 tok/s;  36068 sec
[2020-03-30 05:40:33,670 INFO] Saving checkpoint ../models/model_step_21500.pt
[2020-03-30 05:41:10,948 INFO] Loading train dataset from ../bert_data/cnndm.train.9055.bert.pt, number of examples: 1984
[2020-03-30 05:42:00,381 INFO] Step 21550/210000; acc:  51.12; ppl:  8.81; xent: 2.18; lr: 0.00001362;   0/475 tok/s;  36155 sec
[2020-03-30 05:43:22,510 INFO] Step 21600/210000; acc:  44.25; ppl: 13.80; xent: 2.62; lr: 0.00001361;   0/1181 tok/s;  36237 sec
[2020-03-30 05:43:46,203 INFO] Loading train dataset from ../bert_data/cnndm.train.9056.bert.pt, number of examples: 1985
[2020-03-30 05:44:45,061 INFO] Step 21650/210000; acc:  52.24; ppl:  8.14; xent: 2.10; lr: 0.00001359;   0/542 tok/s;  36320 sec
[2020-03-30 05:46:07,035 INFO] Step 21700/210000; acc:  40.40; ppl: 17.25; xent: 2.85; lr: 0.00001358;   0/1448 tok/s;  36402 sec
[2020-03-30 05:46:22,406 INFO] Loading train dataset from ../bert_data/cnndm.train.9057.bert.pt, number of examples: 1986
[2020-03-30 05:47:29,686 INFO] Step 21750/210000; acc:  52.57; ppl:  8.27; xent: 2.11; lr: 0.00001356;   0/550 tok/s;  36485 sec
[2020-03-30 05:48:52,927 INFO] Step 21800/210000; acc:  35.82; ppl: 22.44; xent: 3.11; lr: 0.00001355;   0/1126 tok/s;  36568 sec
[2020-03-30 05:48:58,141 INFO] Loading train dataset from ../bert_data/cnndm.train.9058.bert.pt, number of examples: 1979
[2020-03-30 05:50:15,571 INFO] Step 21850/210000; acc:  44.18; ppl: 13.35; xent: 2.59; lr: 0.00001353;   0/1026 tok/s;  36650 sec
[2020-03-30 05:51:34,216 INFO] Loading train dataset from ../bert_data/cnndm.train.9059.bert.pt, number of examples: 1987
[2020-03-30 05:51:37,771 INFO] Step 21900/210000; acc:  45.73; ppl: 10.78; xent: 2.38; lr: 0.00001351;   0/548 tok/s;  36733 sec
[2020-03-30 05:52:59,686 INFO] Step 21950/210000; acc:  43.28; ppl: 14.20; xent: 2.65; lr: 0.00001350;   0/1301 tok/s;  36815 sec
[2020-03-30 05:54:10,944 INFO] Loading train dataset from ../bert_data/cnndm.train.9060.bert.pt, number of examples: 1992
[2020-03-30 05:54:22,282 INFO] Step 22000/210000; acc:  48.11; ppl: 11.01; xent: 2.40; lr: 0.00001348;   0/593 tok/s;  36897 sec
[2020-03-30 05:54:22,285 INFO] Saving checkpoint ../models/model_step_22000.pt
[2020-03-30 05:55:45,895 INFO] Step 22050/210000; acc:  41.15; ppl: 17.21; xent: 2.85; lr: 0.00001347;   0/1470 tok/s;  36981 sec
[2020-03-30 05:56:49,255 INFO] Loading train dataset from ../bert_data/cnndm.train.9061.bert.pt, number of examples: 1985
[2020-03-30 05:57:08,996 INFO] Step 22100/210000; acc:  52.87; ppl:  7.73; xent: 2.05; lr: 0.00001345;   0/566 tok/s;  37064 sec
[2020-03-30 05:58:30,785 INFO] Step 22150/210000; acc:  44.75; ppl: 13.39; xent: 2.59; lr: 0.00001344;   0/1160 tok/s;  37146 sec
[2020-03-30 05:59:25,768 INFO] Loading train dataset from ../bert_data/cnndm.train.9062.bert.pt, number of examples: 1982
[2020-03-30 05:59:53,571 INFO] Step 22200/210000; acc:  49.01; ppl:  9.24; xent: 2.22; lr: 0.00001342;   0/507 tok/s;  37228 sec
[2020-03-30 06:01:15,043 INFO] Step 22250/210000; acc:  44.63; ppl: 14.04; xent: 2.64; lr: 0.00001341;   0/1257 tok/s;  37310 sec
[2020-03-30 06:02:01,275 INFO] Loading train dataset from ../bert_data/cnndm.train.9063.bert.pt, number of examples: 1988
[2020-03-30 06:02:37,549 INFO] Step 22300/210000; acc:  53.87; ppl:  8.30; xent: 2.12; lr: 0.00001339;   0/614 tok/s;  37392 sec
[2020-03-30 06:03:59,873 INFO] Step 22350/210000; acc:  38.05; ppl: 21.49; xent: 3.07; lr: 0.00001338;   0/1579 tok/s;  37475 sec
[2020-03-30 06:04:38,622 INFO] Loading train dataset from ../bert_data/cnndm.train.9064.bert.pt, number of examples: 1984
[2020-03-30 06:05:22,931 INFO] Step 22400/210000; acc:  46.18; ppl: 11.38; xent: 2.43; lr: 0.00001336;   0/652 tok/s;  37558 sec
[2020-03-30 06:06:44,419 INFO] Step 22450/210000; acc:  40.51; ppl: 15.85; xent: 2.76; lr: 0.00001335;   0/1411 tok/s;  37639 sec
[2020-03-30 06:07:14,053 INFO] Loading train dataset from ../bert_data/cnndm.train.9065.bert.pt, number of examples: 1986
[2020-03-30 06:08:06,781 INFO] Step 22500/210000; acc:  43.21; ppl: 15.66; xent: 2.75; lr: 0.00001333;   0/1004 tok/s;  37722 sec
[2020-03-30 06:08:06,785 INFO] Saving checkpoint ../models/model_step_22500.pt
[2020-03-30 06:09:30,521 INFO] Step 22550/210000; acc:  39.68; ppl: 17.53; xent: 2.86; lr: 0.00001332;   0/811 tok/s;  37805 sec
[2020-03-30 06:09:52,047 INFO] Loading train dataset from ../bert_data/cnndm.train.9066.bert.pt, number of examples: 1988
[2020-03-30 06:10:52,705 INFO] Step 22600/210000; acc:  45.80; ppl: 11.17; xent: 2.41; lr: 0.00001330;   0/891 tok/s;  37888 sec
[2020-03-30 06:12:14,479 INFO] Step 22650/210000; acc:  49.02; ppl: 11.02; xent: 2.40; lr: 0.00001329;   0/537 tok/s;  37969 sec
[2020-03-30 06:12:28,250 INFO] Loading train dataset from ../bert_data/cnndm.train.9067.bert.pt, number of examples: 1986
[2020-03-30 06:13:36,478 INFO] Step 22700/210000; acc:  45.98; ppl: 12.62; xent: 2.54; lr: 0.00001327;   0/1036 tok/s;  38051 sec
[2020-03-30 06:14:58,783 INFO] Step 22750/210000; acc:  44.25; ppl: 13.17; xent: 2.58; lr: 0.00001326;   0/581 tok/s;  38134 sec
[2020-03-30 06:15:04,224 INFO] Loading train dataset from ../bert_data/cnndm.train.9068.bert.pt, number of examples: 1988
[2020-03-30 06:16:21,288 INFO] Step 22800/210000; acc:  46.65; ppl: 10.51; xent: 2.35; lr: 0.00001325;   0/1137 tok/s;  38216 sec
[2020-03-30 06:17:40,429 INFO] Loading train dataset from ../bert_data/cnndm.train.9069.bert.pt, number of examples: 1982
[2020-03-30 06:17:43,675 INFO] Step 22850/210000; acc:  53.31; ppl:  7.70; xent: 2.04; lr: 0.00001323;   0/505 tok/s;  38298 sec
[2020-03-30 06:19:05,242 INFO] Step 22900/210000; acc:  40.34; ppl: 16.21; xent: 2.79; lr: 0.00001322;   0/1529 tok/s;  38380 sec
[2020-03-30 06:20:14,835 INFO] Loading train dataset from ../bert_data/cnndm.train.9070.bert.pt, number of examples: 1988
[2020-03-30 06:20:27,818 INFO] Step 22950/210000; acc:  46.20; ppl: 12.00; xent: 2.49; lr: 0.00001320;   0/784 tok/s;  38463 sec
[2020-03-30 06:21:49,869 INFO] Step 23000/210000; acc:  40.41; ppl: 16.71; xent: 2.82; lr: 0.00001319;   0/850 tok/s;  38545 sec
[2020-03-30 06:21:49,873 INFO] Saving checkpoint ../models/model_step_23000.pt
[2020-03-30 06:22:53,537 INFO] Loading train dataset from ../bert_data/cnndm.train.9071.bert.pt, number of examples: 1988
[2020-03-30 06:23:14,707 INFO] Step 23050/210000; acc:  46.38; ppl: 12.85; xent: 2.55; lr: 0.00001317;   0/888 tok/s;  38630 sec
[2020-03-30 06:24:37,359 INFO] Step 23100/210000; acc:  42.12; ppl: 16.06; xent: 2.78; lr: 0.00001316;   0/825 tok/s;  38712 sec
[2020-03-30 06:25:30,101 INFO] Loading train dataset from ../bert_data/cnndm.train.9072.bert.pt, number of examples: 1992
[2020-03-30 06:25:59,513 INFO] Step 23150/210000; acc:  41.39; ppl: 16.67; xent: 2.81; lr: 0.00001314;   0/1017 tok/s;  38794 sec
[2020-03-30 06:27:21,403 INFO] Step 23200/210000; acc:  48.42; ppl: 10.99; xent: 2.40; lr: 0.00001313;   0/536 tok/s;  38876 sec
[2020-03-30 06:28:06,270 INFO] Loading train dataset from ../bert_data/cnndm.train.9073.bert.pt, number of examples: 1987
[2020-03-30 06:28:43,827 INFO] Step 23250/210000; acc:  45.43; ppl: 12.70; xent: 2.54; lr: 0.00001312;   0/1089 tok/s;  38959 sec
[2020-03-30 06:30:06,860 INFO] Step 23300/210000; acc:  48.59; ppl: 10.56; xent: 2.36; lr: 0.00001310;   0/411 tok/s;  39042 sec
[2020-03-30 06:30:43,236 INFO] Loading train dataset from ../bert_data/cnndm.train.9074.bert.pt, number of examples: 1991
[2020-03-30 06:31:29,459 INFO] Step 23350/210000; acc:  49.38; ppl: 11.14; xent: 2.41; lr: 0.00001309;   0/1164 tok/s;  39124 sec
[2020-03-30 06:32:51,469 INFO] Step 23400/210000; acc:  48.85; ppl:  8.54; xent: 2.15; lr: 0.00001307;   0/492 tok/s;  39206 sec
[2020-03-30 06:33:19,710 INFO] Loading train dataset from ../bert_data/cnndm.train.9075.bert.pt, number of examples: 1983
[2020-03-30 06:34:13,506 INFO] Step 23450/210000; acc:  42.05; ppl: 15.28; xent: 2.73; lr: 0.00001306;   0/1337 tok/s;  39288 sec
[2020-03-30 06:35:34,979 INFO] Step 23500/210000; acc:  49.56; ppl: 10.20; xent: 2.32; lr: 0.00001305;   0/521 tok/s;  39370 sec
[2020-03-30 06:35:34,983 INFO] Saving checkpoint ../models/model_step_23500.pt
[2020-03-30 06:35:57,053 INFO] Loading train dataset from ../bert_data/cnndm.train.9076.bert.pt, number of examples: 1986
[2020-03-30 06:36:58,932 INFO] Step 23550/210000; acc:  42.62; ppl: 15.28; xent: 2.73; lr: 0.00001303;   0/1290 tok/s;  39454 sec
[2020-03-30 06:38:20,887 INFO] Step 23600/210000; acc:  48.14; ppl: 10.66; xent: 2.37; lr: 0.00001302;   0/556 tok/s;  39536 sec
[2020-03-30 06:38:31,185 INFO] Loading train dataset from ../bert_data/cnndm.train.9077.bert.pt, number of examples: 1990
[2020-03-30 06:39:43,560 INFO] Step 23650/210000; acc:  42.04; ppl: 15.58; xent: 2.75; lr: 0.00001301;   0/1121 tok/s;  39618 sec
[2020-03-30 06:41:05,617 INFO] Step 23700/210000; acc:  42.43; ppl: 13.89; xent: 2.63; lr: 0.00001299;   0/749 tok/s;  39700 sec
[2020-03-30 06:41:07,889 INFO] Loading train dataset from ../bert_data/cnndm.train.9078.bert.pt, number of examples: 1992
[2020-03-30 06:42:27,613 INFO] Step 23750/210000; acc:  40.34; ppl: 16.77; xent: 2.82; lr: 0.00001298;   0/1317 tok/s;  39782 sec
[2020-03-30 06:43:45,196 INFO] Loading train dataset from ../bert_data/cnndm.train.9079.bert.pt, number of examples: 1984
[2020-03-30 06:43:50,234 INFO] Step 23800/210000; acc:  49.89; ppl: 10.31; xent: 2.33; lr: 0.00001296;   0/570 tok/s;  39865 sec
[2020-03-30 06:45:12,300 INFO] Step 23850/210000; acc:  41.86; ppl: 15.81; xent: 2.76; lr: 0.00001295;   0/1509 tok/s;  39947 sec
[2020-03-30 06:46:21,384 INFO] Loading train dataset from ../bert_data/cnndm.train.9080.bert.pt, number of examples: 1988
[2020-03-30 06:46:34,693 INFO] Step 23900/210000; acc:  42.03; ppl: 14.60; xent: 2.68; lr: 0.00001294;   0/814 tok/s;  40030 sec
[2020-03-30 06:47:56,549 INFO] Step 23950/210000; acc:  54.21; ppl:  7.77; xent: 2.05; lr: 0.00001292;   0/362 tok/s;  40111 sec
[2020-03-30 06:48:56,183 INFO] Loading train dataset from ../bert_data/cnndm.train.9081.bert.pt, number of examples: 1991
[2020-03-30 06:49:19,247 INFO] Step 24000/210000; acc:  43.91; ppl: 12.81; xent: 2.55; lr: 0.00001291;   0/1014 tok/s;  40194 sec
[2020-03-30 06:49:19,251 INFO] Saving checkpoint ../models/model_step_24000.pt
[2020-03-30 06:50:43,905 INFO] Step 24050/210000; acc:  56.13; ppl:  7.43; xent: 2.01; lr: 0.00001290;   0/453 tok/s;  40279 sec
[2020-03-30 06:51:35,090 INFO] Loading train dataset from ../bert_data/cnndm.train.9082.bert.pt, number of examples: 1985
[2020-03-30 06:52:05,878 INFO] Step 24100/210000; acc:  42.68; ppl: 15.79; xent: 2.76; lr: 0.00001288;   0/1083 tok/s;  40361 sec
[2020-03-30 06:53:27,777 INFO] Step 24150/210000; acc:  51.43; ppl:  9.10; xent: 2.21; lr: 0.00001287;   0/477 tok/s;  40443 sec
[2020-03-30 06:54:10,686 INFO] Loading train dataset from ../bert_data/cnndm.train.9083.bert.pt, number of examples: 1986
[2020-03-30 06:54:49,828 INFO] Step 24200/210000; acc:  43.78; ppl: 13.90; xent: 2.63; lr: 0.00001286;   0/1403 tok/s;  40525 sec
[2020-03-30 06:56:12,179 INFO] Step 24250/210000; acc:  48.38; ppl: 10.32; xent: 2.33; lr: 0.00001284;   0/479 tok/s;  40607 sec
[2020-03-30 06:56:47,383 INFO] Loading train dataset from ../bert_data/cnndm.train.9084.bert.pt, number of examples: 1982
[2020-03-30 06:57:35,388 INFO] Step 24300/210000; acc:  39.80; ppl: 18.36; xent: 2.91; lr: 0.00001283;   0/1376 tok/s;  40690 sec
[2020-03-30 06:58:57,246 INFO] Step 24350/210000; acc:  49.30; ppl: 10.15; xent: 2.32; lr: 0.00001282;   0/501 tok/s;  40772 sec
[2020-03-30 06:59:23,896 INFO] Loading train dataset from ../bert_data/cnndm.train.9085.bert.pt, number of examples: 1985
[2020-03-30 07:00:19,379 INFO] Step 24400/210000; acc:  39.28; ppl: 17.38; xent: 2.86; lr: 0.00001280;   0/1463 tok/s;  40854 sec
[2020-03-30 07:01:41,674 INFO] Step 24450/210000; acc:  48.26; ppl: 10.10; xent: 2.31; lr: 0.00001279;   0/622 tok/s;  40936 sec
[2020-03-30 07:02:00,570 INFO] Loading train dataset from ../bert_data/cnndm.train.9086.bert.pt, number of examples: 1983
[2020-03-30 07:03:04,803 INFO] Step 24500/210000; acc:  44.30; ppl: 16.69; xent: 2.82; lr: 0.00001278;   0/960 tok/s;  41020 sec
[2020-03-30 07:03:04,807 INFO] Saving checkpoint ../models/model_step_24500.pt
[2020-03-30 07:04:29,387 INFO] Step 24550/210000; acc:  48.35; ppl: 11.09; xent: 2.41; lr: 0.00001276;   0/642 tok/s;  41104 sec
[2020-03-30 07:04:39,881 INFO] Loading train dataset from ../bert_data/cnndm.train.9087.bert.pt, number of examples: 1986
[2020-03-30 07:05:52,168 INFO] Step 24600/210000; acc:  47.55; ppl: 12.14; xent: 2.50; lr: 0.00001275;   0/539 tok/s;  41187 sec
[2020-03-30 07:07:14,114 INFO] Step 24650/210000; acc:  46.25; ppl: 12.03; xent: 2.49; lr: 0.00001274;   0/812 tok/s;  41269 sec
[2020-03-30 07:07:16,361 INFO] Loading train dataset from ../bert_data/cnndm.train.9088.bert.pt, number of examples: 1986
[2020-03-30 07:08:36,897 INFO] Step 24700/210000; acc:  52.54; ppl:  8.24; xent: 2.11; lr: 0.00001273;   0/389 tok/s;  41352 sec
[2020-03-30 07:09:52,732 INFO] Loading train dataset from ../bert_data/cnndm.train.9089.bert.pt, number of examples: 1986
[2020-03-30 07:09:59,156 INFO] Step 24750/210000; acc:  41.44; ppl: 17.24; xent: 2.85; lr: 0.00001271;   0/1015 tok/s;  41434 sec
[2020-03-30 07:11:21,518 INFO] Step 24800/210000; acc:  48.18; ppl:  9.28; xent: 2.23; lr: 0.00001270;   0/423 tok/s;  41516 sec
[2020-03-30 07:12:28,552 INFO] Loading train dataset from ../bert_data/cnndm.train.9090.bert.pt, number of examples: 20
[2020-03-30 07:12:30,549 INFO] Loading train dataset from ../bert_data/cnndm.train.9100.bert.pt, number of examples: 1988
[2020-03-30 07:12:42,053 INFO] Step 24850/210000; acc:  31.66; ppl: 28.55; xent: 3.35; lr: 0.00001269;   0/744 tok/s;  41597 sec
[2020-03-30 07:13:53,667 INFO] Step 24900/210000; acc:  45.63; ppl: 13.83; xent: 2.63; lr: 0.00001267;   0/520 tok/s;  41668 sec
[2020-03-30 07:14:45,654 INFO] Loading train dataset from ../bert_data/cnndm.train.9101.bert.pt, number of examples: 1983
[2020-03-30 07:15:05,635 INFO] Step 24950/210000; acc:  49.86; ppl: 10.11; xent: 2.31; lr: 0.00001266;   0/533 tok/s;  41740 sec
[2020-03-30 07:16:17,698 INFO] Step 25000/210000; acc:  46.58; ppl: 11.67; xent: 2.46; lr: 0.00001265;   0/374 tok/s;  41813 sec
[2020-03-30 07:16:17,701 INFO] Saving checkpoint ../models/model_step_25000.pt
[2020-03-30 07:17:07,043 INFO] Loading train dataset from ../bert_data/cnndm.train.9102.bert.pt, number of examples: 1992
[2020-03-30 07:17:35,146 INFO] Step 25050/210000; acc:  47.65; ppl: 11.69; xent: 2.46; lr: 0.00001264;   0/608 tok/s;  41890 sec
[2020-03-30 07:18:47,835 INFO] Step 25100/210000; acc:  48.06; ppl: 12.28; xent: 2.51; lr: 0.00001262;   0/544 tok/s;  41963 sec
[2020-03-30 07:19:26,123 INFO] Loading train dataset from ../bert_data/cnndm.train.9103.bert.pt, number of examples: 1978
[2020-03-30 07:20:00,777 INFO] Step 25150/210000; acc:  47.97; ppl: 10.01; xent: 2.30; lr: 0.00001261;   0/551 tok/s;  42036 sec
[2020-03-30 07:21:13,685 INFO] Step 25200/210000; acc:  48.36; ppl: 10.88; xent: 2.39; lr: 0.00001260;   0/458 tok/s;  42109 sec
[2020-03-30 07:21:43,014 INFO] Loading train dataset from ../bert_data/cnndm.train.9104.bert.pt, number of examples: 1986
[2020-03-30 07:22:26,546 INFO] Step 25250/210000; acc:  52.58; ppl:  7.44; xent: 2.01; lr: 0.00001259;   0/519 tok/s;  42181 sec
[2020-03-30 07:23:38,985 INFO] Step 25300/210000; acc:  46.21; ppl: 11.42; xent: 2.44; lr: 0.00001257;   0/700 tok/s;  42254 sec
[2020-03-30 07:23:59,467 INFO] Loading train dataset from ../bert_data/cnndm.train.9105.bert.pt, number of examples: 1988
[2020-03-30 07:24:51,006 INFO] Step 25350/210000; acc:  49.65; ppl:  9.93; xent: 2.30; lr: 0.00001256;   0/793 tok/s;  42326 sec
[2020-03-30 07:26:03,738 INFO] Step 25400/210000; acc:  49.94; ppl:  8.25; xent: 2.11; lr: 0.00001255;   0/491 tok/s;  42399 sec
[2020-03-30 07:26:16,838 INFO] Loading train dataset from ../bert_data/cnndm.train.9106.bert.pt, number of examples: 1978
[2020-03-30 07:27:17,196 INFO] Step 25450/210000; acc:  54.27; ppl:  8.30; xent: 2.12; lr: 0.00001254;   0/498 tok/s;  42472 sec
[2020-03-30 07:28:29,782 INFO] Step 25500/210000; acc:  53.42; ppl:  7.92; xent: 2.07; lr: 0.00001252;   0/394 tok/s;  42545 sec
[2020-03-30 07:28:29,784 INFO] Saving checkpoint ../models/model_step_25500.pt
[2020-03-30 07:28:37,951 INFO] Loading train dataset from ../bert_data/cnndm.train.9107.bert.pt, number of examples: 1985
[2020-03-30 07:29:44,628 INFO] Step 25550/210000; acc:  53.99; ppl:  8.29; xent: 2.11; lr: 0.00001251;   0/402 tok/s;  42619 sec
[2020-03-30 07:30:55,506 INFO] Loading train dataset from ../bert_data/cnndm.train.9108.bert.pt, number of examples: 1989
[2020-03-30 07:30:57,063 INFO] Step 25600/210000; acc:  52.60; ppl:  7.67; xent: 2.04; lr: 0.00001250;   0/317 tok/s;  42692 sec
[2020-03-30 07:32:08,993 INFO] Step 25650/210000; acc:  56.10; ppl:  7.52; xent: 2.02; lr: 0.00001249;   0/510 tok/s;  42764 sec
[2020-03-30 07:33:10,939 INFO] Loading train dataset from ../bert_data/cnndm.train.9109.bert.pt, number of examples: 1987
[2020-03-30 07:33:20,943 INFO] Step 25700/210000; acc:  53.16; ppl:  8.75; xent: 2.17; lr: 0.00001248;   0/588 tok/s;  42836 sec
[2020-03-30 07:34:33,836 INFO] Step 25750/210000; acc:  56.24; ppl:  6.53; xent: 1.88; lr: 0.00001246;   0/474 tok/s;  42909 sec
[2020-03-30 07:35:29,094 INFO] Loading train dataset from ../bert_data/cnndm.train.9110.bert.pt, number of examples: 1985
[2020-03-30 07:35:46,459 INFO] Step 25800/210000; acc:  54.87; ppl:  7.41; xent: 2.00; lr: 0.00001245;   0/473 tok/s;  42981 sec
[2020-03-30 07:36:59,089 INFO] Step 25850/210000; acc:  61.14; ppl:  5.08; xent: 1.63; lr: 0.00001244;   0/391 tok/s;  43054 sec
[2020-03-30 07:37:46,945 INFO] Loading train dataset from ../bert_data/cnndm.train.9111.bert.pt, number of examples: 1985
[2020-03-30 07:38:12,119 INFO] Step 25900/210000; acc:  54.38; ppl:  7.63; xent: 2.03; lr: 0.00001243;   0/681 tok/s;  43127 sec
[2020-03-30 07:39:24,070 INFO] Step 25950/210000; acc:  52.48; ppl:  8.37; xent: 2.12; lr: 0.00001242;   0/574 tok/s;  43199 sec
[2020-03-30 07:40:05,477 INFO] Loading train dataset from ../bert_data/cnndm.train.9112.bert.pt, number of examples: 1983
[2020-03-30 07:40:37,585 INFO] Step 26000/210000; acc:  50.36; ppl:  8.87; xent: 2.18; lr: 0.00001240;   0/496 tok/s;  43272 sec
[2020-03-30 07:40:37,595 INFO] Saving checkpoint ../models/model_step_26000.pt
[2020-03-30 07:41:51,722 INFO] Step 26050/210000; acc:  50.56; ppl:  9.35; xent: 2.24; lr: 0.00001239;   0/740 tok/s;  43347 sec
[2020-03-30 07:42:24,195 INFO] Loading train dataset from ../bert_data/cnndm.train.9113.bert.pt, number of examples: 1979
[2020-03-30 07:43:04,760 INFO] Step 26100/210000; acc:  61.01; ppl:  4.62; xent: 1.53; lr: 0.00001238;   0/413 tok/s;  43420 sec
[2020-03-30 07:44:16,907 INFO] Step 26150/210000; acc:  58.14; ppl:  6.68; xent: 1.90; lr: 0.00001237;   0/578 tok/s;  43492 sec
[2020-03-30 07:44:42,629 INFO] Loading train dataset from ../bert_data/cnndm.train.9114.bert.pt, number of examples: 1989
[2020-03-30 07:45:30,814 INFO] Step 26200/210000; acc:  48.40; ppl:  9.55; xent: 2.26; lr: 0.00001236;   0/621 tok/s;  43566 sec
[2020-03-30 07:46:43,352 INFO] Step 26250/210000; acc:  62.23; ppl:  5.17; xent: 1.64; lr: 0.00001234;   0/515 tok/s;  43638 sec
[2020-03-30 07:46:59,781 INFO] Loading train dataset from ../bert_data/cnndm.train.9115.bert.pt, number of examples: 1989
[2020-03-30 07:47:55,714 INFO] Step 26300/210000; acc:  56.18; ppl:  7.08; xent: 1.96; lr: 0.00001233;   0/465 tok/s;  43711 sec
[2020-03-30 07:49:07,808 INFO] Step 26350/210000; acc:  54.23; ppl:  7.84; xent: 2.06; lr: 0.00001232;   0/665 tok/s;  43783 sec
[2020-03-30 07:49:16,715 INFO] Loading train dataset from ../bert_data/cnndm.train.9116.bert.pt, number of examples: 1985
[2020-03-30 07:50:20,762 INFO] Step 26400/210000; acc:  52.35; ppl:  8.97; xent: 2.19; lr: 0.00001231;   0/683 tok/s;  43856 sec
[2020-03-30 07:51:33,754 INFO] Step 26450/210000; acc:  50.75; ppl:  9.38; xent: 2.24; lr: 0.00001230;   0/698 tok/s;  43929 sec
[2020-03-30 07:51:34,120 INFO] Loading train dataset from ../bert_data/cnndm.train.9117.bert.pt, number of examples: 1979
[2020-03-30 07:52:46,778 INFO] Step 26500/210000; acc:  59.46; ppl:  6.06; xent: 1.80; lr: 0.00001229;   0/639 tok/s;  44002 sec
[2020-03-30 07:52:46,782 INFO] Saving checkpoint ../models/model_step_26500.pt
[2020-03-30 07:53:53,797 INFO] Loading train dataset from ../bert_data/cnndm.train.9118.bert.pt, number of examples: 1980
[2020-03-30 07:54:01,094 INFO] Step 26550/210000; acc:  54.61; ppl:  7.61; xent: 2.03; lr: 0.00001227;   0/460 tok/s;  44076 sec
[2020-03-30 07:55:14,352 INFO] Step 26600/210000; acc:  58.78; ppl:  5.93; xent: 1.78; lr: 0.00001226;   0/371 tok/s;  44149 sec
[2020-03-30 07:56:12,932 INFO] Loading train dataset from ../bert_data/cnndm.train.9119.bert.pt, number of examples: 1985
[2020-03-30 07:56:27,175 INFO] Step 26650/210000; acc:  47.95; ppl: 11.12; xent: 2.41; lr: 0.00001225;   0/628 tok/s;  44222 sec
[2020-03-30 07:57:39,575 INFO] Step 26700/210000; acc:  58.13; ppl:  6.39; xent: 1.85; lr: 0.00001224;   0/576 tok/s;  44294 sec
[2020-03-30 07:58:30,759 INFO] Loading train dataset from ../bert_data/cnndm.train.9120.bert.pt, number of examples: 1981
[2020-03-30 07:58:52,526 INFO] Step 26750/210000; acc:  56.68; ppl:  6.75; xent: 1.91; lr: 0.00001223;   0/538 tok/s;  44367 sec
[2020-03-30 08:00:05,520 INFO] Step 26800/210000; acc:  51.87; ppl:  8.43; xent: 2.13; lr: 0.00001222;   0/637 tok/s;  44440 sec
[2020-03-30 08:00:47,466 INFO] Loading train dataset from ../bert_data/cnndm.train.9121.bert.pt, number of examples: 1985
[2020-03-30 08:01:18,078 INFO] Step 26850/210000; acc:  64.54; ppl:  4.42; xent: 1.49; lr: 0.00001221;   0/399 tok/s;  44513 sec
[2020-03-30 08:02:31,538 INFO] Step 26900/210000; acc:  54.20; ppl:  6.86; xent: 1.93; lr: 0.00001219;   0/642 tok/s;  44586 sec
[2020-03-30 08:03:06,435 INFO] Loading train dataset from ../bert_data/cnndm.train.9122.bert.pt, number of examples: 1986
[2020-03-30 08:03:44,451 INFO] Step 26950/210000; acc:  55.70; ppl:  7.12; xent: 1.96; lr: 0.00001218;   0/584 tok/s;  44659 sec
[2020-03-30 08:04:57,376 INFO] Step 27000/210000; acc:  64.34; ppl:  4.66; xent: 1.54; lr: 0.00001217;   0/477 tok/s;  44732 sec
[2020-03-30 08:04:57,380 INFO] Saving checkpoint ../models/model_step_27000.pt
[2020-03-30 08:05:27,459 INFO] Loading train dataset from ../bert_data/cnndm.train.9123.bert.pt, number of examples: 1983
[2020-03-30 08:06:12,691 INFO] Step 27050/210000; acc:  59.46; ppl:  5.95; xent: 1.78; lr: 0.00001216;   0/561 tok/s;  44808 sec
[2020-03-30 08:07:25,365 INFO] Step 27100/210000; acc:  56.64; ppl:  6.92; xent: 1.93; lr: 0.00001215;   0/518 tok/s;  44880 sec
[2020-03-30 08:07:44,627 INFO] Loading train dataset from ../bert_data/cnndm.train.9124.bert.pt, number of examples: 1983
[2020-03-30 08:08:38,803 INFO] Step 27150/210000; acc:  61.10; ppl:  5.45; xent: 1.70; lr: 0.00001214;   0/402 tok/s;  44954 sec
[2020-03-30 08:09:50,532 INFO] Step 27200/210000; acc:  58.68; ppl:  6.32; xent: 1.84; lr: 0.00001213;   0/701 tok/s;  45025 sec
[2020-03-30 08:10:02,265 INFO] Loading train dataset from ../bert_data/cnndm.train.9125.bert.pt, number of examples: 1991
[2020-03-30 08:11:03,385 INFO] Step 27250/210000; acc:  54.42; ppl:  7.25; xent: 1.98; lr: 0.00001212;   0/651 tok/s;  45098 sec
[2020-03-30 08:12:15,464 INFO] Step 27300/210000; acc:  59.01; ppl:  6.09; xent: 1.81; lr: 0.00001210;   0/538 tok/s;  45170 sec
[2020-03-30 08:12:20,357 INFO] Loading train dataset from ../bert_data/cnndm.train.9126.bert.pt, number of examples: 1985
[2020-03-30 08:13:28,285 INFO] Step 27350/210000; acc:  65.86; ppl:  4.16; xent: 1.43; lr: 0.00001209;   0/386 tok/s;  45243 sec
[2020-03-30 08:14:36,394 INFO] Loading train dataset from ../bert_data/cnndm.train.9127.bert.pt, number of examples: 1980
[2020-03-30 08:14:41,091 INFO] Step 27400/210000; acc:  62.68; ppl:  5.26; xent: 1.66; lr: 0.00001208;   0/440 tok/s;  45316 sec
[2020-03-30 08:15:53,266 INFO] Step 27450/210000; acc:  64.35; ppl:  4.02; xent: 1.39; lr: 0.00001207;   0/435 tok/s;  45388 sec
[2020-03-30 08:16:53,393 INFO] Loading train dataset from ../bert_data/cnndm.train.9128.bert.pt, number of examples: 1981
[2020-03-30 08:17:06,548 INFO] Step 27500/210000; acc:  52.77; ppl:  7.75; xent: 2.05; lr: 0.00001206;   0/734 tok/s;  45461 sec
[2020-03-30 08:17:06,550 INFO] Saving checkpoint ../models/model_step_27500.pt
[2020-03-30 08:18:20,796 INFO] Step 27550/210000; acc:  57.64; ppl:  6.11; xent: 1.81; lr: 0.00001205;   0/755 tok/s;  45536 sec
[2020-03-30 08:19:12,862 INFO] Loading train dataset from ../bert_data/cnndm.train.9129.bert.pt, number of examples: 1987
[2020-03-30 08:19:32,785 INFO] Step 27600/210000; acc:  67.80; ppl:  4.02; xent: 1.39; lr: 0.00001204;   0/534 tok/s;  45608 sec
[2020-03-30 08:20:44,983 INFO] Step 27650/210000; acc:  49.12; ppl: 10.43; xent: 2.34; lr: 0.00001203;   0/732 tok/s;  45680 sec
[2020-03-30 08:21:30,687 INFO] Loading train dataset from ../bert_data/cnndm.train.9130.bert.pt, number of examples: 1986
[2020-03-30 08:21:58,499 INFO] Step 27700/210000; acc:  58.94; ppl:  5.76; xent: 1.75; lr: 0.00001202;   0/551 tok/s;  45753 sec
[2020-03-30 08:23:10,963 INFO] Step 27750/210000; acc:  50.51; ppl: 10.80; xent: 2.38; lr: 0.00001201;   0/803 tok/s;  45826 sec
[2020-03-30 08:23:49,006 INFO] Loading train dataset from ../bert_data/cnndm.train.9131.bert.pt, number of examples: 1984
[2020-03-30 08:24:23,747 INFO] Step 27800/210000; acc:  57.71; ppl:  6.15; xent: 1.82; lr: 0.00001200;   0/535 tok/s;  45899 sec
[2020-03-30 08:25:36,734 INFO] Step 27850/210000; acc:  64.04; ppl:  4.53; xent: 1.51; lr: 0.00001198;   0/468 tok/s;  45972 sec
[2020-03-30 08:26:05,545 INFO] Loading train dataset from ../bert_data/cnndm.train.9132.bert.pt, number of examples: 1990
[2020-03-30 08:26:48,620 INFO] Step 27900/210000; acc:  52.43; ppl:  8.24; xent: 2.11; lr: 0.00001197;   0/668 tok/s;  46043 sec
[2020-03-30 08:28:01,025 INFO] Step 27950/210000; acc:  58.43; ppl:  6.00; xent: 1.79; lr: 0.00001196;   0/604 tok/s;  46116 sec
[2020-03-30 08:28:25,048 INFO] Loading train dataset from ../bert_data/cnndm.train.9133.bert.pt, number of examples: 1978
[2020-03-30 08:29:14,125 INFO] Step 28000/210000; acc:  57.36; ppl:  6.15; xent: 1.82; lr: 0.00001195;   0/701 tok/s;  46189 sec
[2020-03-30 08:29:14,128 INFO] Saving checkpoint ../models/model_step_28000.pt
[2020-03-30 08:30:28,941 INFO] Step 28050/210000; acc:  65.63; ppl:  4.37; xent: 1.48; lr: 0.00001194;   0/485 tok/s;  46264 sec
[2020-03-30 08:30:43,170 INFO] Loading train dataset from ../bert_data/cnndm.train.9134.bert.pt, number of examples: 156
[2020-03-30 08:30:53,485 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-30 08:31:48,883 INFO] Step 28100/210000; acc:  41.03; ppl: 29.78; xent: 3.39; lr: 0.00001193;   0/679 tok/s;  46344 sec
[2020-03-30 08:33:13,050 INFO] Step 28150/210000; acc:  44.23; ppl: 19.68; xent: 2.98; lr: 0.00001192;   0/540 tok/s;  46428 sec
[2020-03-30 08:33:40,407 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-30 08:34:37,520 INFO] Step 28200/210000; acc:  39.97; ppl: 32.20; xent: 3.47; lr: 0.00001191;   0/746 tok/s;  46512 sec
[2020-03-30 08:36:01,578 INFO] Step 28250/210000; acc:  39.13; ppl: 31.64; xent: 3.45; lr: 0.00001190;   0/761 tok/s;  46596 sec
[2020-03-30 08:36:28,998 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-30 08:37:25,948 INFO] Step 28300/210000; acc:  45.09; ppl: 19.81; xent: 2.99; lr: 0.00001189;   0/821 tok/s;  46681 sec
[2020-03-30 08:38:50,216 INFO] Step 28350/210000; acc:  36.24; ppl: 33.04; xent: 3.50; lr: 0.00001188;   0/772 tok/s;  46765 sec
[2020-03-30 08:39:15,640 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-30 08:40:14,641 INFO] Step 28400/210000; acc:  43.30; ppl: 23.23; xent: 3.15; lr: 0.00001187;   0/704 tok/s;  46849 sec
[2020-03-30 08:41:39,048 INFO] Step 28450/210000; acc:  47.19; ppl: 17.35; xent: 2.85; lr: 0.00001186;   0/725 tok/s;  46934 sec
[2020-03-30 08:42:02,886 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-30 08:43:03,598 INFO] Step 28500/210000; acc:  42.70; ppl: 19.54; xent: 2.97; lr: 0.00001185;   0/652 tok/s;  47018 sec
[2020-03-30 08:43:03,601 INFO] Saving checkpoint ../models/model_step_28500.pt
[2020-03-30 08:44:29,839 INFO] Step 28550/210000; acc:  41.90; ppl: 21.54; xent: 3.07; lr: 0.00001184;   0/951 tok/s;  47105 sec
[2020-03-30 08:44:52,137 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-30 08:45:54,488 INFO] Step 28600/210000; acc:  46.09; ppl: 16.75; xent: 2.82; lr: 0.00001183;   0/719 tok/s;  47189 sec
[2020-03-30 08:47:18,591 INFO] Step 28650/210000; acc:  38.83; ppl: 28.01; xent: 3.33; lr: 0.00001182;   0/729 tok/s;  47273 sec
[2020-03-30 08:47:41,192 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-30 08:48:43,474 INFO] Step 28700/210000; acc:  45.50; ppl: 15.59; xent: 2.75; lr: 0.00001181;   0/436 tok/s;  47358 sec
[2020-03-30 08:50:07,093 INFO] Step 28750/210000; acc:  44.92; ppl: 19.51; xent: 2.97; lr: 0.00001180;   0/975 tok/s;  47442 sec
[2020-03-30 08:50:27,782 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-30 08:51:31,980 INFO] Step 28800/210000; acc:  45.87; ppl: 18.92; xent: 2.94; lr: 0.00001179;   0/722 tok/s;  47527 sec
[2020-03-30 08:52:56,081 INFO] Step 28850/210000; acc:  48.96; ppl: 13.93; xent: 2.63; lr: 0.00001177;   0/492 tok/s;  47611 sec
[2020-03-30 08:53:16,824 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-30 08:54:21,025 INFO] Step 28900/210000; acc:  46.65; ppl: 15.88; xent: 2.77; lr: 0.00001176;   0/776 tok/s;  47696 sec
[2020-03-30 08:55:45,021 INFO] Step 28950/210000; acc:  45.42; ppl: 19.22; xent: 2.96; lr: 0.00001175;   0/718 tok/s;  47780 sec
[2020-03-30 08:56:03,722 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-30 08:57:09,881 INFO] Step 29000/210000; acc:  48.17; ppl: 16.67; xent: 2.81; lr: 0.00001174;   0/797 tok/s;  47865 sec
[2020-03-30 08:57:09,884 INFO] Saving checkpoint ../models/model_step_29000.pt
[2020-03-30 08:58:36,160 INFO] Step 29050/210000; acc:  48.30; ppl: 14.67; xent: 2.69; lr: 0.00001173;   0/702 tok/s;  47951 sec
[2020-03-30 08:58:53,278 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-30 08:59:59,883 INFO] Step 29100/210000; acc:  42.38; ppl: 21.62; xent: 3.07; lr: 0.00001172;   0/861 tok/s;  48035 sec
[2020-03-30 09:01:24,171 INFO] Step 29150/210000; acc:  42.67; ppl: 22.04; xent: 3.09; lr: 0.00001171;   0/816 tok/s;  48119 sec
[2020-03-30 09:01:39,677 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-30 09:02:48,897 INFO] Step 29200/210000; acc:  44.83; ppl: 17.14; xent: 2.84; lr: 0.00001170;   0/941 tok/s;  48204 sec
[2020-03-30 09:04:13,012 INFO] Step 29250/210000; acc:  47.10; ppl: 16.03; xent: 2.77; lr: 0.00001169;   0/707 tok/s;  48288 sec
[2020-03-30 09:04:28,908 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-30 09:05:37,779 INFO] Step 29300/210000; acc:  46.01; ppl: 17.59; xent: 2.87; lr: 0.00001168;   0/1079 tok/s;  48373 sec
[2020-03-30 09:07:01,706 INFO] Step 29350/210000; acc:  42.97; ppl: 19.81; xent: 2.99; lr: 0.00001167;   0/756 tok/s;  48457 sec
[2020-03-30 09:07:15,617 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-30 09:08:26,343 INFO] Step 29400/210000; acc:  45.91; ppl: 18.51; xent: 2.92; lr: 0.00001166;   0/606 tok/s;  48541 sec
[2020-03-30 09:09:50,459 INFO] Step 29450/210000; acc:  50.67; ppl: 13.36; xent: 2.59; lr: 0.00001165;   0/595 tok/s;  48625 sec
[2020-03-30 09:10:02,477 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-30 09:11:14,812 INFO] Step 29500/210000; acc:  47.09; ppl: 16.05; xent: 2.78; lr: 0.00001164;   0/531 tok/s;  48710 sec
[2020-03-30 09:11:14,842 INFO] Saving checkpoint ../models/model_step_29500.pt
[2020-03-30 09:12:41,328 INFO] Step 29550/210000; acc:  46.01; ppl: 19.02; xent: 2.95; lr: 0.00001163;   0/550 tok/s;  48796 sec
[2020-03-30 09:12:51,712 INFO] Loading train dataset from ../bert_data/cnndm.train.103.bert.pt, number of examples: 2000
[2020-03-30 09:14:05,591 INFO] Step 29600/210000; acc:  47.43; ppl: 14.64; xent: 2.68; lr: 0.00001162;   0/764 tok/s;  48880 sec
[2020-03-30 09:15:29,588 INFO] Step 29650/210000; acc:  45.61; ppl: 16.78; xent: 2.82; lr: 0.00001161;   0/545 tok/s;  48964 sec
[2020-03-30 09:15:40,470 INFO] Loading train dataset from ../bert_data/cnndm.train.104.bert.pt, number of examples: 2000
[2020-03-30 09:16:54,708 INFO] Step 29700/210000; acc:  43.72; ppl: 18.15; xent: 2.90; lr: 0.00001161;   0/770 tok/s;  49050 sec
[2020-03-30 09:18:18,889 INFO] Step 29750/210000; acc:  52.37; ppl: 11.77; xent: 2.47; lr: 0.00001160;   0/748 tok/s;  49134 sec
[2020-03-30 09:18:28,131 INFO] Loading train dataset from ../bert_data/cnndm.train.105.bert.pt, number of examples: 2001
[2020-03-30 09:19:43,934 INFO] Step 29800/210000; acc:  48.02; ppl: 14.47; xent: 2.67; lr: 0.00001159;   0/938 tok/s;  49219 sec
[2020-03-30 09:21:08,151 INFO] Step 29850/210000; acc:  50.57; ppl: 12.01; xent: 2.49; lr: 0.00001158;   0/993 tok/s;  49303 sec
[2020-03-30 09:21:15,188 INFO] Loading train dataset from ../bert_data/cnndm.train.106.bert.pt, number of examples: 1999
[2020-03-30 09:22:33,063 INFO] Step 29900/210000; acc:  49.55; ppl: 12.67; xent: 2.54; lr: 0.00001157;   0/602 tok/s;  49388 sec
[2020-03-30 09:23:57,297 INFO] Step 29950/210000; acc:  52.30; ppl: 11.75; xent: 2.46; lr: 0.00001156;   0/1131 tok/s;  49472 sec
[2020-03-30 09:24:02,620 INFO] Loading train dataset from ../bert_data/cnndm.train.107.bert.pt, number of examples: 1999
[2020-03-30 09:25:21,820 INFO] Step 30000/210000; acc:  49.48; ppl: 13.99; xent: 2.64; lr: 0.00001155;   0/589 tok/s;  49557 sec
[2020-03-30 09:25:21,848 INFO] Saving checkpoint ../models/model_step_30000.pt
[2020-03-30 09:26:48,541 INFO] Step 30050/210000; acc:  39.21; ppl: 26.16; xent: 3.26; lr: 0.00001154;   0/487 tok/s;  49643 sec
[2020-03-30 09:26:52,237 INFO] Loading train dataset from ../bert_data/cnndm.train.108.bert.pt, number of examples: 2000
[2020-03-30 09:28:12,834 INFO] Step 30100/210000; acc:  46.78; ppl: 15.84; xent: 2.76; lr: 0.00001153;   0/856 tok/s;  49728 sec
[2020-03-30 09:29:36,402 INFO] Step 30150/210000; acc:  54.75; ppl: 10.03; xent: 2.31; lr: 0.00001152;   0/1090 tok/s;  49811 sec
[2020-03-30 09:29:40,604 INFO] Loading train dataset from ../bert_data/cnndm.train.109.bert.pt, number of examples: 2000
[2020-03-30 09:31:01,121 INFO] Step 30200/210000; acc:  47.61; ppl: 13.08; xent: 2.57; lr: 0.00001151;   0/637 tok/s;  49896 sec
[2020-03-30 09:32:25,443 INFO] Step 30250/210000; acc:  48.30; ppl: 14.80; xent: 2.69; lr: 0.00001150;   0/750 tok/s;  49980 sec
[2020-03-30 09:32:27,907 INFO] Loading train dataset from ../bert_data/cnndm.train.11.bert.pt, number of examples: 1999
[2020-03-30 09:33:50,608 INFO] Step 30300/210000; acc:  48.44; ppl: 14.19; xent: 2.65; lr: 0.00001149;   0/834 tok/s;  50065 sec
[2020-03-30 09:35:15,697 INFO] Step 30350/210000; acc:  43.42; ppl: 19.16; xent: 2.95; lr: 0.00001148;   0/803 tok/s;  50151 sec
[2020-03-30 09:35:16,053 INFO] Loading train dataset from ../bert_data/cnndm.train.110.bert.pt, number of examples: 2000
[2020-03-30 09:36:40,578 INFO] Step 30400/210000; acc:  46.45; ppl: 15.33; xent: 2.73; lr: 0.00001147;   0/903 tok/s;  50235 sec
[2020-03-30 09:38:04,210 INFO] Loading train dataset from ../bert_data/cnndm.train.111.bert.pt, number of examples: 2000
[2020-03-30 09:38:06,020 INFO] Step 30450/210000; acc:  49.13; ppl: 12.68; xent: 2.54; lr: 0.00001146;   0/477 tok/s;  50321 sec
[2020-03-30 09:39:29,767 INFO] Step 30500/210000; acc:  49.31; ppl: 13.96; xent: 2.64; lr: 0.00001145;   0/719 tok/s;  50405 sec
[2020-03-30 09:39:29,771 INFO] Saving checkpoint ../models/model_step_30500.pt
[2020-03-30 09:40:53,601 INFO] Loading train dataset from ../bert_data/cnndm.train.112.bert.pt, number of examples: 1999
[2020-03-30 09:40:57,049 INFO] Step 30550/210000; acc:  51.79; ppl: 11.00; xent: 2.40; lr: 0.00001144;   0/579 tok/s;  50492 sec
[2020-03-30 09:42:21,147 INFO] Step 30600/210000; acc:  53.56; ppl: 11.95; xent: 2.48; lr: 0.00001143;   0/595 tok/s;  50576 sec
[2020-03-30 09:43:40,990 INFO] Loading train dataset from ../bert_data/cnndm.train.113.bert.pt, number of examples: 1999
[2020-03-30 09:43:46,286 INFO] Step 30650/210000; acc:  46.65; ppl: 14.76; xent: 2.69; lr: 0.00001142;   0/625 tok/s;  50661 sec
[2020-03-30 09:45:10,629 INFO] Step 30700/210000; acc:  51.35; ppl: 11.09; xent: 2.41; lr: 0.00001141;   0/614 tok/s;  50745 sec
[2020-03-30 09:46:28,715 INFO] Loading train dataset from ../bert_data/cnndm.train.114.bert.pt, number of examples: 1998
[2020-03-30 09:46:35,537 INFO] Step 30750/210000; acc:  42.34; ppl: 17.65; xent: 2.87; lr: 0.00001141;   0/748 tok/s;  50830 sec
[2020-03-30 09:47:59,255 INFO] Step 30800/210000; acc:  47.90; ppl: 14.54; xent: 2.68; lr: 0.00001140;   0/523 tok/s;  50914 sec
[2020-03-30 09:49:17,848 INFO] Loading train dataset from ../bert_data/cnndm.train.115.bert.pt, number of examples: 2000
[2020-03-30 09:49:24,532 INFO] Step 30850/210000; acc:  45.79; ppl: 15.43; xent: 2.74; lr: 0.00001139;   0/916 tok/s;  50999 sec
[2020-03-30 09:50:49,007 INFO] Step 30900/210000; acc:  51.56; ppl: 12.90; xent: 2.56; lr: 0.00001138;   0/838 tok/s;  51084 sec
[2020-03-30 09:52:03,455 INFO] Loading train dataset from ../bert_data/cnndm.train.116.bert.pt, number of examples: 2000
[2020-03-30 09:52:13,666 INFO] Step 30950/210000; acc:  51.69; ppl: 11.49; xent: 2.44; lr: 0.00001137;   0/1134 tok/s;  51168 sec
[2020-03-30 09:53:38,009 INFO] Step 31000/210000; acc:  39.79; ppl: 22.42; xent: 3.11; lr: 0.00001136;   0/1029 tok/s;  51253 sec
[2020-03-30 09:53:38,013 INFO] Saving checkpoint ../models/model_step_31000.pt
[2020-03-30 09:54:55,112 INFO] Loading train dataset from ../bert_data/cnndm.train.117.bert.pt, number of examples: 2000
[2020-03-30 09:55:05,353 INFO] Step 31050/210000; acc:  52.22; ppl: 10.78; xent: 2.38; lr: 0.00001135;   0/459 tok/s;  51340 sec
[2020-03-30 09:56:29,683 INFO] Step 31100/210000; acc:  50.93; ppl: 11.60; xent: 2.45; lr: 0.00001134;   0/519 tok/s;  51425 sec
[2020-03-30 09:57:43,223 INFO] Loading train dataset from ../bert_data/cnndm.train.118.bert.pt, number of examples: 2001
[2020-03-30 09:57:55,159 INFO] Step 31150/210000; acc:  49.67; ppl: 12.64; xent: 2.54; lr: 0.00001133;   0/628 tok/s;  51510 sec
[2020-03-30 09:59:19,332 INFO] Step 31200/210000; acc:  47.73; ppl: 14.65; xent: 2.68; lr: 0.00001132;   0/598 tok/s;  51594 sec
[2020-03-30 10:00:30,117 INFO] Loading train dataset from ../bert_data/cnndm.train.119.bert.pt, number of examples: 2000
[2020-03-30 10:00:43,752 INFO] Step 31250/210000; acc:  51.32; ppl: 11.57; xent: 2.45; lr: 0.00001131;   0/673 tok/s;  51679 sec
[2020-03-30 10:02:07,882 INFO] Step 31300/210000; acc:  45.45; ppl: 15.60; xent: 2.75; lr: 0.00001130;   0/556 tok/s;  51763 sec
[2020-03-30 10:03:17,319 INFO] Loading train dataset from ../bert_data/cnndm.train.12.bert.pt, number of examples: 2001
[2020-03-30 10:03:32,389 INFO] Step 31350/210000; acc:  51.67; ppl: 12.01; xent: 2.49; lr: 0.00001130;   0/748 tok/s;  51847 sec
[2020-03-30 10:04:56,409 INFO] Step 31400/210000; acc:  46.37; ppl: 15.80; xent: 2.76; lr: 0.00001129;   0/616 tok/s;  51931 sec
[2020-03-30 10:06:05,639 INFO] Loading train dataset from ../bert_data/cnndm.train.120.bert.pt, number of examples: 2001
[2020-03-30 10:06:20,682 INFO] Step 31450/210000; acc:  50.90; ppl: 12.73; xent: 2.54; lr: 0.00001128;   0/774 tok/s;  52016 sec
[2020-03-30 10:07:45,229 INFO] Step 31500/210000; acc:  51.82; ppl: 10.41; xent: 2.34; lr: 0.00001127;   0/719 tok/s;  52100 sec
[2020-03-30 10:07:45,255 INFO] Saving checkpoint ../models/model_step_31500.pt
[2020-03-30 10:08:55,222 INFO] Loading train dataset from ../bert_data/cnndm.train.121.bert.pt, number of examples: 2001
[2020-03-30 10:09:12,372 INFO] Step 31550/210000; acc:  47.02; ppl: 14.76; xent: 2.69; lr: 0.00001126;   0/1091 tok/s;  52187 sec
[2020-03-30 10:10:36,778 INFO] Step 31600/210000; acc:  44.31; ppl: 17.38; xent: 2.86; lr: 0.00001125;   0/841 tok/s;  52272 sec
[2020-03-30 10:11:43,041 INFO] Loading train dataset from ../bert_data/cnndm.train.122.bert.pt, number of examples: 2000
[2020-03-30 10:12:01,679 INFO] Step 31650/210000; acc:  42.30; ppl: 16.86; xent: 2.82; lr: 0.00001124;   0/759 tok/s;  52357 sec
[2020-03-30 10:13:25,857 INFO] Step 31700/210000; acc:  46.66; ppl: 14.25; xent: 2.66; lr: 0.00001123;   0/655 tok/s;  52441 sec
[2020-03-30 10:14:30,496 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001
[2020-03-30 10:14:50,628 INFO] Step 31750/210000; acc:  47.95; ppl: 13.63; xent: 2.61; lr: 0.00001122;   0/526 tok/s;  52525 sec
[2020-03-30 10:16:15,057 INFO] Step 31800/210000; acc:  52.16; ppl:  9.94; xent: 2.30; lr: 0.00001122;   0/544 tok/s;  52610 sec
[2020-03-30 10:17:19,407 INFO] Loading train dataset from ../bert_data/cnndm.train.124.bert.pt, number of examples: 2001
[2020-03-30 10:17:39,581 INFO] Step 31850/210000; acc:  48.46; ppl: 14.08; xent: 2.64; lr: 0.00001121;   0/584 tok/s;  52694 sec
[2020-03-30 10:19:04,087 INFO] Step 31900/210000; acc:  48.96; ppl: 13.56; xent: 2.61; lr: 0.00001120;   0/425 tok/s;  52779 sec
[2020-03-30 10:20:06,641 INFO] Loading train dataset from ../bert_data/cnndm.train.125.bert.pt, number of examples: 2000
[2020-03-30 10:20:28,749 INFO] Step 31950/210000; acc:  42.16; ppl: 21.04; xent: 3.05; lr: 0.00001119;   0/669 tok/s;  52864 sec
[2020-03-30 10:21:53,574 INFO] Step 32000/210000; acc:  46.32; ppl: 16.06; xent: 2.78; lr: 0.00001118;   0/724 tok/s;  52948 sec
[2020-03-30 10:21:53,577 INFO] Saving checkpoint ../models/model_step_32000.pt
[2020-03-30 10:22:57,348 INFO] Loading train dataset from ../bert_data/cnndm.train.126.bert.pt, number of examples: 1999
[2020-03-30 10:23:20,726 INFO] Step 32050/210000; acc:  49.72; ppl: 14.33; xent: 2.66; lr: 0.00001117;   0/853 tok/s;  53036 sec
[2020-03-30 10:24:44,619 INFO] Step 32100/210000; acc:  51.55; ppl: 11.73; xent: 2.46; lr: 0.00001116;   0/804 tok/s;  53119 sec
[2020-03-30 10:25:44,465 INFO] Loading train dataset from ../bert_data/cnndm.train.127.bert.pt, number of examples: 2000
[2020-03-30 10:26:09,435 INFO] Step 32150/210000; acc:  49.58; ppl: 13.58; xent: 2.61; lr: 0.00001115;   0/1063 tok/s;  53204 sec
[2020-03-30 10:27:33,782 INFO] Step 32200/210000; acc:  49.83; ppl: 12.12; xent: 2.49; lr: 0.00001115;   0/1040 tok/s;  53289 sec
[2020-03-30 10:28:31,401 INFO] Loading train dataset from ../bert_data/cnndm.train.128.bert.pt, number of examples: 2001
[2020-03-30 10:28:58,245 INFO] Step 32250/210000; acc:  52.90; ppl: 10.47; xent: 2.35; lr: 0.00001114;   0/1042 tok/s;  53373 sec
[2020-03-30 10:30:22,826 INFO] Step 32300/210000; acc:  51.62; ppl: 10.37; xent: 2.34; lr: 0.00001113;   0/1041 tok/s;  53458 sec
[2020-03-30 10:31:18,709 INFO] Loading train dataset from ../bert_data/cnndm.train.129.bert.pt, number of examples: 2000
[2020-03-30 10:31:47,562 INFO] Step 32350/210000; acc:  47.69; ppl: 13.82; xent: 2.63; lr: 0.00001112;   0/543 tok/s;  53542 sec
[2020-03-30 10:33:11,531 INFO] Step 32400/210000; acc:  47.26; ppl: 14.72; xent: 2.69; lr: 0.00001111;   0/557 tok/s;  53626 sec
[2020-03-30 10:34:05,833 INFO] Loading train dataset from ../bert_data/cnndm.train.13.bert.pt, number of examples: 2001
[2020-03-30 10:34:36,181 INFO] Step 32450/210000; acc:  53.91; ppl: 10.19; xent: 2.32; lr: 0.00001110;   0/669 tok/s;  53711 sec
[2020-03-30 10:36:00,544 INFO] Step 32500/210000; acc:  45.83; ppl: 15.01; xent: 2.71; lr: 0.00001109;   0/669 tok/s;  53795 sec
[2020-03-30 10:36:00,571 INFO] Saving checkpoint ../models/model_step_32500.pt
[2020-03-30 10:36:57,620 INFO] Loading train dataset from ../bert_data/cnndm.train.130.bert.pt, number of examples: 2000
[2020-03-30 10:37:27,918 INFO] Step 32550/210000; acc:  43.03; ppl: 15.74; xent: 2.76; lr: 0.00001109;   0/778 tok/s;  53883 sec
[2020-03-30 10:38:51,763 INFO] Step 32600/210000; acc:  45.01; ppl: 16.87; xent: 2.83; lr: 0.00001108;   0/733 tok/s;  53967 sec
[2020-03-30 10:39:43,856 INFO] Loading train dataset from ../bert_data/cnndm.train.131.bert.pt, number of examples: 2001
[2020-03-30 10:40:15,939 INFO] Step 32650/210000; acc:  47.76; ppl: 14.49; xent: 2.67; lr: 0.00001107;   0/814 tok/s;  54051 sec
[2020-03-30 10:41:39,817 INFO] Step 32700/210000; acc:  46.50; ppl: 14.91; xent: 2.70; lr: 0.00001106;   0/779 tok/s;  54135 sec
[2020-03-30 10:42:30,719 INFO] Loading train dataset from ../bert_data/cnndm.train.132.bert.pt, number of examples: 2001
[2020-03-30 10:43:04,315 INFO] Step 32750/210000; acc:  48.51; ppl: 13.09; xent: 2.57; lr: 0.00001105;   0/880 tok/s;  54219 sec
[2020-03-30 10:44:28,365 INFO] Step 32800/210000; acc:  51.24; ppl:  9.81; xent: 2.28; lr: 0.00001104;   0/705 tok/s;  54303 sec
[2020-03-30 10:45:19,147 INFO] Loading train dataset from ../bert_data/cnndm.train.133.bert.pt, number of examples: 1999
[2020-03-30 10:45:52,708 INFO] Step 32850/210000; acc:  50.75; ppl: 12.12; xent: 2.49; lr: 0.00001103;   0/1088 tok/s;  54388 sec
[2020-03-30 10:47:16,830 INFO] Step 32900/210000; acc:  42.88; ppl: 18.17; xent: 2.90; lr: 0.00001103;   0/1190 tok/s;  54472 sec
[2020-03-30 10:48:05,915 INFO] Loading train dataset from ../bert_data/cnndm.train.134.bert.pt, number of examples: 2001
[2020-03-30 10:48:41,340 INFO] Step 32950/210000; acc:  59.39; ppl:  7.15; xent: 1.97; lr: 0.00001102;   0/670 tok/s;  54556 sec
[2020-03-30 10:50:05,183 INFO] Step 33000/210000; acc:  46.45; ppl: 13.98; xent: 2.64; lr: 0.00001101;   0/844 tok/s;  54640 sec
[2020-03-30 10:50:05,207 INFO] Saving checkpoint ../models/model_step_33000.pt
[2020-03-30 10:50:55,186 INFO] Loading train dataset from ../bert_data/cnndm.train.135.bert.pt, number of examples: 1999
[2020-03-30 10:51:32,164 INFO] Step 33050/210000; acc:  49.71; ppl: 11.93; xent: 2.48; lr: 0.00001100;   0/517 tok/s;  54727 sec
[2020-03-30 10:52:56,155 INFO] Step 33100/210000; acc:  48.06; ppl: 12.99; xent: 2.56; lr: 0.00001099;   0/525 tok/s;  54811 sec
[2020-03-30 10:53:43,840 INFO] Loading train dataset from ../bert_data/cnndm.train.136.bert.pt, number of examples: 2001
[2020-03-30 10:54:20,631 INFO] Step 33150/210000; acc:  50.05; ppl: 14.02; xent: 2.64; lr: 0.00001098;   0/654 tok/s;  54895 sec
[2020-03-30 10:55:44,627 INFO] Step 33200/210000; acc:  55.27; ppl:  9.07; xent: 2.20; lr: 0.00001098;   0/556 tok/s;  54979 sec
[2020-03-30 10:56:30,313 INFO] Loading train dataset from ../bert_data/cnndm.train.137.bert.pt, number of examples: 2000
[2020-03-30 10:57:08,808 INFO] Step 33250/210000; acc:  46.84; ppl: 14.45; xent: 2.67; lr: 0.00001097;   0/647 tok/s;  55064 sec
[2020-03-30 10:58:32,481 INFO] Step 33300/210000; acc:  55.21; ppl:  8.67; xent: 2.16; lr: 0.00001096;   0/681 tok/s;  55147 sec
[2020-03-30 10:59:16,774 INFO] Loading train dataset from ../bert_data/cnndm.train.138.bert.pt, number of examples: 2000
[2020-03-30 10:59:57,406 INFO] Step 33350/210000; acc:  49.73; ppl: 11.82; xent: 2.47; lr: 0.00001095;   0/672 tok/s;  55232 sec
[2020-03-30 11:01:21,697 INFO] Step 33400/210000; acc:  45.48; ppl: 14.79; xent: 2.69; lr: 0.00001094;   0/670 tok/s;  55317 sec
[2020-03-30 11:02:06,115 INFO] Loading train dataset from ../bert_data/cnndm.train.139.bert.pt, number of examples: 2001
[2020-03-30 11:02:46,142 INFO] Step 33450/210000; acc:  47.52; ppl: 13.91; xent: 2.63; lr: 0.00001094;   0/843 tok/s;  55401 sec
[2020-03-30 11:04:10,487 INFO] Step 33500/210000; acc:  44.96; ppl: 14.19; xent: 2.65; lr: 0.00001093;   0/846 tok/s;  55485 sec
[2020-03-30 11:04:10,512 INFO] Saving checkpoint ../models/model_step_33500.pt
[2020-03-30 11:04:54,918 INFO] Loading train dataset from ../bert_data/cnndm.train.14.bert.pt, number of examples: 1998
[2020-03-30 11:05:37,090 INFO] Step 33550/210000; acc:  50.78; ppl: 12.06; xent: 2.49; lr: 0.00001092;   0/826 tok/s;  55572 sec
[2020-03-30 11:07:01,877 INFO] Step 33600/210000; acc:  49.34; ppl: 13.22; xent: 2.58; lr: 0.00001091;   0/767 tok/s;  55657 sec
[2020-03-30 11:07:42,776 INFO] Loading train dataset from ../bert_data/cnndm.train.140.bert.pt, number of examples: 2000
[2020-03-30 11:08:26,554 INFO] Step 33650/210000; acc:  46.30; ppl: 14.49; xent: 2.67; lr: 0.00001090;   0/898 tok/s;  55741 sec
[2020-03-30 11:09:50,918 INFO] Step 33700/210000; acc:  42.71; ppl: 17.92; xent: 2.89; lr: 0.00001089;   0/1090 tok/s;  55826 sec
[2020-03-30 11:10:29,849 INFO] Loading train dataset from ../bert_data/cnndm.train.141.bert.pt, number of examples: 1999
[2020-03-30 11:11:15,313 INFO] Step 33750/210000; acc:  49.25; ppl: 13.27; xent: 2.59; lr: 0.00001089;   0/497 tok/s;  55910 sec
[2020-03-30 11:12:38,757 INFO] Step 33800/210000; acc:  43.87; ppl: 17.48; xent: 2.86; lr: 0.00001088;   0/784 tok/s;  55994 sec
[2020-03-30 11:13:18,323 INFO] Loading train dataset from ../bert_data/cnndm.train.142.bert.pt, number of examples: 2000
[2020-03-30 11:14:03,781 INFO] Step 33850/210000; acc:  47.33; ppl: 16.24; xent: 2.79; lr: 0.00001087;   0/547 tok/s;  56079 sec
[2020-03-30 11:15:27,855 INFO] Step 33900/210000; acc:  44.33; ppl: 16.55; xent: 2.81; lr: 0.00001086;   0/672 tok/s;  56163 sec
[2020-03-30 11:16:05,295 INFO] Loading train dataset from ../bert_data/cnndm.train.143.bert.pt, number of examples: 1084
[2020-03-30 11:16:52,430 INFO] Step 33950/210000; acc:  47.81; ppl: 13.88; xent: 2.63; lr: 0.00001085;   0/519 tok/s;  56247 sec
[2020-03-30 11:17:36,452 INFO] Loading train dataset from ../bert_data/cnndm.train.15.bert.pt, number of examples: 1999
[2020-03-30 11:18:16,708 INFO] Step 34000/210000; acc:  49.37; ppl: 13.40; xent: 2.60; lr: 0.00001085;   0/809 tok/s;  56332 sec
[2020-03-30 11:18:16,712 INFO] Saving checkpoint ../models/model_step_34000.pt
[2020-03-30 11:19:42,550 INFO] Step 34050/210000; acc:  53.41; ppl: 10.32; xent: 2.33; lr: 0.00001084;   0/660 tok/s;  56417 sec
[2020-03-30 11:20:26,787 INFO] Loading train dataset from ../bert_data/cnndm.train.150.bert.pt, number of examples: 1985
[2020-03-30 11:21:01,796 INFO] Step 34100/210000; acc:  51.27; ppl:  8.59; xent: 2.15; lr: 0.00001083;   0/549 tok/s;  56497 sec
[2020-03-30 11:22:15,091 INFO] Step 34150/210000; acc:  51.91; ppl:  7.88; xent: 2.06; lr: 0.00001082;   0/613 tok/s;  56570 sec
[2020-03-30 11:22:44,777 INFO] Loading train dataset from ../bert_data/cnndm.train.151.bert.pt, number of examples: 1990
[2020-03-30 11:23:28,465 INFO] Step 34200/210000; acc:  64.52; ppl:  4.45; xent: 1.49; lr: 0.00001081;   0/446 tok/s;  56643 sec
[2020-03-30 11:24:42,569 INFO] Step 34250/210000; acc:  62.06; ppl:  5.52; xent: 1.71; lr: 0.00001081;   0/413 tok/s;  56717 sec
[2020-03-30 11:25:04,684 INFO] Loading train dataset from ../bert_data/cnndm.train.152.bert.pt, number of examples: 993
[2020-03-30 11:25:55,393 INFO] Step 34300/210000; acc:  63.71; ppl:  4.48; xent: 1.50; lr: 0.00001080;   0/372 tok/s;  56790 sec
[2020-03-30 11:26:12,862 INFO] Loading train dataset from ../bert_data/cnndm.train.153.bert.pt, number of examples: 1982
[2020-03-30 11:27:15,497 INFO] Step 34350/210000; acc:  40.68; ppl: 18.84; xent: 2.94; lr: 0.00001079;   0/1352 tok/s;  56870 sec
[2020-03-30 11:28:38,190 INFO] Step 34400/210000; acc:  46.17; ppl: 11.96; xent: 2.48; lr: 0.00001078;   0/584 tok/s;  56953 sec
[2020-03-30 11:28:50,263 INFO] Loading train dataset from ../bert_data/cnndm.train.154.bert.pt, number of examples: 1978
[2020-03-30 11:30:01,344 INFO] Step 34450/210000; acc:  40.47; ppl: 19.72; xent: 2.98; lr: 0.00001078;   0/1328 tok/s;  57036 sec
[2020-03-30 11:31:24,659 INFO] Step 34500/210000; acc:  44.81; ppl: 14.13; xent: 2.65; lr: 0.00001077;   0/1011 tok/s;  57119 sec
[2020-03-30 11:31:24,662 INFO] Saving checkpoint ../models/model_step_34500.pt
[2020-03-30 11:31:28,908 INFO] Loading train dataset from ../bert_data/cnndm.train.155.bert.pt, number of examples: 1988
[2020-03-30 11:32:49,396 INFO] Step 34550/210000; acc:  53.53; ppl:  8.32; xent: 2.12; lr: 0.00001076;   0/437 tok/s;  57204 sec
[2020-03-30 11:34:06,848 INFO] Loading train dataset from ../bert_data/cnndm.train.156.bert.pt, number of examples: 1993
[2020-03-30 11:34:13,189 INFO] Step 34600/210000; acc:  38.86; ppl: 20.65; xent: 3.03; lr: 0.00001075;   0/1422 tok/s;  57288 sec
[2020-03-30 11:35:35,778 INFO] Step 34650/210000; acc:  49.24; ppl:  9.52; xent: 2.25; lr: 0.00001074;   0/554 tok/s;  57371 sec
[2020-03-30 11:36:44,514 INFO] Loading train dataset from ../bert_data/cnndm.train.157.bert.pt, number of examples: 1981
[2020-03-30 11:36:59,440 INFO] Step 34700/210000; acc:  44.84; ppl: 13.94; xent: 2.63; lr: 0.00001074;   0/1386 tok/s;  57454 sec
[2020-03-30 11:38:22,453 INFO] Step 34750/210000; acc:  50.19; ppl:  8.85; xent: 2.18; lr: 0.00001073;   0/487 tok/s;  57537 sec
[2020-03-30 11:39:22,130 INFO] Loading train dataset from ../bert_data/cnndm.train.158.bert.pt, number of examples: 1987
[2020-03-30 11:39:46,837 INFO] Step 34800/210000; acc:  44.58; ppl: 12.99; xent: 2.56; lr: 0.00001072;   0/756 tok/s;  57622 sec
[2020-03-30 11:41:08,830 INFO] Step 34850/210000; acc:  47.41; ppl: 10.83; xent: 2.38; lr: 0.00001071;   0/810 tok/s;  57704 sec
[2020-03-30 11:41:59,043 INFO] Loading train dataset from ../bert_data/cnndm.train.159.bert.pt, number of examples: 1986
[2020-03-30 11:42:31,964 INFO] Step 34900/210000; acc:  53.83; ppl:  7.82; xent: 2.06; lr: 0.00001071;   0/474 tok/s;  57787 sec
[2020-03-30 11:43:54,636 INFO] Step 34950/210000; acc:  42.25; ppl: 13.58; xent: 2.61; lr: 0.00001070;   0/979 tok/s;  57869 sec
[2020-03-30 11:44:34,321 INFO] Loading train dataset from ../bert_data/cnndm.train.16.bert.pt, number of examples: 2001
[2020-03-30 11:45:18,072 INFO] Step 35000/210000; acc:  46.62; ppl: 17.81; xent: 2.88; lr: 0.00001069;   0/713 tok/s;  57953 sec
[2020-03-30 11:45:18,079 INFO] Saving checkpoint ../models/model_step_35000.pt
[2020-03-30 11:46:44,463 INFO] Step 35050/210000; acc:  50.22; ppl: 11.87; xent: 2.47; lr: 0.00001068;   0/949 tok/s;  58039 sec
[2020-03-30 11:47:25,686 INFO] Loading train dataset from ../bert_data/cnndm.train.160.bert.pt, number of examples: 1983
[2020-03-30 11:48:08,728 INFO] Step 35100/210000; acc:  47.05; ppl: 10.52; xent: 2.35; lr: 0.00001068;   0/600 tok/s;  58124 sec
[2020-03-30 11:49:31,559 INFO] Step 35150/210000; acc:  42.17; ppl: 16.72; xent: 2.82; lr: 0.00001067;   0/1362 tok/s;  58206 sec
[2020-03-30 11:50:04,383 INFO] Loading train dataset from ../bert_data/cnndm.train.161.bert.pt, number of examples: 1990
[2020-03-30 11:50:54,845 INFO] Step 35200/210000; acc:  44.67; ppl: 11.48; xent: 2.44; lr: 0.00001066;   0/691 tok/s;  58290 sec
[2020-03-30 11:52:17,003 INFO] Step 35250/210000; acc:  38.35; ppl: 22.96; xent: 3.13; lr: 0.00001065;   0/1488 tok/s;  58372 sec
[2020-03-30 11:52:40,149 INFO] Loading train dataset from ../bert_data/cnndm.train.162.bert.pt, number of examples: 1986
[2020-03-30 11:53:39,915 INFO] Step 35300/210000; acc:  53.32; ppl:  7.60; xent: 2.03; lr: 0.00001064;   0/623 tok/s;  58455 sec
[2020-03-30 11:55:02,359 INFO] Step 35350/210000; acc:  38.92; ppl: 18.59; xent: 2.92; lr: 0.00001064;   0/1354 tok/s;  58537 sec
[2020-03-30 11:55:17,981 INFO] Loading train dataset from ../bert_data/cnndm.train.163.bert.pt, number of examples: 1982
[2020-03-30 11:56:25,383 INFO] Step 35400/210000; acc:  49.85; ppl:  9.25; xent: 2.22; lr: 0.00001063;   0/840 tok/s;  58620 sec
[2020-03-30 11:57:47,901 INFO] Step 35450/210000; acc:  40.14; ppl: 17.77; xent: 2.88; lr: 0.00001062;   0/863 tok/s;  58703 sec
[2020-03-30 11:57:53,012 INFO] Loading train dataset from ../bert_data/cnndm.train.164.bert.pt, number of examples: 1984
[2020-03-30 11:59:10,437 INFO] Step 35500/210000; acc:  45.49; ppl: 12.98; xent: 2.56; lr: 0.00001061;   0/963 tok/s;  58785 sec
[2020-03-30 11:59:10,441 INFO] Saving checkpoint ../models/model_step_35500.pt
[2020-03-30 12:00:31,887 INFO] Loading train dataset from ../bert_data/cnndm.train.165.bert.pt, number of examples: 1984
[2020-03-30 12:00:35,193 INFO] Step 35550/210000; acc:  51.72; ppl:  8.33; xent: 2.12; lr: 0.00001061;   0/551 tok/s;  58870 sec
[2020-03-30 12:01:57,545 INFO] Step 35600/210000; acc:  44.42; ppl: 14.38; xent: 2.67; lr: 0.00001060;   0/1205 tok/s;  58952 sec
[2020-03-30 12:03:07,057 INFO] Loading train dataset from ../bert_data/cnndm.train.166.bert.pt, number of examples: 1370
[2020-03-30 12:03:20,304 INFO] Step 35650/210000; acc:  47.45; ppl: 10.29; xent: 2.33; lr: 0.00001059;   0/759 tok/s;  59035 sec
[2020-03-30 12:04:42,868 INFO] Step 35700/210000; acc:  42.18; ppl: 14.22; xent: 2.65; lr: 0.00001059;   0/1420 tok/s;  59118 sec
[2020-03-30 12:04:56,785 INFO] Loading train dataset from ../bert_data/cnndm.train.167.bert.pt, number of examples: 1089
[2020-03-30 12:06:04,250 INFO] Step 35750/210000; acc:  52.92; ppl:  7.32; xent: 1.99; lr: 0.00001058;   0/640 tok/s;  59199 sec
[2020-03-30 12:06:22,211 INFO] Loading train dataset from ../bert_data/cnndm.train.168.bert.pt, number of examples: 1991
[2020-03-30 12:07:25,125 INFO] Step 35800/210000; acc:  44.21; ppl: 13.66; xent: 2.61; lr: 0.00001057;   0/1032 tok/s;  59280 sec
[2020-03-30 12:08:45,786 INFO] Step 35850/210000; acc:  49.16; ppl:  8.62; xent: 2.15; lr: 0.00001056;   0/704 tok/s;  59361 sec
[2020-03-30 12:08:55,424 INFO] Loading train dataset from ../bert_data/cnndm.train.169.bert.pt, number of examples: 1995
[2020-03-30 12:10:06,775 INFO] Step 35900/210000; acc:  43.51; ppl: 14.50; xent: 2.67; lr: 0.00001056;   0/781 tok/s;  59442 sec
[2020-03-30 12:11:27,072 INFO] Step 35950/210000; acc:  49.87; ppl:  9.05; xent: 2.20; lr: 0.00001055;   0/765 tok/s;  59522 sec
[2020-03-30 12:11:29,009 INFO] Loading train dataset from ../bert_data/cnndm.train.17.bert.pt, number of examples: 2000
[2020-03-30 12:12:51,567 INFO] Step 36000/210000; acc:  52.35; ppl: 11.53; xent: 2.45; lr: 0.00001054;   0/775 tok/s;  59606 sec
[2020-03-30 12:12:51,571 INFO] Saving checkpoint ../models/model_step_36000.pt
[2020-03-30 12:14:17,883 INFO] Step 36050/210000; acc:  51.25; ppl: 11.40; xent: 2.43; lr: 0.00001053;   0/840 tok/s;  59693 sec
[2020-03-30 12:14:20,310 INFO] Loading train dataset from ../bert_data/cnndm.train.170.bert.pt, number of examples: 1981
[2020-03-30 12:15:39,577 INFO] Step 36100/210000; acc:  54.51; ppl:  6.81; xent: 1.92; lr: 0.00001053;   0/462 tok/s;  59774 sec
[2020-03-30 12:16:51,188 INFO] Loading train dataset from ../bert_data/cnndm.train.171.bert.pt, number of examples: 1990
[2020-03-30 12:17:01,171 INFO] Step 36150/210000; acc:  40.01; ppl: 16.55; xent: 2.81; lr: 0.00001052;   0/925 tok/s;  59856 sec
[2020-03-30 12:18:21,438 INFO] Step 36200/210000; acc:  57.39; ppl:  6.05; xent: 1.80; lr: 0.00001051;   0/721 tok/s;  59936 sec
[2020-03-30 12:19:26,681 INFO] Loading train dataset from ../bert_data/cnndm.train.172.bert.pt, number of examples: 1983
[2020-03-30 12:19:42,757 INFO] Step 36250/210000; acc:  47.96; ppl: 10.51; xent: 2.35; lr: 0.00001050;   0/931 tok/s;  60018 sec
[2020-03-30 12:21:02,726 INFO] Step 36300/210000; acc:  52.22; ppl:  8.74; xent: 2.17; lr: 0.00001050;   0/648 tok/s;  60098 sec
[2020-03-30 12:21:57,794 INFO] Loading train dataset from ../bert_data/cnndm.train.18.bert.pt, number of examples: 1998
[2020-03-30 12:22:24,771 INFO] Step 36350/210000; acc:  43.45; ppl: 17.88; xent: 2.88; lr: 0.00001049;   0/714 tok/s;  60180 sec
[2020-03-30 12:23:48,961 INFO] Step 36400/210000; acc:  45.88; ppl: 14.32; xent: 2.66; lr: 0.00001048;   0/1010 tok/s;  60264 sec
[2020-03-30 12:24:44,837 INFO] Loading train dataset from ../bert_data/cnndm.train.19.bert.pt, number of examples: 2000
[2020-03-30 12:25:13,426 INFO] Step 36450/210000; acc:  48.04; ppl: 15.26; xent: 2.73; lr: 0.00001048;   0/501 tok/s;  60348 sec
[2020-03-30 12:26:37,354 INFO] Step 36500/210000; acc:  49.47; ppl: 12.47; xent: 2.52; lr: 0.00001047;   0/897 tok/s;  60432 sec
[2020-03-30 12:26:37,357 INFO] Saving checkpoint ../models/model_step_36500.pt
[2020-03-30 12:27:34,116 INFO] Loading train dataset from ../bert_data/cnndm.train.2.bert.pt, number of examples: 2001
[2020-03-30 12:28:04,351 INFO] Step 36550/210000; acc:  49.57; ppl: 11.51; xent: 2.44; lr: 0.00001046;   0/571 tok/s;  60519 sec
[2020-03-30 12:29:28,457 INFO] Step 36600/210000; acc:  50.00; ppl: 13.60; xent: 2.61; lr: 0.00001045;   0/853 tok/s;  60603 sec
[2020-03-30 12:30:22,864 INFO] Loading train dataset from ../bert_data/cnndm.train.20.bert.pt, number of examples: 2000
[2020-03-30 12:30:53,078 INFO] Step 36650/210000; acc:  49.80; ppl: 12.81; xent: 2.55; lr: 0.00001045;   0/739 tok/s;  60688 sec
[2020-03-30 12:32:17,463 INFO] Step 36700/210000; acc:  47.38; ppl: 14.80; xent: 2.69; lr: 0.00001044;   0/674 tok/s;  60772 sec
[2020-03-30 12:33:10,578 INFO] Loading train dataset from ../bert_data/cnndm.train.21.bert.pt, number of examples: 2001
[2020-03-30 12:33:42,536 INFO] Step 36750/210000; acc:  50.24; ppl: 11.52; xent: 2.44; lr: 0.00001043;   0/741 tok/s;  60857 sec
[2020-03-30 12:35:06,693 INFO] Step 36800/210000; acc:  47.09; ppl: 14.54; xent: 2.68; lr: 0.00001043;   0/632 tok/s;  60942 sec
[2020-03-30 12:35:57,455 INFO] Loading train dataset from ../bert_data/cnndm.train.22.bert.pt, number of examples: 1999
[2020-03-30 12:36:31,153 INFO] Step 36850/210000; acc:  47.78; ppl: 14.91; xent: 2.70; lr: 0.00001042;   0/1028 tok/s;  61026 sec
[2020-03-30 12:37:55,166 INFO] Step 36900/210000; acc:  53.11; ppl: 10.38; xent: 2.34; lr: 0.00001041;   0/728 tok/s;  61110 sec
[2020-03-30 12:38:46,402 INFO] Loading train dataset from ../bert_data/cnndm.train.23.bert.pt, number of examples: 2001
[2020-03-30 12:39:20,275 INFO] Step 36950/210000; acc:  53.22; ppl: 10.62; xent: 2.36; lr: 0.00001040;   0/1037 tok/s;  61195 sec
[2020-03-30 12:40:44,227 INFO] Step 37000/210000; acc:  47.25; ppl: 14.25; xent: 2.66; lr: 0.00001040;   0/817 tok/s;  61279 sec
[2020-03-30 12:40:44,254 INFO] Saving checkpoint ../models/model_step_37000.pt
[2020-03-30 12:41:35,639 INFO] Loading train dataset from ../bert_data/cnndm.train.24.bert.pt, number of examples: 2001
[2020-03-30 12:42:10,866 INFO] Step 37050/210000; acc:  53.08; ppl:  9.79; xent: 2.28; lr: 0.00001039;   0/684 tok/s;  61366 sec
[2020-03-30 12:43:34,952 INFO] Step 37100/210000; acc:  47.45; ppl: 12.91; xent: 2.56; lr: 0.00001038;   0/862 tok/s;  61450 sec
[2020-03-30 12:44:22,383 INFO] Loading train dataset from ../bert_data/cnndm.train.25.bert.pt, number of examples: 2001
[2020-03-30 12:44:59,458 INFO] Step 37150/210000; acc:  49.92; ppl: 12.63; xent: 2.54; lr: 0.00001038;   0/660 tok/s;  61534 sec
[2020-03-30 12:46:23,615 INFO] Step 37200/210000; acc:  52.11; ppl:  9.83; xent: 2.29; lr: 0.00001037;   0/482 tok/s;  61618 sec
[2020-03-30 12:47:11,101 INFO] Loading train dataset from ../bert_data/cnndm.train.26.bert.pt, number of examples: 2000
[2020-03-30 12:47:48,293 INFO] Step 37250/210000; acc:  44.44; ppl: 14.42; xent: 2.67; lr: 0.00001036;   0/594 tok/s;  61703 sec
[2020-03-30 12:49:11,918 INFO] Step 37300/210000; acc:  50.86; ppl: 11.95; xent: 2.48; lr: 0.00001036;   0/721 tok/s;  61787 sec
[2020-03-30 12:49:57,696 INFO] Loading train dataset from ../bert_data/cnndm.train.27.bert.pt, number of examples: 2001
[2020-03-30 12:50:36,655 INFO] Step 37350/210000; acc:  50.95; ppl: 11.65; xent: 2.46; lr: 0.00001035;   0/632 tok/s;  61871 sec
[2020-03-30 12:52:00,668 INFO] Step 37400/210000; acc:  54.79; ppl:  9.53; xent: 2.25; lr: 0.00001034;   0/615 tok/s;  61955 sec
[2020-03-30 12:52:47,291 INFO] Loading train dataset from ../bert_data/cnndm.train.28.bert.pt, number of examples: 2000
[2020-03-30 12:53:26,283 INFO] Step 37450/210000; acc:  49.58; ppl: 12.21; xent: 2.50; lr: 0.00001033;   0/709 tok/s;  62041 sec
[2020-03-30 12:54:50,240 INFO] Step 37500/210000; acc:  50.44; ppl: 10.70; xent: 2.37; lr: 0.00001033;   0/614 tok/s;  62125 sec
[2020-03-30 12:54:50,267 INFO] Saving checkpoint ../models/model_step_37500.pt
[2020-03-30 12:55:36,945 INFO] Loading train dataset from ../bert_data/cnndm.train.29.bert.pt, number of examples: 1999
[2020-03-30 12:56:17,282 INFO] Step 37550/210000; acc:  45.64; ppl: 15.18; xent: 2.72; lr: 0.00001032;   0/820 tok/s;  62212 sec
[2020-03-30 12:57:41,369 INFO] Step 37600/210000; acc:  52.21; ppl: 11.41; xent: 2.43; lr: 0.00001031;   0/750 tok/s;  62296 sec
[2020-03-30 12:58:24,379 INFO] Loading train dataset from ../bert_data/cnndm.train.3.bert.pt, number of examples: 2001
[2020-03-30 12:59:06,151 INFO] Step 37650/210000; acc:  52.69; ppl: 10.24; xent: 2.33; lr: 0.00001031;   0/908 tok/s;  62381 sec
[2020-03-30 13:00:30,133 INFO] Step 37700/210000; acc:  42.87; ppl: 20.42; xent: 3.02; lr: 0.00001030;   0/803 tok/s;  62465 sec
[2020-03-30 13:01:10,801 INFO] Loading train dataset from ../bert_data/cnndm.train.30.bert.pt, number of examples: 1996
[2020-03-30 13:01:54,549 INFO] Step 37750/210000; acc:  51.34; ppl: 10.98; xent: 2.40; lr: 0.00001029;   0/1068 tok/s;  62549 sec
[2020-03-30 13:03:19,138 INFO] Step 37800/210000; acc:  50.72; ppl: 12.14; xent: 2.50; lr: 0.00001029;   0/1018 tok/s;  62634 sec
[2020-03-30 13:03:58,195 INFO] Loading train dataset from ../bert_data/cnndm.train.31.bert.pt, number of examples: 2000
[2020-03-30 13:04:43,778 INFO] Step 37850/210000; acc:  51.00; ppl: 11.67; xent: 2.46; lr: 0.00001028;   0/496 tok/s;  62719 sec
[2020-03-30 13:06:07,895 INFO] Step 37900/210000; acc:  42.73; ppl: 18.16; xent: 2.90; lr: 0.00001027;   0/630 tok/s;  62803 sec
[2020-03-30 13:06:47,611 INFO] Loading train dataset from ../bert_data/cnndm.train.32.bert.pt, number of examples: 1998
[2020-03-30 13:07:32,984 INFO] Step 37950/210000; acc:  54.72; ppl:  8.32; xent: 2.12; lr: 0.00001027;   0/582 tok/s;  62888 sec
[2020-03-30 13:08:56,952 INFO] Step 38000/210000; acc:  50.46; ppl: 11.34; xent: 2.43; lr: 0.00001026;   0/528 tok/s;  62972 sec
[2020-03-30 13:08:56,978 INFO] Saving checkpoint ../models/model_step_38000.pt
[2020-03-30 13:09:37,102 INFO] Loading train dataset from ../bert_data/cnndm.train.33.bert.pt, number of examples: 1999
[2020-03-30 13:10:24,465 INFO] Step 38050/210000; acc:  55.89; ppl:  7.97; xent: 2.08; lr: 0.00001025;   0/578 tok/s;  63059 sec
[2020-03-30 13:11:48,429 INFO] Step 38100/210000; acc:  50.60; ppl:  9.66; xent: 2.27; lr: 0.00001025;   0/584 tok/s;  63143 sec
[2020-03-30 13:12:23,910 INFO] Loading train dataset from ../bert_data/cnndm.train.34.bert.pt, number of examples: 2000
[2020-03-30 13:13:12,472 INFO] Step 38150/210000; acc:  46.03; ppl: 14.93; xent: 2.70; lr: 0.00001024;   0/618 tok/s;  63227 sec
[2020-03-30 13:14:36,834 INFO] Step 38200/210000; acc:  50.09; ppl: 10.26; xent: 2.33; lr: 0.00001023;   0/639 tok/s;  63312 sec
[2020-03-30 13:15:10,840 INFO] Loading train dataset from ../bert_data/cnndm.train.35.bert.pt, number of examples: 1998
[2020-03-30 13:16:01,402 INFO] Step 38250/210000; acc:  50.33; ppl: 11.46; xent: 2.44; lr: 0.00001023;   0/992 tok/s;  63396 sec
[2020-03-30 13:17:25,243 INFO] Step 38300/210000; acc:  49.60; ppl: 11.43; xent: 2.44; lr: 0.00001022;   0/680 tok/s;  63480 sec
[2020-03-30 13:17:59,986 INFO] Loading train dataset from ../bert_data/cnndm.train.36.bert.pt, number of examples: 2000
[2020-03-30 13:18:50,720 INFO] Step 38350/210000; acc:  50.00; ppl: 12.18; xent: 2.50; lr: 0.00001021;   0/980 tok/s;  63566 sec
[2020-03-30 13:20:14,944 INFO] Step 38400/210000; acc:  46.22; ppl: 13.41; xent: 2.60; lr: 0.00001021;   0/754 tok/s;  63650 sec
[2020-03-30 13:20:47,725 INFO] Loading train dataset from ../bert_data/cnndm.train.37.bert.pt, number of examples: 1999
[2020-03-30 13:21:39,815 INFO] Step 38450/210000; acc:  53.47; ppl:  9.99; xent: 2.30; lr: 0.00001020;   0/518 tok/s;  63735 sec
[2020-03-30 13:23:03,647 INFO] Step 38500/210000; acc:  48.29; ppl: 14.23; xent: 2.66; lr: 0.00001019;   0/829 tok/s;  63818 sec
[2020-03-30 13:23:03,650 INFO] Saving checkpoint ../models/model_step_38500.pt
[2020-03-30 13:23:36,311 INFO] Loading train dataset from ../bert_data/cnndm.train.38.bert.pt, number of examples: 2001
[2020-03-30 13:24:29,861 INFO] Step 38550/210000; acc:  51.17; ppl: 11.54; xent: 2.45; lr: 0.00001019;   0/523 tok/s;  63905 sec
[2020-03-30 13:25:54,237 INFO] Step 38600/210000; acc:  53.72; ppl: 10.15; xent: 2.32; lr: 0.00001018;   0/619 tok/s;  63989 sec
[2020-03-30 13:26:23,090 INFO] Loading train dataset from ../bert_data/cnndm.train.39.bert.pt, number of examples: 2000
[2020-03-30 13:27:18,631 INFO] Step 38650/210000; acc:  49.30; ppl: 11.81; xent: 2.47; lr: 0.00001017;   0/655 tok/s;  64073 sec
[2020-03-30 13:28:42,754 INFO] Step 38700/210000; acc:  49.05; ppl: 12.12; xent: 2.49; lr: 0.00001017;   0/611 tok/s;  64158 sec
[2020-03-30 13:29:12,207 INFO] Loading train dataset from ../bert_data/cnndm.train.4.bert.pt, number of examples: 2001
[2020-03-30 13:30:07,895 INFO] Step 38750/210000; acc:  51.67; ppl: 11.94; xent: 2.48; lr: 0.00001016;   0/714 tok/s;  64243 sec
[2020-03-30 13:31:31,693 INFO] Step 38800/210000; acc:  49.62; ppl: 12.20; xent: 2.50; lr: 0.00001015;   0/625 tok/s;  64327 sec
[2020-03-30 13:31:59,324 INFO] Loading train dataset from ../bert_data/cnndm.train.40.bert.pt, number of examples: 1999
[2020-03-30 13:32:56,548 INFO] Step 38850/210000; acc:  49.48; ppl: 12.30; xent: 2.51; lr: 0.00001015;   0/798 tok/s;  64411 sec
[2020-03-30 13:34:20,322 INFO] Step 38900/210000; acc:  47.76; ppl: 13.22; xent: 2.58; lr: 0.00001014;   0/804 tok/s;  64495 sec
[2020-03-30 13:34:45,923 INFO] Loading train dataset from ../bert_data/cnndm.train.41.bert.pt, number of examples: 2000
[2020-03-30 13:35:44,628 INFO] Step 38950/210000; acc:  50.67; ppl: 11.08; xent: 2.40; lr: 0.00001013;   0/750 tok/s;  64579 sec
[2020-03-30 13:37:08,793 INFO] Step 39000/210000; acc:  51.02; ppl: 10.87; xent: 2.39; lr: 0.00001013;   0/766 tok/s;  64664 sec
[2020-03-30 13:37:08,818 INFO] Saving checkpoint ../models/model_step_39000.pt
[2020-03-30 13:37:37,184 INFO] Loading train dataset from ../bert_data/cnndm.train.42.bert.pt, number of examples: 2000
[2020-03-30 13:38:36,567 INFO] Step 39050/210000; acc:  44.87; ppl: 14.74; xent: 2.69; lr: 0.00001012;   0/845 tok/s;  64751 sec
[2020-03-30 13:40:00,619 INFO] Step 39100/210000; acc:  50.09; ppl: 10.74; xent: 2.37; lr: 0.00001011;   0/691 tok/s;  64835 sec
[2020-03-30 13:40:25,131 INFO] Loading train dataset from ../bert_data/cnndm.train.43.bert.pt, number of examples: 2001
[2020-03-30 13:41:25,473 INFO] Step 39150/210000; acc:  49.03; ppl: 12.05; xent: 2.49; lr: 0.00001011;   0/739 tok/s;  64920 sec
[2020-03-30 13:42:49,767 INFO] Step 39200/210000; acc:  49.67; ppl: 11.54; xent: 2.45; lr: 0.00001010;   0/881 tok/s;  65005 sec
[2020-03-30 13:43:12,114 INFO] Loading train dataset from ../bert_data/cnndm.train.44.bert.pt, number of examples: 1998
[2020-03-30 13:44:14,817 INFO] Step 39250/210000; acc:  49.33; ppl: 11.88; xent: 2.47; lr: 0.00001010;   0/540 tok/s;  65090 sec
[2020-03-30 13:45:39,409 INFO] Step 39300/210000; acc:  50.69; ppl: 10.36; xent: 2.34; lr: 0.00001009;   0/619 tok/s;  65174 sec
[2020-03-30 13:46:00,182 INFO] Loading train dataset from ../bert_data/cnndm.train.45.bert.pt, number of examples: 2001
[2020-03-30 13:47:04,652 INFO] Step 39350/210000; acc:  53.28; ppl: 10.37; xent: 2.34; lr: 0.00001008;   0/624 tok/s;  65259 sec
[2020-03-30 13:48:28,902 INFO] Step 39400/210000; acc:  51.70; ppl: 10.34; xent: 2.34; lr: 0.00001008;   0/616 tok/s;  65344 sec
[2020-03-30 13:48:47,905 INFO] Loading train dataset from ../bert_data/cnndm.train.46.bert.pt, number of examples: 2001
[2020-03-30 13:49:53,180 INFO] Step 39450/210000; acc:  48.46; ppl: 11.49; xent: 2.44; lr: 0.00001007;   0/656 tok/s;  65428 sec
[2020-03-30 13:51:16,917 INFO] Step 39500/210000; acc:  50.17; ppl: 12.45; xent: 2.52; lr: 0.00001006;   0/540 tok/s;  65512 sec
[2020-03-30 13:51:16,920 INFO] Saving checkpoint ../models/model_step_39500.pt
[2020-03-30 13:51:37,971 INFO] Loading train dataset from ../bert_data/cnndm.train.47.bert.pt, number of examples: 2000
[2020-03-30 13:52:43,755 INFO] Step 39550/210000; acc:  51.95; ppl: 11.30; xent: 2.42; lr: 0.00001006;   0/662 tok/s;  65599 sec
[2020-03-30 13:54:07,927 INFO] Step 39600/210000; acc:  53.77; ppl: 10.01; xent: 2.30; lr: 0.00001005;   0/460 tok/s;  65683 sec
[2020-03-30 13:54:25,035 INFO] Loading train dataset from ../bert_data/cnndm.train.48.bert.pt, number of examples: 2000
[2020-03-30 13:55:32,236 INFO] Step 39650/210000; acc:  49.61; ppl: 11.81; xent: 2.47; lr: 0.00001004;   0/824 tok/s;  65767 sec
[2020-03-30 13:56:56,790 INFO] Step 39700/210000; acc:  52.57; ppl: 10.68; xent: 2.37; lr: 0.00001004;   0/904 tok/s;  65852 sec
[2020-03-30 13:57:12,705 INFO] Loading train dataset from ../bert_data/cnndm.train.49.bert.pt, number of examples: 2001
[2020-03-30 13:58:21,947 INFO] Step 39750/210000; acc:  45.58; ppl: 15.85; xent: 2.76; lr: 0.00001003;   0/1067 tok/s;  65937 sec
[2020-03-30 13:59:45,911 INFO] Step 39800/210000; acc:  49.40; ppl: 12.19; xent: 2.50; lr: 0.00001003;   0/851 tok/s;  66021 sec
[2020-03-30 14:00:01,279 INFO] Loading train dataset from ../bert_data/cnndm.train.5.bert.pt, number of examples: 2001
[2020-03-30 14:01:10,271 INFO] Step 39850/210000; acc:  41.95; ppl: 17.33; xent: 2.85; lr: 0.00001002;   0/712 tok/s;  66105 sec
[2020-03-30 14:02:34,312 INFO] Step 39900/210000; acc:  54.63; ppl:  7.92; xent: 2.07; lr: 0.00001001;   0/552 tok/s;  66189 sec
[2020-03-30 14:02:48,090 INFO] Loading train dataset from ../bert_data/cnndm.train.50.bert.pt, number of examples: 2001
[2020-03-30 14:03:58,828 INFO] Step 39950/210000; acc:  47.85; ppl: 10.42; xent: 2.34; lr: 0.00001001;   0/466 tok/s;  66274 sec
[2020-03-30 14:05:22,379 INFO] Step 40000/210000; acc:  53.49; ppl: 10.11; xent: 2.31; lr: 0.00001000;   0/924 tok/s;  66357 sec
[2020-03-30 14:05:22,383 INFO] Saving checkpoint ../models/model_step_40000.pt
[2020-03-30 14:05:38,408 INFO] Loading train dataset from ../bert_data/cnndm.train.51.bert.pt, number of examples: 2000
[2020-03-30 14:06:48,823 INFO] Step 40050/210000; acc:  53.46; ppl:  9.34; xent: 2.23; lr: 0.00000999;   0/631 tok/s;  66444 sec
[2020-03-30 14:08:12,778 INFO] Step 40100/210000; acc:  46.14; ppl: 13.20; xent: 2.58; lr: 0.00000999;   0/953 tok/s;  66528 sec
[2020-03-30 14:08:24,878 INFO] Loading train dataset from ../bert_data/cnndm.train.52.bert.pt, number of examples: 2001
[2020-03-30 14:09:37,589 INFO] Step 40150/210000; acc:  52.89; ppl:  9.63; xent: 2.27; lr: 0.00000998;   0/668 tok/s;  66612 sec
[2020-03-30 14:11:01,470 INFO] Step 40200/210000; acc:  53.72; ppl: 10.67; xent: 2.37; lr: 0.00000998;   0/641 tok/s;  66696 sec
[2020-03-30 14:11:14,018 INFO] Loading train dataset from ../bert_data/cnndm.train.53.bert.pt, number of examples: 1999
[2020-03-30 14:12:26,352 INFO] Step 40250/210000; acc:  51.23; ppl: 11.08; xent: 2.41; lr: 0.00000997;   0/654 tok/s;  66781 sec
[2020-03-30 14:13:50,701 INFO] Step 40300/210000; acc:  49.89; ppl: 10.24; xent: 2.33; lr: 0.00000996;   0/552 tok/s;  66866 sec
[2020-03-30 14:14:01,160 INFO] Loading train dataset from ../bert_data/cnndm.train.54.bert.pt, number of examples: 2001
[2020-03-30 14:15:15,139 INFO] Step 40350/210000; acc:  56.34; ppl:  9.06; xent: 2.20; lr: 0.00000996;   0/799 tok/s;  66950 sec
[2020-03-30 14:16:39,260 INFO] Step 40400/210000; acc:  54.38; ppl:  9.00; xent: 2.20; lr: 0.00000995;   0/596 tok/s;  67034 sec
[2020-03-30 14:16:48,202 INFO] Loading train dataset from ../bert_data/cnndm.train.55.bert.pt, number of examples: 1999
[2020-03-30 14:18:03,794 INFO] Step 40450/210000; acc:  49.79; ppl: 12.72; xent: 2.54; lr: 0.00000994;   0/849 tok/s;  67119 sec
[2020-03-30 14:19:28,159 INFO] Step 40500/210000; acc:  57.70; ppl:  6.79; xent: 1.92; lr: 0.00000994;   0/656 tok/s;  67203 sec
[2020-03-30 14:19:28,163 INFO] Saving checkpoint ../models/model_step_40500.pt
[2020-03-30 14:19:39,596 INFO] Loading train dataset from ../bert_data/cnndm.train.56.bert.pt, number of examples: 2001
[2020-03-30 14:20:55,282 INFO] Step 40550/210000; acc:  48.99; ppl: 12.35; xent: 2.51; lr: 0.00000993;   0/747 tok/s;  67290 sec
[2020-03-30 14:22:19,618 INFO] Step 40600/210000; acc:  49.05; ppl: 12.07; xent: 2.49; lr: 0.00000993;   0/806 tok/s;  67374 sec
[2020-03-30 14:22:27,126 INFO] Loading train dataset from ../bert_data/cnndm.train.57.bert.pt, number of examples: 1999
[2020-03-30 14:23:44,565 INFO] Step 40650/210000; acc:  56.05; ppl:  8.28; xent: 2.11; lr: 0.00000992;   0/623 tok/s;  67459 sec
[2020-03-30 14:25:08,823 INFO] Step 40700/210000; acc:  46.24; ppl: 13.45; xent: 2.60; lr: 0.00000991;   0/490 tok/s;  67544 sec
[2020-03-30 14:25:14,215 INFO] Loading train dataset from ../bert_data/cnndm.train.58.bert.pt, number of examples: 2000
[2020-03-30 14:26:33,526 INFO] Step 40750/210000; acc:  50.79; ppl: 11.38; xent: 2.43; lr: 0.00000991;   0/511 tok/s;  67628 sec
[2020-03-30 14:27:58,267 INFO] Step 40800/210000; acc:  50.23; ppl: 10.80; xent: 2.38; lr: 0.00000990;   0/652 tok/s;  67713 sec
[2020-03-30 14:28:01,999 INFO] Loading train dataset from ../bert_data/cnndm.train.59.bert.pt, number of examples: 2000
[2020-03-30 14:29:23,390 INFO] Step 40850/210000; acc:  54.12; ppl: 10.22; xent: 2.32; lr: 0.00000990;   0/597 tok/s;  67798 sec
[2020-03-30 14:30:47,678 INFO] Step 40900/210000; acc:  51.89; ppl:  9.69; xent: 2.27; lr: 0.00000989;   0/653 tok/s;  67882 sec
[2020-03-30 14:30:49,782 INFO] Loading train dataset from ../bert_data/cnndm.train.6.bert.pt, number of examples: 2001
[2020-03-30 14:32:12,608 INFO] Step 40950/210000; acc:  55.82; ppl:  9.24; xent: 2.22; lr: 0.00000988;   0/818 tok/s;  67967 sec
[2020-03-30 14:33:37,439 INFO] Step 41000/210000; acc:  53.04; ppl: 10.18; xent: 2.32; lr: 0.00000988;   0/842 tok/s;  68052 sec
[2020-03-30 14:33:37,466 INFO] Saving checkpoint ../models/model_step_41000.pt
[2020-03-30 14:33:40,023 INFO] Loading train dataset from ../bert_data/cnndm.train.60.bert.pt, number of examples: 2000
[2020-03-30 14:35:03,891 INFO] Step 41050/210000; acc:  49.85; ppl: 11.24; xent: 2.42; lr: 0.00000987;   0/788 tok/s;  68139 sec
[2020-03-30 14:36:28,664 INFO] Step 41100/210000; acc:  50.50; ppl: 10.41; xent: 2.34; lr: 0.00000987;   0/754 tok/s;  68223 sec
[2020-03-30 14:36:29,041 INFO] Loading train dataset from ../bert_data/cnndm.train.61.bert.pt, number of examples: 2001
[2020-03-30 14:37:53,430 INFO] Step 41150/210000; acc:  45.78; ppl: 13.99; xent: 2.64; lr: 0.00000986;   0/728 tok/s;  68308 sec
[2020-03-30 14:39:15,964 INFO] Loading train dataset from ../bert_data/cnndm.train.62.bert.pt, number of examples: 2001
[2020-03-30 14:39:17,672 INFO] Step 41200/210000; acc:  50.50; ppl: 10.64; xent: 2.36; lr: 0.00000985;   0/472 tok/s;  68392 sec
[2020-03-30 14:40:42,270 INFO] Step 41250/210000; acc:  47.62; ppl: 14.47; xent: 2.67; lr: 0.00000985;   0/618 tok/s;  68477 sec
[2020-03-30 14:42:03,326 INFO] Loading train dataset from ../bert_data/cnndm.train.63.bert.pt, number of examples: 2001
[2020-03-30 14:42:06,719 INFO] Step 41300/210000; acc:  47.85; ppl: 14.01; xent: 2.64; lr: 0.00000984;   0/537 tok/s;  68562 sec
[2020-03-30 14:43:30,700 INFO] Step 41350/210000; acc:  48.69; ppl: 12.03; xent: 2.49; lr: 0.00000984;   0/932 tok/s;  68646 sec
[2020-03-30 14:44:52,129 INFO] Loading train dataset from ../bert_data/cnndm.train.64.bert.pt, number of examples: 2001
[2020-03-30 14:44:55,592 INFO] Step 41400/210000; acc:  49.50; ppl: 11.68; xent: 2.46; lr: 0.00000983;   0/551 tok/s;  68730 sec
[2020-03-30 14:46:19,641 INFO] Step 41450/210000; acc:  48.97; ppl: 11.52; xent: 2.44; lr: 0.00000982;   0/782 tok/s;  68814 sec
[2020-03-30 14:47:39,025 INFO] Loading train dataset from ../bert_data/cnndm.train.65.bert.pt, number of examples: 2000
[2020-03-30 14:47:44,169 INFO] Step 41500/210000; acc:  47.37; ppl: 12.95; xent: 2.56; lr: 0.00000982;   0/637 tok/s;  68899 sec
[2020-03-30 14:47:44,171 INFO] Saving checkpoint ../models/model_step_41500.pt
[2020-03-30 14:49:10,521 INFO] Step 41550/210000; acc:  47.38; ppl: 12.93; xent: 2.56; lr: 0.00000981;   0/592 tok/s;  68985 sec
[2020-03-30 14:50:28,679 INFO] Loading train dataset from ../bert_data/cnndm.train.66.bert.pt, number of examples: 2001
[2020-03-30 14:50:35,348 INFO] Step 41600/210000; acc:  54.58; ppl:  8.71; xent: 2.16; lr: 0.00000981;   0/797 tok/s;  69070 sec
[2020-03-30 14:51:59,474 INFO] Step 41650/210000; acc:  53.26; ppl: 10.86; xent: 2.38; lr: 0.00000980;   0/654 tok/s;  69154 sec
[2020-03-30 14:53:17,797 INFO] Loading train dataset from ../bert_data/cnndm.train.67.bert.pt, number of examples: 1999
[2020-03-30 14:53:24,518 INFO] Step 41700/210000; acc:  50.89; ppl: 12.11; xent: 2.49; lr: 0.00000979;   0/872 tok/s;  69239 sec
[2020-03-30 14:54:48,792 INFO] Step 41750/210000; acc:  53.52; ppl: 10.24; xent: 2.33; lr: 0.00000979;   0/741 tok/s;  69324 sec
[2020-03-30 14:56:05,445 INFO] Loading train dataset from ../bert_data/cnndm.train.68.bert.pt, number of examples: 1999
[2020-03-30 14:56:13,769 INFO] Step 41800/210000; acc:  50.96; ppl: 12.15; xent: 2.50; lr: 0.00000978;   0/1112 tok/s;  69409 sec
[2020-03-30 14:57:38,144 INFO] Step 41850/210000; acc:  47.66; ppl: 13.05; xent: 2.57; lr: 0.00000978;   0/918 tok/s;  69493 sec
[2020-03-30 14:58:53,423 INFO] Loading train dataset from ../bert_data/cnndm.train.69.bert.pt, number of examples: 2000
[2020-03-30 14:59:03,511 INFO] Step 41900/210000; acc:  48.63; ppl: 12.42; xent: 2.52; lr: 0.00000977;   0/559 tok/s;  69578 sec
[2020-03-30 15:00:27,692 INFO] Step 41950/210000; acc:  57.35; ppl:  7.14; xent: 1.97; lr: 0.00000976;   0/498 tok/s;  69663 sec
[2020-03-30 15:01:40,305 INFO] Loading train dataset from ../bert_data/cnndm.train.7.bert.pt, number of examples: 2001
[2020-03-30 15:01:52,196 INFO] Step 42000/210000; acc:  49.24; ppl: 12.46; xent: 2.52; lr: 0.00000976;   0/596 tok/s;  69747 sec
[2020-03-30 15:01:52,199 INFO] Saving checkpoint ../models/model_step_42000.pt
[2020-03-30 15:03:17,927 INFO] Step 42050/210000; acc:  51.24; ppl: 10.89; xent: 2.39; lr: 0.00000975;   0/570 tok/s;  69833 sec
[2020-03-30 15:04:29,628 INFO] Loading train dataset from ../bert_data/cnndm.train.70.bert.pt, number of examples: 2000
[2020-03-30 15:04:43,231 INFO] Step 42100/210000; acc:  52.90; ppl: 10.27; xent: 2.33; lr: 0.00000975;   0/610 tok/s;  69918 sec
[2020-03-30 15:06:07,206 INFO] Step 42150/210000; acc:  54.78; ppl:  9.06; xent: 2.20; lr: 0.00000974;   0/699 tok/s;  70002 sec
[2020-03-30 15:07:17,752 INFO] Loading train dataset from ../bert_data/cnndm.train.71.bert.pt, number of examples: 1999
[2020-03-30 15:07:31,189 INFO] Step 42200/210000; acc:  54.17; ppl:  7.88; xent: 2.06; lr: 0.00000974;   0/674 tok/s;  70086 sec
[2020-03-30 15:08:55,734 INFO] Step 42250/210000; acc:  53.18; ppl: 10.04; xent: 2.31; lr: 0.00000973;   0/736 tok/s;  70171 sec
[2020-03-30 15:10:05,098 INFO] Loading train dataset from ../bert_data/cnndm.train.72.bert.pt, number of examples: 2000
[2020-03-30 15:10:20,248 INFO] Step 42300/210000; acc:  55.77; ppl:  7.73; xent: 2.05; lr: 0.00000972;   0/745 tok/s;  70255 sec
[2020-03-30 15:11:44,793 INFO] Step 42350/210000; acc:  54.07; ppl:  9.10; xent: 2.21; lr: 0.00000972;   0/643 tok/s;  70340 sec
[2020-03-30 15:12:52,546 INFO] Loading train dataset from ../bert_data/cnndm.train.73.bert.pt, number of examples: 2000
[2020-03-30 15:13:09,279 INFO] Step 42400/210000; acc:  50.13; ppl: 12.49; xent: 2.53; lr: 0.00000971;   0/922 tok/s;  70424 sec
[2020-03-30 15:14:33,523 INFO] Step 42450/210000; acc:  48.78; ppl: 12.06; xent: 2.49; lr: 0.00000971;   0/1119 tok/s;  70508 sec
[2020-03-30 15:15:39,730 INFO] Loading train dataset from ../bert_data/cnndm.train.74.bert.pt, number of examples: 2000
[2020-03-30 15:15:58,237 INFO] Step 42500/210000; acc:  52.93; ppl: 10.83; xent: 2.38; lr: 0.00000970;   0/1092 tok/s;  70593 sec
[2020-03-30 15:15:58,240 INFO] Saving checkpoint ../models/model_step_42500.pt
[2020-03-30 15:17:24,394 INFO] Step 42550/210000; acc:  49.84; ppl: 11.24; xent: 2.42; lr: 0.00000970;   0/770 tok/s;  70679 sec
[2020-03-30 15:18:30,686 INFO] Loading train dataset from ../bert_data/cnndm.train.75.bert.pt, number of examples: 1999
[2020-03-30 15:18:49,274 INFO] Step 42600/210000; acc:  51.38; ppl: 11.20; xent: 2.42; lr: 0.00000969;   0/571 tok/s;  70764 sec
[2020-03-30 15:20:13,677 INFO] Step 42650/210000; acc:  49.83; ppl: 12.39; xent: 2.52; lr: 0.00000968;   0/853 tok/s;  70848 sec
[2020-03-30 15:21:18,161 INFO] Loading train dataset from ../bert_data/cnndm.train.76.bert.pt, number of examples: 1999
[2020-03-30 15:21:38,429 INFO] Step 42700/210000; acc:  56.56; ppl:  7.28; xent: 1.98; lr: 0.00000968;   0/543 tok/s;  70933 sec
[2020-03-30 15:23:02,791 INFO] Step 42750/210000; acc:  49.14; ppl: 12.17; xent: 2.50; lr: 0.00000967;   0/603 tok/s;  71018 sec
[2020-03-30 15:24:05,234 INFO] Loading train dataset from ../bert_data/cnndm.train.77.bert.pt, number of examples: 2001
[2020-03-30 15:24:27,312 INFO] Step 42800/210000; acc:  54.27; ppl:  9.61; xent: 2.26; lr: 0.00000967;   0/728 tok/s;  71102 sec
[2020-03-30 15:25:51,909 INFO] Step 42850/210000; acc:  44.36; ppl: 14.96; xent: 2.71; lr: 0.00000966;   0/599 tok/s;  71187 sec
[2020-03-30 15:26:53,086 INFO] Loading train dataset from ../bert_data/cnndm.train.78.bert.pt, number of examples: 2001
[2020-03-30 15:27:16,529 INFO] Step 42900/210000; acc:  51.13; ppl: 12.31; xent: 2.51; lr: 0.00000966;   0/761 tok/s;  71271 sec
[2020-03-30 15:28:40,279 INFO] Step 42950/210000; acc:  53.03; ppl:  9.83; xent: 2.29; lr: 0.00000965;   0/771 tok/s;  71355 sec
[2020-03-30 15:29:41,201 INFO] Loading train dataset from ../bert_data/cnndm.train.79.bert.pt, number of examples: 1999
[2020-03-30 15:30:04,563 INFO] Step 43000/210000; acc:  53.47; ppl:  8.63; xent: 2.16; lr: 0.00000964;   0/735 tok/s;  71439 sec
[2020-03-30 15:30:04,566 INFO] Saving checkpoint ../models/model_step_43000.pt
[2020-03-30 15:31:31,182 INFO] Step 43050/210000; acc:  49.48; ppl: 11.93; xent: 2.48; lr: 0.00000964;   0/810 tok/s;  71526 sec
[2020-03-30 15:32:30,207 INFO] Loading train dataset from ../bert_data/cnndm.train.8.bert.pt, number of examples: 2000
[2020-03-30 15:32:55,599 INFO] Step 43100/210000; acc:  53.36; ppl:  9.99; xent: 2.30; lr: 0.00000963;   0/893 tok/s;  71610 sec
[2020-03-30 15:34:20,463 INFO] Step 43150/210000; acc:  49.47; ppl: 11.40; xent: 2.43; lr: 0.00000963;   0/1008 tok/s;  71695 sec
[2020-03-30 15:35:18,106 INFO] Loading train dataset from ../bert_data/cnndm.train.80.bert.pt, number of examples: 1999
[2020-03-30 15:35:45,290 INFO] Step 43200/210000; acc:  45.11; ppl: 14.12; xent: 2.65; lr: 0.00000962;   0/660 tok/s;  71780 sec
[2020-03-30 15:37:09,609 INFO] Step 43250/210000; acc:  49.56; ppl: 11.28; xent: 2.42; lr: 0.00000962;   0/855 tok/s;  71864 sec
[2020-03-30 15:38:05,252 INFO] Loading train dataset from ../bert_data/cnndm.train.81.bert.pt, number of examples: 2000
[2020-03-30 15:38:33,985 INFO] Step 43300/210000; acc:  56.21; ppl:  7.87; xent: 2.06; lr: 0.00000961;   0/540 tok/s;  71949 sec
[2020-03-30 15:39:58,366 INFO] Step 43350/210000; acc:  54.17; ppl:  8.96; xent: 2.19; lr: 0.00000961;   0/702 tok/s;  72033 sec
[2020-03-30 15:40:52,339 INFO] Loading train dataset from ../bert_data/cnndm.train.82.bert.pt, number of examples: 2001
[2020-03-30 15:41:22,741 INFO] Step 43400/210000; acc:  56.50; ppl:  7.29; xent: 1.99; lr: 0.00000960;   0/619 tok/s;  72118 sec
[2020-03-30 15:42:46,814 INFO] Step 43450/210000; acc:  52.97; ppl:  8.31; xent: 2.12; lr: 0.00000959;   0/590 tok/s;  72202 sec
[2020-03-30 15:43:41,571 INFO] Loading train dataset from ../bert_data/cnndm.train.83.bert.pt, number of examples: 1999
[2020-03-30 15:44:11,916 INFO] Step 43500/210000; acc:  51.32; ppl: 10.09; xent: 2.31; lr: 0.00000959;   0/642 tok/s;  72287 sec
[2020-03-30 15:44:11,919 INFO] Saving checkpoint ../models/model_step_43500.pt
[2020-03-30 15:45:38,437 INFO] Step 43550/210000; acc:  55.33; ppl:  8.85; xent: 2.18; lr: 0.00000958;   0/754 tok/s;  72373 sec
[2020-03-30 15:46:31,683 INFO] Loading train dataset from ../bert_data/cnndm.train.84.bert.pt, number of examples: 2000
[2020-03-30 15:47:03,517 INFO] Step 43600/210000; acc:  47.18; ppl: 13.23; xent: 2.58; lr: 0.00000958;   0/844 tok/s;  72458 sec
[2020-03-30 15:48:27,983 INFO] Step 43650/210000; acc:  50.25; ppl: 10.39; xent: 2.34; lr: 0.00000957;   0/844 tok/s;  72543 sec
[2020-03-30 15:49:19,181 INFO] Loading train dataset from ../bert_data/cnndm.train.85.bert.pt, number of examples: 2001
[2020-03-30 15:49:52,906 INFO] Step 43700/210000; acc:  51.39; ppl: 11.80; xent: 2.47; lr: 0.00000957;   0/1051 tok/s;  72628 sec
[2020-03-30 15:51:16,996 INFO] Step 43750/210000; acc:  46.25; ppl: 15.62; xent: 2.75; lr: 0.00000956;   0/987 tok/s;  72712 sec
[2020-03-30 15:52:06,045 INFO] Loading train dataset from ../bert_data/cnndm.train.86.bert.pt, number of examples: 1999
[2020-03-30 15:52:41,316 INFO] Step 43800/210000; acc:  49.83; ppl: 10.40; xent: 2.34; lr: 0.00000956;   0/858 tok/s;  72796 sec
[2020-03-30 15:54:05,934 INFO] Step 43850/210000; acc:  49.70; ppl: 10.59; xent: 2.36; lr: 0.00000955;   0/853 tok/s;  72881 sec
[2020-03-30 15:54:53,342 INFO] Loading train dataset from ../bert_data/cnndm.train.87.bert.pt, number of examples: 1999
[2020-03-30 15:55:30,822 INFO] Step 43900/210000; acc:  52.26; ppl: 10.25; xent: 2.33; lr: 0.00000955;   0/603 tok/s;  72966 sec
[2020-03-30 15:56:55,479 INFO] Step 43950/210000; acc:  52.95; ppl:  9.95; xent: 2.30; lr: 0.00000954;   0/556 tok/s;  73050 sec
[2020-03-30 15:57:41,244 INFO] Loading train dataset from ../bert_data/cnndm.train.88.bert.pt, number of examples: 1999
[2020-03-30 15:58:19,825 INFO] Step 44000/210000; acc:  46.75; ppl: 13.07; xent: 2.57; lr: 0.00000953;   0/681 tok/s;  73135 sec
[2020-03-30 15:58:19,828 INFO] Saving checkpoint ../models/model_step_44000.pt
[2020-03-30 15:59:45,995 INFO] Step 44050/210000; acc:  53.97; ppl: 10.08; xent: 2.31; lr: 0.00000953;   0/649 tok/s;  73221 sec
[2020-03-30 16:00:30,101 INFO] Loading train dataset from ../bert_data/cnndm.train.89.bert.pt, number of examples: 2001
[2020-03-30 16:01:10,099 INFO] Step 44100/210000; acc:  52.78; ppl: 10.17; xent: 2.32; lr: 0.00000952;   0/729 tok/s;  73305 sec
[2020-03-30 16:02:34,314 INFO] Step 44150/210000; acc:  54.84; ppl:  9.96; xent: 2.30; lr: 0.00000952;   0/650 tok/s;  73389 sec
[2020-03-30 16:03:18,769 INFO] Loading train dataset from ../bert_data/cnndm.train.9.bert.pt, number of examples: 1999
[2020-03-30 16:03:59,232 INFO] Step 44200/210000; acc:  52.28; ppl:  9.48; xent: 2.25; lr: 0.00000951;   0/906 tok/s;  73474 sec
[2020-03-30 16:05:23,405 INFO] Step 44250/210000; acc:  53.44; ppl:  8.91; xent: 2.19; lr: 0.00000951;   0/860 tok/s;  73558 sec
[2020-03-30 16:06:06,061 INFO] Loading train dataset from ../bert_data/cnndm.train.90.bert.pt, number of examples: 2000
[2020-03-30 16:06:48,405 INFO] Step 44300/210000; acc:  52.58; ppl:  9.50; xent: 2.25; lr: 0.00000950;   0/1079 tok/s;  73643 sec
[2020-03-30 16:08:13,053 INFO] Step 44350/210000; acc:  52.78; ppl:  9.86; xent: 2.29; lr: 0.00000950;   0/766 tok/s;  73728 sec
[2020-03-30 16:08:53,845 INFO] Loading train dataset from ../bert_data/cnndm.train.9000.bert.pt, number of examples: 1984
[2020-03-30 16:09:36,527 INFO] Step 44400/210000; acc:  46.66; ppl: 10.94; xent: 2.39; lr: 0.00000949;   0/507 tok/s;  73811 sec
[2020-03-30 16:10:58,583 INFO] Step 44450/210000; acc:  39.49; ppl: 21.11; xent: 3.05; lr: 0.00000949;   0/1493 tok/s;  73893 sec
[2020-03-30 16:11:28,463 INFO] Loading train dataset from ../bert_data/cnndm.train.9001.bert.pt, number of examples: 1983
[2020-03-30 16:12:20,974 INFO] Step 44500/210000; acc:  44.88; ppl: 13.05; xent: 2.57; lr: 0.00000948;   0/674 tok/s;  73976 sec
[2020-03-30 16:12:20,977 INFO] Saving checkpoint ../models/model_step_44500.pt
[2020-03-30 16:13:45,450 INFO] Step 44550/210000; acc:  41.31; ppl: 17.62; xent: 2.87; lr: 0.00000948;   0/1251 tok/s;  74060 sec
[2020-03-30 16:14:09,292 INFO] Loading train dataset from ../bert_data/cnndm.train.9002.bert.pt, number of examples: 1990
[2020-03-30 16:15:08,177 INFO] Step 44600/210000; acc:  42.40; ppl: 13.21; xent: 2.58; lr: 0.00000947;   0/866 tok/s;  74143 sec
[2020-03-30 16:16:30,261 INFO] Step 44650/210000; acc:  38.40; ppl: 20.68; xent: 3.03; lr: 0.00000946;   0/1258 tok/s;  74225 sec
[2020-03-30 16:16:45,443 INFO] Loading train dataset from ../bert_data/cnndm.train.9003.bert.pt, number of examples: 1985
[2020-03-30 16:17:52,904 INFO] Step 44700/210000; acc:  47.00; ppl: 10.79; xent: 2.38; lr: 0.00000946;   0/698 tok/s;  74308 sec
[2020-03-30 16:19:14,737 INFO] Step 44750/210000; acc:  41.73; ppl: 18.95; xent: 2.94; lr: 0.00000945;   0/1005 tok/s;  74390 sec
[2020-03-30 16:19:19,909 INFO] Loading train dataset from ../bert_data/cnndm.train.9004.bert.pt, number of examples: 1981
[2020-03-30 16:20:36,819 INFO] Step 44800/210000; acc:  44.40; ppl: 14.45; xent: 2.67; lr: 0.00000945;   0/728 tok/s;  74472 sec
[2020-03-30 16:21:55,904 INFO] Loading train dataset from ../bert_data/cnndm.train.9005.bert.pt, number of examples: 1986
[2020-03-30 16:21:59,277 INFO] Step 44850/210000; acc:  56.77; ppl:  7.03; xent: 1.95; lr: 0.00000944;   0/419 tok/s;  74554 sec
[2020-03-30 16:23:20,914 INFO] Step 44900/210000; acc:  48.31; ppl:  9.93; xent: 2.30; lr: 0.00000944;   0/942 tok/s;  74636 sec
[2020-03-30 16:24:32,467 INFO] Loading train dataset from ../bert_data/cnndm.train.9006.bert.pt, number of examples: 1993
[2020-03-30 16:24:43,895 INFO] Step 44950/210000; acc:  51.26; ppl:  7.12; xent: 1.96; lr: 0.00000943;   0/469 tok/s;  74719 sec
[2020-03-30 16:26:05,670 INFO] Step 45000/210000; acc:  44.09; ppl: 14.43; xent: 2.67; lr: 0.00000943;   0/1114 tok/s;  74800 sec
[2020-03-30 16:26:05,674 INFO] Saving checkpoint ../models/model_step_45000.pt
[2020-03-30 16:27:11,095 INFO] Loading train dataset from ../bert_data/cnndm.train.9007.bert.pt, number of examples: 1983
[2020-03-30 16:27:31,200 INFO] Step 45050/210000; acc:  46.89; ppl:  9.89; xent: 2.29; lr: 0.00000942;   0/492 tok/s;  74886 sec
[2020-03-30 16:28:52,885 INFO] Step 45100/210000; acc:  41.46; ppl: 15.98; xent: 2.77; lr: 0.00000942;   0/1228 tok/s;  74968 sec
[2020-03-30 16:29:47,782 INFO] Loading train dataset from ../bert_data/cnndm.train.9008.bert.pt, number of examples: 1990
[2020-03-30 16:30:15,596 INFO] Step 45150/210000; acc:  52.37; ppl:  8.78; xent: 2.17; lr: 0.00000941;   0/675 tok/s;  75050 sec
[2020-03-30 16:31:38,520 INFO] Step 45200/210000; acc:  38.62; ppl: 20.12; xent: 3.00; lr: 0.00000941;   0/1513 tok/s;  75133 sec
[2020-03-30 16:32:24,868 INFO] Loading train dataset from ../bert_data/cnndm.train.9009.bert.pt, number of examples: 1988
[2020-03-30 16:33:01,723 INFO] Step 45250/210000; acc:  47.66; ppl: 10.09; xent: 2.31; lr: 0.00000940;   0/723 tok/s;  75217 sec
[2020-03-30 16:34:24,863 INFO] Step 45300/210000; acc:  41.91; ppl: 16.11; xent: 2.78; lr: 0.00000940;   0/1022 tok/s;  75300 sec
[2020-03-30 16:35:03,687 INFO] Loading train dataset from ../bert_data/cnndm.train.9010.bert.pt, number of examples: 1984
[2020-03-30 16:35:47,945 INFO] Step 45350/210000; acc:  46.72; ppl: 11.24; xent: 2.42; lr: 0.00000939;   0/893 tok/s;  75383 sec
[2020-03-30 16:37:09,963 INFO] Step 45400/210000; acc:  47.54; ppl: 10.86; xent: 2.39; lr: 0.00000939;   0/573 tok/s;  75465 sec
[2020-03-30 16:37:38,204 INFO] Loading train dataset from ../bert_data/cnndm.train.9011.bert.pt, number of examples: 1983
[2020-03-30 16:38:32,768 INFO] Step 45450/210000; acc:  45.77; ppl: 12.71; xent: 2.54; lr: 0.00000938;   0/1040 tok/s;  75548 sec
[2020-03-30 16:39:54,365 INFO] Step 45500/210000; acc:  46.15; ppl: 11.14; xent: 2.41; lr: 0.00000938;   0/650 tok/s;  75629 sec
[2020-03-30 16:39:54,368 INFO] Saving checkpoint ../models/model_step_45500.pt
[2020-03-30 16:40:18,782 INFO] Loading train dataset from ../bert_data/cnndm.train.9012.bert.pt, number of examples: 1985
[2020-03-30 16:41:19,861 INFO] Step 45550/210000; acc:  44.95; ppl: 13.60; xent: 2.61; lr: 0.00000937;   0/1066 tok/s;  75715 sec
[2020-03-30 16:42:43,155 INFO] Step 45600/210000; acc:  55.25; ppl:  6.72; xent: 1.90; lr: 0.00000937;   0/418 tok/s;  75798 sec
[2020-03-30 16:42:54,769 INFO] Loading train dataset from ../bert_data/cnndm.train.9013.bert.pt, number of examples: 1987
[2020-03-30 16:44:05,936 INFO] Step 45650/210000; acc:  45.31; ppl: 13.25; xent: 2.58; lr: 0.00000936;   0/1279 tok/s;  75881 sec
[2020-03-30 16:45:28,124 INFO] Step 45700/210000; acc:  53.04; ppl:  8.17; xent: 2.10; lr: 0.00000936;   0/618 tok/s;  75963 sec
[2020-03-30 16:45:31,696 INFO] Loading train dataset from ../bert_data/cnndm.train.9014.bert.pt, number of examples: 1976
[2020-03-30 16:46:51,210 INFO] Step 45750/210000; acc:  47.20; ppl: 12.44; xent: 2.52; lr: 0.00000935;   0/1211 tok/s;  76046 sec
[2020-03-30 16:48:08,177 INFO] Loading train dataset from ../bert_data/cnndm.train.9015.bert.pt, number of examples: 1982
[2020-03-30 16:48:14,915 INFO] Step 45800/210000; acc:  41.52; ppl: 15.56; xent: 2.74; lr: 0.00000935;   0/1173 tok/s;  76130 sec
[2020-03-30 16:49:37,508 INFO] Step 45850/210000; acc:  55.19; ppl:  7.48; xent: 2.01; lr: 0.00000934;   0/465 tok/s;  76212 sec
[2020-03-30 16:50:43,363 INFO] Loading train dataset from ../bert_data/cnndm.train.9016.bert.pt, number of examples: 1984
[2020-03-30 16:50:59,377 INFO] Step 45900/210000; acc:  40.05; ppl: 16.59; xent: 2.81; lr: 0.00000934;   0/1472 tok/s;  76294 sec
[2020-03-30 16:52:20,990 INFO] Step 45950/210000; acc:  52.82; ppl:  7.68; xent: 2.04; lr: 0.00000933;   0/460 tok/s;  76376 sec
[2020-03-30 16:53:18,946 INFO] Loading train dataset from ../bert_data/cnndm.train.9017.bert.pt, number of examples: 1984
[2020-03-30 16:53:44,130 INFO] Step 46000/210000; acc:  43.34; ppl: 13.85; xent: 2.63; lr: 0.00000933;   0/1445 tok/s;  76459 sec
[2020-03-30 16:53:44,133 INFO] Saving checkpoint ../models/model_step_46000.pt
[2020-03-30 16:55:08,050 INFO] Step 46050/210000; acc:  56.72; ppl:  6.72; xent: 1.91; lr: 0.00000932;   0/504 tok/s;  76543 sec
[2020-03-30 16:55:59,574 INFO] Loading train dataset from ../bert_data/cnndm.train.9018.bert.pt, number of examples: 1989
[2020-03-30 16:56:30,704 INFO] Step 46100/210000; acc:  41.00; ppl: 16.68; xent: 2.81; lr: 0.00000931;   0/1440 tok/s;  76626 sec
[2020-03-30 16:57:52,675 INFO] Step 46150/210000; acc:  52.44; ppl:  8.48; xent: 2.14; lr: 0.00000931;   0/591 tok/s;  76707 sec
[2020-03-30 16:58:36,226 INFO] Loading train dataset from ../bert_data/cnndm.train.9019.bert.pt, number of examples: 1986
[2020-03-30 16:59:15,992 INFO] Step 46200/210000; acc:  46.04; ppl: 12.15; xent: 2.50; lr: 0.00000930;   0/1020 tok/s;  76791 sec
[2020-03-30 17:00:37,259 INFO] Step 46250/210000; acc:  48.29; ppl:  9.23; xent: 2.22; lr: 0.00000930;   0/606 tok/s;  76872 sec
[2020-03-30 17:01:12,235 INFO] Loading train dataset from ../bert_data/cnndm.train.9020.bert.pt, number of examples: 1992
[2020-03-30 17:01:59,600 INFO] Step 46300/210000; acc:  46.62; ppl: 11.35; xent: 2.43; lr: 0.00000929;   0/956 tok/s;  76954 sec
[2020-03-30 17:03:21,734 INFO] Step 46350/210000; acc:  50.32; ppl:  8.44; xent: 2.13; lr: 0.00000929;   0/803 tok/s;  77037 sec
[2020-03-30 17:03:49,159 INFO] Loading train dataset from ../bert_data/cnndm.train.9021.bert.pt, number of examples: 1988
[2020-03-30 17:04:45,477 INFO] Step 46400/210000; acc:  41.92; ppl: 15.65; xent: 2.75; lr: 0.00000928;   0/1418 tok/s;  77120 sec
[2020-03-30 17:06:06,937 INFO] Step 46450/210000; acc:  50.41; ppl:  9.23; xent: 2.22; lr: 0.00000928;   0/649 tok/s;  77202 sec
[2020-03-30 17:06:25,101 INFO] Loading train dataset from ../bert_data/cnndm.train.9022.bert.pt, number of examples: 1991
[2020-03-30 17:07:28,872 INFO] Step 46500/210000; acc:  39.38; ppl: 18.20; xent: 2.90; lr: 0.00000927;   0/1258 tok/s;  77284 sec
[2020-03-30 17:07:28,875 INFO] Saving checkpoint ../models/model_step_46500.pt
[2020-03-30 17:08:52,438 INFO] Step 46550/210000; acc:  51.58; ppl:  8.07; xent: 2.09; lr: 0.00000927;   0/541 tok/s;  77367 sec
[2020-03-30 17:09:04,493 INFO] Loading train dataset from ../bert_data/cnndm.train.9023.bert.pt, number of examples: 1986
[2020-03-30 17:10:14,839 INFO] Step 46600/210000; acc:  36.98; ppl: 25.39; xent: 3.23; lr: 0.00000926;   0/1361 tok/s;  77450 sec
[2020-03-30 17:11:37,329 INFO] Step 46650/210000; acc:  50.90; ppl:  8.18; xent: 2.10; lr: 0.00000926;   0/654 tok/s;  77532 sec
[2020-03-30 17:11:41,321 INFO] Loading train dataset from ../bert_data/cnndm.train.9024.bert.pt, number of examples: 1989
[2020-03-30 17:13:00,095 INFO] Step 46700/210000; acc:  41.98; ppl: 17.21; xent: 2.85; lr: 0.00000925;   0/1166 tok/s;  77615 sec
[2020-03-30 17:14:17,837 INFO] Loading train dataset from ../bert_data/cnndm.train.9025.bert.pt, number of examples: 1984
[2020-03-30 17:14:22,774 INFO] Step 46750/210000; acc:  54.93; ppl:  7.67; xent: 2.04; lr: 0.00000925;   0/774 tok/s;  77698 sec
[2020-03-30 17:15:45,001 INFO] Step 46800/210000; acc:  52.54; ppl:  8.45; xent: 2.13; lr: 0.00000925;   0/534 tok/s;  77780 sec
[2020-03-30 17:16:52,498 INFO] Loading train dataset from ../bert_data/cnndm.train.9026.bert.pt, number of examples: 1986
[2020-03-30 17:17:07,796 INFO] Step 46850/210000; acc:  41.64; ppl: 16.49; xent: 2.80; lr: 0.00000924;   0/1212 tok/s;  77863 sec
[2020-03-30 17:18:29,892 INFO] Step 46900/210000; acc:  54.35; ppl:  7.37; xent: 2.00; lr: 0.00000924;   0/373 tok/s;  77945 sec
[2020-03-30 17:19:29,710 INFO] Loading train dataset from ../bert_data/cnndm.train.9027.bert.pt, number of examples: 1989
[2020-03-30 17:19:52,717 INFO] Step 46950/210000; acc:  47.61; ppl: 11.68; xent: 2.46; lr: 0.00000923;   0/1207 tok/s;  78028 sec
[2020-03-30 17:21:14,783 INFO] Step 47000/210000; acc:  54.02; ppl:  7.61; xent: 2.03; lr: 0.00000923;   0/520 tok/s;  78110 sec
[2020-03-30 17:21:14,786 INFO] Saving checkpoint ../models/model_step_47000.pt
[2020-03-30 17:22:08,200 INFO] Loading train dataset from ../bert_data/cnndm.train.9028.bert.pt, number of examples: 1986
[2020-03-30 17:22:39,460 INFO] Step 47050/210000; acc:  45.34; ppl: 11.54; xent: 2.45; lr: 0.00000922;   0/1102 tok/s;  78194 sec
[2020-03-30 17:24:02,313 INFO] Step 47100/210000; acc:  52.12; ppl:  8.30; xent: 2.12; lr: 0.00000922;   0/470 tok/s;  78277 sec
[2020-03-30 17:24:47,148 INFO] Loading train dataset from ../bert_data/cnndm.train.9029.bert.pt, number of examples: 1981
[2020-03-30 17:25:24,812 INFO] Step 47150/210000; acc:  42.73; ppl: 14.50; xent: 2.67; lr: 0.00000921;   0/1151 tok/s;  78360 sec
[2020-03-30 17:26:47,642 INFO] Step 47200/210000; acc:  49.51; ppl:  9.28; xent: 2.23; lr: 0.00000921;   0/443 tok/s;  78442 sec
[2020-03-30 17:27:23,126 INFO] Loading train dataset from ../bert_data/cnndm.train.9030.bert.pt, number of examples: 1984
[2020-03-30 17:28:10,828 INFO] Step 47250/210000; acc:  41.07; ppl: 16.63; xent: 2.81; lr: 0.00000920;   0/1392 tok/s;  78526 sec
[2020-03-30 17:29:33,164 INFO] Step 47300/210000; acc:  57.21; ppl:  6.23; xent: 1.83; lr: 0.00000920;   0/523 tok/s;  78608 sec
[2020-03-30 17:30:00,371 INFO] Loading train dataset from ../bert_data/cnndm.train.9031.bert.pt, number of examples: 1975
[2020-03-30 17:30:56,404 INFO] Step 47350/210000; acc:  45.08; ppl: 12.72; xent: 2.54; lr: 0.00000919;   0/724 tok/s;  78691 sec
[2020-03-30 17:32:19,784 INFO] Step 47400/210000; acc:  47.52; ppl: 10.83; xent: 2.38; lr: 0.00000919;   0/933 tok/s;  78775 sec
[2020-03-30 17:32:35,009 INFO] Loading train dataset from ../bert_data/cnndm.train.9032.bert.pt, number of examples: 1986
[2020-03-30 17:33:42,326 INFO] Step 47450/210000; acc:  53.97; ppl:  8.50; xent: 2.14; lr: 0.00000918;   0/485 tok/s;  78857 sec
[2020-03-30 17:35:04,487 INFO] Step 47500/210000; acc:  38.34; ppl: 18.05; xent: 2.89; lr: 0.00000918;   0/1254 tok/s;  78939 sec
[2020-03-30 17:35:04,491 INFO] Saving checkpoint ../models/model_step_47500.pt
[2020-03-30 17:35:13,418 INFO] Loading train dataset from ../bert_data/cnndm.train.9033.bert.pt, number of examples: 1986
[2020-03-30 17:36:28,913 INFO] Step 47550/210000; acc:  56.07; ppl:  6.39; xent: 1.86; lr: 0.00000917;   0/601 tok/s;  79024 sec
[2020-03-30 17:37:49,057 INFO] Loading train dataset from ../bert_data/cnndm.train.9034.bert.pt, number of examples: 1989
[2020-03-30 17:37:50,767 INFO] Step 47600/210000; acc:  45.53; ppl: 13.99; xent: 2.64; lr: 0.00000917;   0/664 tok/s;  79106 sec
[2020-03-30 17:39:12,073 INFO] Step 47650/210000; acc:  51.87; ppl:  9.43; xent: 2.24; lr: 0.00000916;   0/806 tok/s;  79187 sec
[2020-03-30 17:40:24,326 INFO] Loading train dataset from ../bert_data/cnndm.train.9035.bert.pt, number of examples: 1988
[2020-03-30 17:40:34,241 INFO] Step 47700/210000; acc:  58.02; ppl:  6.02; xent: 1.80; lr: 0.00000916;   0/408 tok/s;  79269 sec
[2020-03-30 17:41:55,675 INFO] Step 47750/210000; acc:  44.62; ppl: 14.14; xent: 2.65; lr: 0.00000915;   0/899 tok/s;  79350 sec
[2020-03-30 17:42:59,062 INFO] Loading train dataset from ../bert_data/cnndm.train.9036.bert.pt, number of examples: 1984
[2020-03-30 17:43:17,616 INFO] Step 47800/210000; acc:  55.98; ppl:  6.07; xent: 1.80; lr: 0.00000915;   0/383 tok/s;  79432 sec
[2020-03-30 17:44:40,016 INFO] Step 47850/210000; acc:  40.66; ppl: 16.73; xent: 2.82; lr: 0.00000914;   0/1203 tok/s;  79515 sec
[2020-03-30 17:45:35,514 INFO] Loading train dataset from ../bert_data/cnndm.train.9037.bert.pt, number of examples: 1983
[2020-03-30 17:46:03,230 INFO] Step 47900/210000; acc:  49.52; ppl:  9.97; xent: 2.30; lr: 0.00000914;   0/874 tok/s;  79598 sec
[2020-03-30 17:47:25,364 INFO] Step 47950/210000; acc:  48.74; ppl: 10.86; xent: 2.38; lr: 0.00000913;   0/872 tok/s;  79680 sec
[2020-03-30 17:48:09,325 INFO] Loading train dataset from ../bert_data/cnndm.train.9038.bert.pt, number of examples: 1982
[2020-03-30 17:48:47,372 INFO] Step 48000/210000; acc:  42.70; ppl: 14.29; xent: 2.66; lr: 0.00000913;   0/943 tok/s;  79762 sec
[2020-03-30 17:48:47,374 INFO] Saving checkpoint ../models/model_step_48000.pt
[2020-03-30 17:50:12,065 INFO] Step 48050/210000; acc:  44.16; ppl: 14.66; xent: 2.69; lr: 0.00000912;   0/882 tok/s;  79847 sec
[2020-03-30 17:50:48,423 INFO] Loading train dataset from ../bert_data/cnndm.train.9039.bert.pt, number of examples: 1983
[2020-03-30 17:51:34,191 INFO] Step 48100/210000; acc:  46.01; ppl: 11.28; xent: 2.42; lr: 0.00000912;   0/993 tok/s;  79929 sec
[2020-03-30 17:52:55,217 INFO] Step 48150/210000; acc:  55.36; ppl:  6.78; xent: 1.91; lr: 0.00000911;   0/426 tok/s;  80010 sec
[2020-03-30 17:53:23,975 INFO] Loading train dataset from ../bert_data/cnndm.train.9040.bert.pt, number of examples: 1982
[2020-03-30 17:54:18,711 INFO] Step 48200/210000; acc:  43.17; ppl: 13.60; xent: 2.61; lr: 0.00000911;   0/1513 tok/s;  80094 sec
[2020-03-30 17:55:40,510 INFO] Step 48250/210000; acc:  52.28; ppl:  7.52; xent: 2.02; lr: 0.00000911;   0/543 tok/s;  80175 sec
[2020-03-30 17:55:59,322 INFO] Loading train dataset from ../bert_data/cnndm.train.9041.bert.pt, number of examples: 1986
[2020-03-30 17:57:03,721 INFO] Step 48300/210000; acc:  44.81; ppl: 13.04; xent: 2.57; lr: 0.00000910;   0/994 tok/s;  80259 sec
[2020-03-30 17:58:24,766 INFO] Step 48350/210000; acc:  47.37; ppl: 10.43; xent: 2.34; lr: 0.00000910;   0/621 tok/s;  80340 sec
[2020-03-30 17:58:34,875 INFO] Loading train dataset from ../bert_data/cnndm.train.9042.bert.pt, number of examples: 1984
[2020-03-30 17:59:46,837 INFO] Step 48400/210000; acc:  45.17; ppl: 14.55; xent: 2.68; lr: 0.00000909;   0/1105 tok/s;  80422 sec
[2020-03-30 18:01:08,386 INFO] Step 48450/210000; acc:  51.89; ppl:  8.75; xent: 2.17; lr: 0.00000909;   0/804 tok/s;  80503 sec
[2020-03-30 18:01:10,392 INFO] Loading train dataset from ../bert_data/cnndm.train.9043.bert.pt, number of examples: 1984
[2020-03-30 18:02:30,586 INFO] Step 48500/210000; acc:  49.52; ppl: 10.43; xent: 2.34; lr: 0.00000908;   0/563 tok/s;  80585 sec
[2020-03-30 18:02:30,589 INFO] Saving checkpoint ../models/model_step_48500.pt
[2020-03-30 18:03:49,021 INFO] Loading train dataset from ../bert_data/cnndm.train.9044.bert.pt, number of examples: 1988
[2020-03-30 18:03:55,674 INFO] Step 48550/210000; acc:  49.09; ppl:  9.19; xent: 2.22; lr: 0.00000908;   0/1019 tok/s;  80670 sec
[2020-03-30 18:05:18,182 INFO] Step 48600/210000; acc:  55.19; ppl:  6.48; xent: 1.87; lr: 0.00000907;   0/416 tok/s;  80753 sec
[2020-03-30 18:06:26,072 INFO] Loading train dataset from ../bert_data/cnndm.train.9045.bert.pt, number of examples: 1983
[2020-03-30 18:06:40,640 INFO] Step 48650/210000; acc:  47.30; ppl: 10.94; xent: 2.39; lr: 0.00000907;   0/1081 tok/s;  80835 sec
[2020-03-30 18:08:03,209 INFO] Step 48700/210000; acc:  49.80; ppl:  9.09; xent: 2.21; lr: 0.00000906;   0/510 tok/s;  80918 sec
[2020-03-30 18:09:00,713 INFO] Loading train dataset from ../bert_data/cnndm.train.9046.bert.pt, number of examples: 1983
[2020-03-30 18:09:25,413 INFO] Step 48750/210000; acc:  41.01; ppl: 17.71; xent: 2.87; lr: 0.00000906;   0/1317 tok/s;  81000 sec
[2020-03-30 18:10:47,919 INFO] Step 48800/210000; acc:  51.96; ppl:  8.17; xent: 2.10; lr: 0.00000905;   0/636 tok/s;  81083 sec
[2020-03-30 18:11:37,953 INFO] Loading train dataset from ../bert_data/cnndm.train.9047.bert.pt, number of examples: 1983
[2020-03-30 18:12:11,302 INFO] Step 48850/210000; acc:  45.35; ppl: 12.78; xent: 2.55; lr: 0.00000905;   0/859 tok/s;  81166 sec
[2020-03-30 18:13:33,357 INFO] Step 48900/210000; acc:  47.81; ppl: 11.24; xent: 2.42; lr: 0.00000904;   0/819 tok/s;  81248 sec
[2020-03-30 18:14:14,864 INFO] Loading train dataset from ../bert_data/cnndm.train.9048.bert.pt, number of examples: 1984
[2020-03-30 18:14:55,682 INFO] Step 48950/210000; acc:  57.32; ppl:  6.91; xent: 1.93; lr: 0.00000904;   0/588 tok/s;  81331 sec
[2020-03-30 18:16:17,512 INFO] Step 49000/210000; acc:  47.20; ppl: 10.63; xent: 2.36; lr: 0.00000904;   0/888 tok/s;  81412 sec
[2020-03-30 18:16:17,516 INFO] Saving checkpoint ../models/model_step_49000.pt
[2020-03-30 18:16:53,270 INFO] Loading train dataset from ../bert_data/cnndm.train.9049.bert.pt, number of examples: 1978
[2020-03-30 18:17:43,006 INFO] Step 49050/210000; acc:  54.67; ppl:  7.11; xent: 1.96; lr: 0.00000903;   0/408 tok/s;  81498 sec
[2020-03-30 18:19:04,644 INFO] Step 49100/210000; acc:  45.20; ppl: 12.45; xent: 2.52; lr: 0.00000903;   0/942 tok/s;  81579 sec
[2020-03-30 18:19:27,531 INFO] Loading train dataset from ../bert_data/cnndm.train.9050.bert.pt, number of examples: 1978
[2020-03-30 18:20:26,747 INFO] Step 49150/210000; acc:  49.90; ppl:  8.82; xent: 2.18; lr: 0.00000902;   0/609 tok/s;  81662 sec
[2020-03-30 18:21:48,454 INFO] Step 49200/210000; acc:  45.08; ppl: 12.61; xent: 2.53; lr: 0.00000902;   0/1210 tok/s;  81743 sec
[2020-03-30 18:22:03,578 INFO] Loading train dataset from ../bert_data/cnndm.train.9051.bert.pt, number of examples: 1981
[2020-03-30 18:23:10,647 INFO] Step 49250/210000; acc:  47.99; ppl:  9.73; xent: 2.27; lr: 0.00000901;   0/757 tok/s;  81825 sec
[2020-03-30 18:24:32,946 INFO] Step 49300/210000; acc:  43.98; ppl: 13.13; xent: 2.58; lr: 0.00000901;   0/1033 tok/s;  81908 sec
[2020-03-30 18:24:37,970 INFO] Loading train dataset from ../bert_data/cnndm.train.9052.bert.pt, number of examples: 1981
[2020-03-30 18:25:55,306 INFO] Step 49350/210000; acc:  40.45; ppl: 17.99; xent: 2.89; lr: 0.00000900;   0/1551 tok/s;  81990 sec
[2020-03-30 18:27:12,420 INFO] Loading train dataset from ../bert_data/cnndm.train.9053.bert.pt, number of examples: 1981
[2020-03-30 18:27:17,189 INFO] Step 49400/210000; acc:  45.35; ppl: 12.37; xent: 2.52; lr: 0.00000900;   0/793 tok/s;  82072 sec
[2020-03-30 18:28:38,718 INFO] Step 49450/210000; acc:  41.42; ppl: 15.19; xent: 2.72; lr: 0.00000899;   0/1425 tok/s;  82154 sec
[2020-03-30 18:29:48,044 INFO] Loading train dataset from ../bert_data/cnndm.train.9054.bert.pt, number of examples: 1987
[2020-03-30 18:30:00,977 INFO] Step 49500/210000; acc:  46.48; ppl:  9.63; xent: 2.27; lr: 0.00000899;   0/688 tok/s;  82236 sec
[2020-03-30 18:30:00,980 INFO] Saving checkpoint ../models/model_step_49500.pt
[2020-03-30 18:31:25,594 INFO] Step 49550/210000; acc:  40.18; ppl: 16.39; xent: 2.80; lr: 0.00000898;   0/1220 tok/s;  82320 sec
[2020-03-30 18:32:26,624 INFO] Loading train dataset from ../bert_data/cnndm.train.9055.bert.pt, number of examples: 1984
[2020-03-30 18:32:47,845 INFO] Step 49600/210000; acc:  54.46; ppl:  6.70; xent: 1.90; lr: 0.00000898;   0/681 tok/s;  82403 sec
[2020-03-30 18:34:09,897 INFO] Step 49650/210000; acc:  51.14; ppl:  9.44; xent: 2.24; lr: 0.00000898;   0/648 tok/s;  82485 sec
[2020-03-30 18:35:00,883 INFO] Loading train dataset from ../bert_data/cnndm.train.9056.bert.pt, number of examples: 1985
[2020-03-30 18:35:31,750 INFO] Step 49700/210000; acc:  48.10; ppl: 10.76; xent: 2.38; lr: 0.00000897;   0/1151 tok/s;  82567 sec
[2020-03-30 18:36:54,315 INFO] Step 49750/210000; acc:  55.37; ppl:  7.33; xent: 1.99; lr: 0.00000897;   0/462 tok/s;  82649 sec
[2020-03-30 18:37:37,524 INFO] Loading train dataset from ../bert_data/cnndm.train.9057.bert.pt, number of examples: 1986
[2020-03-30 18:38:17,140 INFO] Step 49800/210000; acc:  44.67; ppl: 11.83; xent: 2.47; lr: 0.00000896;   0/1378 tok/s;  82732 sec
[2020-03-30 18:39:39,939 INFO] Step 49850/210000; acc:  50.06; ppl:  8.90; xent: 2.19; lr: 0.00000896;   0/578 tok/s;  82815 sec
[2020-03-30 18:40:14,874 INFO] Loading train dataset from ../bert_data/cnndm.train.9058.bert.pt, number of examples: 1979
[2020-03-30 18:41:03,164 INFO] Step 49900/210000; acc:  47.08; ppl: 11.86; xent: 2.47; lr: 0.00000895;   0/1145 tok/s;  82898 sec
[2020-03-30 18:42:24,900 INFO] Step 49950/210000; acc:  50.10; ppl:  8.78; xent: 2.17; lr: 0.00000895;   0/731 tok/s;  82980 sec
[2020-03-30 18:42:49,787 INFO] Loading train dataset from ../bert_data/cnndm.train.9059.bert.pt, number of examples: 1987
[2020-03-30 18:43:47,318 INFO] Step 50000/210000; acc:  59.09; ppl:  5.73; xent: 1.75; lr: 0.00000894;   0/478 tok/s;  83062 sec
[2020-03-30 18:43:47,322 INFO] Saving checkpoint ../models/model_step_50000.pt
[2020-03-30 18:45:11,159 INFO] Step 50050/210000; acc:  46.02; ppl: 10.26; xent: 2.33; lr: 0.00000894;   0/919 tok/s;  83146 sec
[2020-03-30 18:45:27,974 INFO] Loading train dataset from ../bert_data/cnndm.train.9060.bert.pt, number of examples: 1992
[2020-03-30 18:46:33,839 INFO] Step 50100/210000; acc:  54.74; ppl:  6.66; xent: 1.90; lr: 0.00000894;   0/483 tok/s;  83229 sec
[2020-03-30 18:47:56,821 INFO] Step 50150/210000; acc:  47.80; ppl: 10.96; xent: 2.39; lr: 0.00000893;   0/1046 tok/s;  83312 sec
[2020-03-30 18:48:05,422 INFO] Loading train dataset from ../bert_data/cnndm.train.9061.bert.pt, number of examples: 1985
[2020-03-30 18:49:19,921 INFO] Step 50200/210000; acc:  58.06; ppl:  6.39; xent: 1.86; lr: 0.00000893;   0/405 tok/s;  83395 sec
[2020-03-30 18:50:42,448 INFO] Step 50250/210000; acc:  48.18; ppl: 11.16; xent: 2.41; lr: 0.00000892;   0/909 tok/s;  83477 sec
[2020-03-30 18:50:45,100 INFO] Loading train dataset from ../bert_data/cnndm.train.9062.bert.pt, number of examples: 1982
[2020-03-30 18:52:05,750 INFO] Step 50300/210000; acc:  58.28; ppl:  6.11; xent: 1.81; lr: 0.00000892;   0/376 tok/s;  83561 sec
[2020-03-30 18:53:21,561 INFO] Loading train dataset from ../bert_data/cnndm.train.9063.bert.pt, number of examples: 1988
[2020-03-30 18:53:28,058 INFO] Step 50350/210000; acc:  45.84; ppl: 12.18; xent: 2.50; lr: 0.00000891;   0/1144 tok/s;  83643 sec
[2020-03-30 18:54:50,023 INFO] Step 50400/210000; acc:  53.80; ppl:  9.15; xent: 2.21; lr: 0.00000891;   0/465 tok/s;  83725 sec
[2020-03-30 18:55:56,668 INFO] Loading train dataset from ../bert_data/cnndm.train.9064.bert.pt, number of examples: 1984
[2020-03-30 18:56:13,166 INFO] Step 50450/210000; acc:  40.87; ppl: 15.39; xent: 2.73; lr: 0.00000890;   0/1622 tok/s;  83808 sec
[2020-03-30 18:57:34,738 INFO] Step 50500/210000; acc:  54.49; ppl:  6.85; xent: 1.92; lr: 0.00000890;   0/530 tok/s;  83890 sec
[2020-03-30 18:57:34,742 INFO] Saving checkpoint ../models/model_step_50500.pt
[2020-03-30 18:58:34,336 INFO] Loading train dataset from ../bert_data/cnndm.train.9065.bert.pt, number of examples: 1986
[2020-03-30 18:58:59,356 INFO] Step 50550/210000; acc:  43.75; ppl: 13.69; xent: 2.62; lr: 0.00000890;   0/1279 tok/s;  83974 sec
[2020-03-30 19:00:21,120 INFO] Step 50600/210000; acc:  51.24; ppl:  8.97; xent: 2.19; lr: 0.00000889;   0/621 tok/s;  84056 sec
[2020-03-30 19:01:10,052 INFO] Loading train dataset from ../bert_data/cnndm.train.9066.bert.pt, number of examples: 1988
[2020-03-30 19:01:43,195 INFO] Step 50650/210000; acc:  41.25; ppl: 15.76; xent: 2.76; lr: 0.00000889;   0/1303 tok/s;  84138 sec
[2020-03-30 19:03:05,560 INFO] Step 50700/210000; acc:  49.02; ppl: 10.04; xent: 2.31; lr: 0.00000888;   0/589 tok/s;  84220 sec
[2020-03-30 19:03:47,300 INFO] Loading train dataset from ../bert_data/cnndm.train.9067.bert.pt, number of examples: 1986
[2020-03-30 19:04:28,050 INFO] Step 50750/210000; acc:  42.71; ppl: 14.27; xent: 2.66; lr: 0.00000888;   0/1029 tok/s;  84303 sec
[2020-03-30 19:05:49,452 INFO] Step 50800/210000; acc:  51.83; ppl:  8.09; xent: 2.09; lr: 0.00000887;   0/614 tok/s;  84384 sec
[2020-03-30 19:06:22,884 INFO] Loading train dataset from ../bert_data/cnndm.train.9068.bert.pt, number of examples: 1988
[2020-03-30 19:07:12,409 INFO] Step 50850/210000; acc:  47.07; ppl: 10.77; xent: 2.38; lr: 0.00000887;   0/511 tok/s;  84467 sec
[2020-03-30 19:08:34,432 INFO] Step 50900/210000; acc:  49.89; ppl:  9.88; xent: 2.29; lr: 0.00000886;   0/824 tok/s;  84549 sec
[2020-03-30 19:08:59,313 INFO] Loading train dataset from ../bert_data/cnndm.train.9069.bert.pt, number of examples: 1982
[2020-03-30 19:09:57,212 INFO] Step 50950/210000; acc:  53.73; ppl:  7.65; xent: 2.03; lr: 0.00000886;   0/438 tok/s;  84632 sec
[2020-03-30 19:11:19,580 INFO] Step 51000/210000; acc:  44.34; ppl: 12.10; xent: 2.49; lr: 0.00000886;   0/1147 tok/s;  84714 sec
[2020-03-30 19:11:19,584 INFO] Saving checkpoint ../models/model_step_51000.pt
[2020-03-30 19:11:36,851 INFO] Loading train dataset from ../bert_data/cnndm.train.9070.bert.pt, number of examples: 1988
[2020-03-30 19:12:44,235 INFO] Step 51050/210000; acc:  50.83; ppl:  8.21; xent: 2.10; lr: 0.00000885;   0/711 tok/s;  84799 sec
[2020-03-30 19:14:06,489 INFO] Step 51100/210000; acc:  43.90; ppl: 12.75; xent: 2.55; lr: 0.00000885;   0/1453 tok/s;  84881 sec
[2020-03-30 19:14:13,246 INFO] Loading train dataset from ../bert_data/cnndm.train.9071.bert.pt, number of examples: 1988
[2020-03-30 19:15:28,427 INFO] Step 51150/210000; acc:  54.39; ppl:  6.61; xent: 1.89; lr: 0.00000884;   0/553 tok/s;  84963 sec
[2020-03-30 19:16:48,993 INFO] Loading train dataset from ../bert_data/cnndm.train.9072.bert.pt, number of examples: 1992
[2020-03-30 19:16:50,726 INFO] Step 51200/210000; acc:  44.00; ppl: 15.07; xent: 2.71; lr: 0.00000884;   0/921 tok/s;  85046 sec
[2020-03-30 19:18:12,766 INFO] Step 51250/210000; acc:  49.57; ppl: 10.11; xent: 2.31; lr: 0.00000883;   0/686 tok/s;  85128 sec
[2020-03-30 19:19:25,969 INFO] Loading train dataset from ../bert_data/cnndm.train.9073.bert.pt, number of examples: 1987
[2020-03-30 19:19:35,909 INFO] Step 51300/210000; acc:  47.75; ppl: 11.33; xent: 2.43; lr: 0.00000883;   0/665 tok/s;  85211 sec
[2020-03-30 19:20:58,490 INFO] Step 51350/210000; acc:  47.06; ppl: 10.51; xent: 2.35; lr: 0.00000883;   0/753 tok/s;  85293 sec
[2020-03-30 19:22:03,008 INFO] Loading train dataset from ../bert_data/cnndm.train.9074.bert.pt, number of examples: 1991
[2020-03-30 19:22:21,078 INFO] Step 51400/210000; acc:  49.00; ppl:  9.68; xent: 2.27; lr: 0.00000882;   0/561 tok/s;  85376 sec
[2020-03-30 19:23:43,569 INFO] Step 51450/210000; acc:  50.00; ppl:  8.90; xent: 2.19; lr: 0.00000882;   0/940 tok/s;  85458 sec
[2020-03-30 19:24:39,735 INFO] Loading train dataset from ../bert_data/cnndm.train.9075.bert.pt, number of examples: 1983
[2020-03-30 19:25:06,082 INFO] Step 51500/210000; acc:  53.81; ppl:  7.50; xent: 2.01; lr: 0.00000881;   0/458 tok/s;  85541 sec
[2020-03-30 19:25:06,085 INFO] Saving checkpoint ../models/model_step_51500.pt
[2020-03-30 19:26:30,611 INFO] Step 51550/210000; acc:  44.99; ppl: 12.16; xent: 2.50; lr: 0.00000881;   0/1267 tok/s;  85625 sec
[2020-03-30 19:27:17,876 INFO] Loading train dataset from ../bert_data/cnndm.train.9076.bert.pt, number of examples: 1986
[2020-03-30 19:27:52,512 INFO] Step 51600/210000; acc:  51.91; ppl:  8.61; xent: 2.15; lr: 0.00000880;   0/512 tok/s;  85707 sec
[2020-03-30 19:29:13,897 INFO] Step 51650/210000; acc:  46.38; ppl: 12.45; xent: 2.52; lr: 0.00000880;   0/1229 tok/s;  85789 sec
[2020-03-30 19:29:53,777 INFO] Loading train dataset from ../bert_data/cnndm.train.9077.bert.pt, number of examples: 1990
[2020-03-30 19:30:36,193 INFO] Step 51700/210000; acc:  49.26; ppl:  9.26; xent: 2.23; lr: 0.00000880;   0/535 tok/s;  85871 sec
[2020-03-30 19:31:58,772 INFO] Step 51750/210000; acc:  45.04; ppl: 13.60; xent: 2.61; lr: 0.00000879;   0/1277 tok/s;  85954 sec
[2020-03-30 19:32:30,513 INFO] Loading train dataset from ../bert_data/cnndm.train.9078.bert.pt, number of examples: 1992
[2020-03-30 19:33:20,720 INFO] Step 51800/210000; acc:  52.02; ppl:  7.95; xent: 2.07; lr: 0.00000879;   0/626 tok/s;  86036 sec
[2020-03-30 19:34:42,816 INFO] Step 51850/210000; acc:  46.26; ppl: 11.34; xent: 2.43; lr: 0.00000878;   0/1115 tok/s;  86118 sec
[2020-03-30 19:35:08,484 INFO] Loading train dataset from ../bert_data/cnndm.train.9079.bert.pt, number of examples: 1984
[2020-03-30 19:36:05,783 INFO] Step 51900/210000; acc:  51.41; ppl:  7.29; xent: 1.99; lr: 0.00000878;   0/601 tok/s;  86201 sec
[2020-03-30 19:37:27,281 INFO] Step 51950/210000; acc:  46.65; ppl: 11.71; xent: 2.46; lr: 0.00000877;   0/1250 tok/s;  86282 sec
[2020-03-30 19:37:42,550 INFO] Loading train dataset from ../bert_data/cnndm.train.9080.bert.pt, number of examples: 1988
[2020-03-30 19:38:49,749 INFO] Step 52000/210000; acc:  57.96; ppl:  5.93; xent: 1.78; lr: 0.00000877;   0/592 tok/s;  86365 sec
[2020-03-30 19:38:49,752 INFO] Saving checkpoint ../models/model_step_52000.pt
[2020-03-30 19:40:13,877 INFO] Step 52050/210000; acc:  44.53; ppl: 13.57; xent: 2.61; lr: 0.00000877;   0/1212 tok/s;  86449 sec
[2020-03-30 19:40:21,337 INFO] Loading train dataset from ../bert_data/cnndm.train.9081.bert.pt, number of examples: 1991
[2020-03-30 19:41:36,459 INFO] Step 52100/210000; acc:  48.46; ppl: 10.51; xent: 2.35; lr: 0.00000876;   0/744 tok/s;  86531 sec
[2020-03-30 19:42:57,955 INFO] Loading train dataset from ../bert_data/cnndm.train.9082.bert.pt, number of examples: 1985
[2020-03-30 19:42:59,722 INFO] Step 52150/210000; acc:  54.36; ppl:  7.55; xent: 2.02; lr: 0.00000876;   0/465 tok/s;  86615 sec
[2020-03-30 19:44:20,809 INFO] Step 52200/210000; acc:  47.88; ppl:  9.35; xent: 2.23; lr: 0.00000875;   0/857 tok/s;  86696 sec
[2020-03-30 19:45:31,734 INFO] Loading train dataset from ../bert_data/cnndm.train.9083.bert.pt, number of examples: 1986
[2020-03-30 19:45:43,194 INFO] Step 52250/210000; acc:  52.11; ppl:  8.01; xent: 2.08; lr: 0.00000875;   0/436 tok/s;  86778 sec
[2020-03-30 19:47:05,186 INFO] Step 52300/210000; acc:  50.08; ppl:  9.75; xent: 2.28; lr: 0.00000875;   0/771 tok/s;  86860 sec
[2020-03-30 19:48:10,442 INFO] Loading train dataset from ../bert_data/cnndm.train.9084.bert.pt, number of examples: 1982
[2020-03-30 19:48:28,458 INFO] Step 52350/210000; acc:  57.18; ppl:  6.22; xent: 1.83; lr: 0.00000874;   0/462 tok/s;  86943 sec
[2020-03-30 19:49:50,816 INFO] Step 52400/210000; acc:  46.34; ppl: 11.95; xent: 2.48; lr: 0.00000874;   0/1073 tok/s;  87026 sec
[2020-03-30 19:50:47,592 INFO] Loading train dataset from ../bert_data/cnndm.train.9085.bert.pt, number of examples: 1985
[2020-03-30 19:51:13,363 INFO] Step 52450/210000; acc:  54.73; ppl:  6.97; xent: 1.94; lr: 0.00000873;   0/468 tok/s;  87108 sec
[2020-03-30 19:52:35,612 INFO] Step 52500/210000; acc:  46.66; ppl: 10.60; xent: 2.36; lr: 0.00000873;   0/1133 tok/s;  87190 sec
[2020-03-30 19:52:35,616 INFO] Saving checkpoint ../models/model_step_52500.pt
[2020-03-30 19:53:27,756 INFO] Loading train dataset from ../bert_data/cnndm.train.9086.bert.pt, number of examples: 1983
[2020-03-30 19:54:01,844 INFO] Step 52550/210000; acc:  48.33; ppl:  9.02; xent: 2.20; lr: 0.00000872;   0/650 tok/s;  87277 sec
[2020-03-30 19:55:24,620 INFO] Step 52600/210000; acc:  42.44; ppl: 15.72; xent: 2.76; lr: 0.00000872;   0/1448 tok/s;  87359 sec
[2020-03-30 19:56:02,672 INFO] Loading train dataset from ../bert_data/cnndm.train.9087.bert.pt, number of examples: 1986
[2020-03-30 19:56:46,945 INFO] Step 52650/210000; acc:  52.15; ppl:  7.86; xent: 2.06; lr: 0.00000872;   0/699 tok/s;  87442 sec
[2020-03-30 19:58:08,614 INFO] Step 52700/210000; acc:  43.36; ppl: 14.99; xent: 2.71; lr: 0.00000871;   0/1468 tok/s;  87523 sec
[2020-03-30 19:58:38,478 INFO] Loading train dataset from ../bert_data/cnndm.train.9088.bert.pt, number of examples: 1986
[2020-03-30 19:59:31,066 INFO] Step 52750/210000; acc:  53.35; ppl:  7.96; xent: 2.07; lr: 0.00000871;   0/797 tok/s;  87606 sec
[2020-03-30 20:00:53,516 INFO] Step 52800/210000; acc:  48.39; ppl: 10.94; xent: 2.39; lr: 0.00000870;   0/1122 tok/s;  87688 sec
[2020-03-30 20:01:14,755 INFO] Loading train dataset from ../bert_data/cnndm.train.9089.bert.pt, number of examples: 1986
[2020-03-30 20:02:15,288 INFO] Step 52850/210000; acc:  49.41; ppl:  8.86; xent: 2.18; lr: 0.00000870;   0/632 tok/s;  87770 sec
[2020-03-30 20:03:37,262 INFO] Step 52900/210000; acc:  39.40; ppl: 17.50; xent: 2.86; lr: 0.00000870;   0/1008 tok/s;  87852 sec
[2020-03-30 20:03:50,389 INFO] Loading train dataset from ../bert_data/cnndm.train.9090.bert.pt, number of examples: 20
[2020-03-30 20:03:52,212 INFO] Loading train dataset from ../bert_data/cnndm.train.9100.bert.pt, number of examples: 1988
[2020-03-30 20:04:50,939 INFO] Step 52950/210000; acc:  45.14; ppl: 10.83; xent: 2.38; lr: 0.00000869;   0/592 tok/s;  87926 sec
[2020-03-30 20:06:03,244 INFO] Step 53000/210000; acc:  54.86; ppl:  6.51; xent: 1.87; lr: 0.00000869;   0/421 tok/s;  87998 sec
[2020-03-30 20:06:03,247 INFO] Saving checkpoint ../models/model_step_53000.pt
[2020-03-30 20:06:11,448 INFO] Loading train dataset from ../bert_data/cnndm.train.9101.bert.pt, number of examples: 1983
[2020-03-30 20:07:17,672 INFO] Step 53050/210000; acc:  58.13; ppl:  6.41; xent: 1.86; lr: 0.00000868;   0/400 tok/s;  88072 sec
[2020-03-30 20:08:27,211 INFO] Loading train dataset from ../bert_data/cnndm.train.9102.bert.pt, number of examples: 1992
[2020-03-30 20:08:30,250 INFO] Step 53100/210000; acc:  58.02; ppl:  6.80; xent: 1.92; lr: 0.00000868;   0/408 tok/s;  88145 sec
[2020-03-30 20:09:42,724 INFO] Step 53150/210000; acc:  55.82; ppl:  6.52; xent: 1.87; lr: 0.00000868;   0/511 tok/s;  88218 sec
[2020-03-30 20:10:45,354 INFO] Loading train dataset from ../bert_data/cnndm.train.9103.bert.pt, number of examples: 1978
[2020-03-30 20:10:55,140 INFO] Step 53200/210000; acc:  54.22; ppl:  7.01; xent: 1.95; lr: 0.00000867;   0/594 tok/s;  88290 sec
[2020-03-30 20:12:08,150 INFO] Step 53250/210000; acc:  65.05; ppl:  4.56; xent: 1.52; lr: 0.00000867;   0/434 tok/s;  88363 sec
[2020-03-30 20:13:02,384 INFO] Loading train dataset from ../bert_data/cnndm.train.9104.bert.pt, number of examples: 1986
[2020-03-30 20:13:21,534 INFO] Step 53300/210000; acc:  61.47; ppl:  5.00; xent: 1.61; lr: 0.00000866;   0/492 tok/s;  88436 sec
[2020-03-30 20:14:33,895 INFO] Step 53350/210000; acc:  54.83; ppl:  7.21; xent: 1.97; lr: 0.00000866;   0/679 tok/s;  88509 sec
[2020-03-30 20:15:20,833 INFO] Loading train dataset from ../bert_data/cnndm.train.9105.bert.pt, number of examples: 1988
[2020-03-30 20:15:47,190 INFO] Step 53400/210000; acc:  50.66; ppl:  9.01; xent: 2.20; lr: 0.00000865;   0/672 tok/s;  88582 sec
[2020-03-30 20:16:59,545 INFO] Step 53450/210000; acc:  53.68; ppl:  7.19; xent: 1.97; lr: 0.00000865;   0/606 tok/s;  88654 sec
[2020-03-30 20:17:38,802 INFO] Loading train dataset from ../bert_data/cnndm.train.9106.bert.pt, number of examples: 1978
[2020-03-30 20:18:12,042 INFO] Step 53500/210000; acc:  53.17; ppl:  7.72; xent: 2.04; lr: 0.00000865;   0/534 tok/s;  88727 sec
[2020-03-30 20:18:12,053 INFO] Saving checkpoint ../models/model_step_53500.pt
[2020-03-30 20:19:27,413 INFO] Step 53550/210000; acc:  59.02; ppl:  5.69; xent: 1.74; lr: 0.00000864;   0/425 tok/s;  88802 sec
[2020-03-30 20:19:58,422 INFO] Loading train dataset from ../bert_data/cnndm.train.9107.bert.pt, number of examples: 1985
[2020-03-30 20:20:41,067 INFO] Step 53600/210000; acc:  57.10; ppl:  6.13; xent: 1.81; lr: 0.00000864;   0/475 tok/s;  88876 sec
[2020-03-30 20:21:52,657 INFO] Step 53650/210000; acc:  60.23; ppl:  5.87; xent: 1.77; lr: 0.00000863;   0/580 tok/s;  88947 sec
[2020-03-30 20:22:16,209 INFO] Loading train dataset from ../bert_data/cnndm.train.9108.bert.pt, number of examples: 1989
[2020-03-30 20:23:05,341 INFO] Step 53700/210000; acc:  57.65; ppl:  6.28; xent: 1.84; lr: 0.00000863;   0/560 tok/s;  89020 sec
[2020-03-30 20:24:17,654 INFO] Step 53750/210000; acc:  59.53; ppl:  6.09; xent: 1.81; lr: 0.00000863;   0/435 tok/s;  89092 sec
[2020-03-30 20:24:33,925 INFO] Loading train dataset from ../bert_data/cnndm.train.9109.bert.pt, number of examples: 1987
[2020-03-30 20:25:30,777 INFO] Step 53800/210000; acc:  62.23; ppl:  5.16; xent: 1.64; lr: 0.00000862;   0/470 tok/s;  89166 sec
[2020-03-30 20:26:43,778 INFO] Step 53850/210000; acc:  53.91; ppl:  7.79; xent: 2.05; lr: 0.00000862;   0/475 tok/s;  89239 sec
[2020-03-30 20:26:51,105 INFO] Loading train dataset from ../bert_data/cnndm.train.9110.bert.pt, number of examples: 1985
[2020-03-30 20:27:56,404 INFO] Step 53900/210000; acc:  64.41; ppl:  4.07; xent: 1.40; lr: 0.00000861;   0/386 tok/s;  89311 sec
[2020-03-30 20:29:08,823 INFO] Step 53950/210000; acc:  52.90; ppl:  8.75; xent: 2.17; lr: 0.00000861;   0/746 tok/s;  89384 sec
[2020-03-30 20:29:09,097 INFO] Loading train dataset from ../bert_data/cnndm.train.9111.bert.pt, number of examples: 1985
[2020-03-30 20:30:21,528 INFO] Step 54000/210000; acc:  60.82; ppl:  5.98; xent: 1.79; lr: 0.00000861;   0/606 tok/s;  89456 sec
[2020-03-30 20:30:21,531 INFO] Saving checkpoint ../models/model_step_54000.pt
[2020-03-30 20:31:29,131 INFO] Loading train dataset from ../bert_data/cnndm.train.9112.bert.pt, number of examples: 1983
[2020-03-30 20:31:36,682 INFO] Step 54050/210000; acc:  60.03; ppl:  5.74; xent: 1.75; lr: 0.00000860;   0/503 tok/s;  89532 sec
[2020-03-30 20:32:49,182 INFO] Step 54100/210000; acc:  66.30; ppl:  4.05; xent: 1.40; lr: 0.00000860;   0/418 tok/s;  89604 sec
[2020-03-30 20:33:45,962 INFO] Loading train dataset from ../bert_data/cnndm.train.9113.bert.pt, number of examples: 1979
[2020-03-30 20:34:02,294 INFO] Step 54150/210000; acc:  62.14; ppl:  4.88; xent: 1.59; lr: 0.00000859;   0/389 tok/s;  89677 sec
[2020-03-30 20:35:14,137 INFO] Step 54200/210000; acc:  57.11; ppl:  5.77; xent: 1.75; lr: 0.00000859;   0/567 tok/s;  89749 sec
[2020-03-30 20:36:04,119 INFO] Loading train dataset from ../bert_data/cnndm.train.9114.bert.pt, number of examples: 1989
[2020-03-30 20:36:27,650 INFO] Step 54250/210000; acc:  55.85; ppl:  6.68; xent: 1.90; lr: 0.00000859;   0/621 tok/s;  89822 sec
[2020-03-30 20:37:39,734 INFO] Step 54300/210000; acc:  63.65; ppl:  5.03; xent: 1.62; lr: 0.00000858;   0/516 tok/s;  89895 sec
[2020-03-30 20:38:21,995 INFO] Loading train dataset from ../bert_data/cnndm.train.9115.bert.pt, number of examples: 1989
[2020-03-30 20:38:52,455 INFO] Step 54350/210000; acc:  58.01; ppl:  5.75; xent: 1.75; lr: 0.00000858;   0/414 tok/s;  89967 sec
[2020-03-30 20:40:04,308 INFO] Step 54400/210000; acc:  60.81; ppl:  5.62; xent: 1.73; lr: 0.00000857;   0/703 tok/s;  90039 sec
[2020-03-30 20:40:39,380 INFO] Loading train dataset from ../bert_data/cnndm.train.9116.bert.pt, number of examples: 1985
[2020-03-30 20:41:17,912 INFO] Step 54450/210000; acc:  61.66; ppl:  5.30; xent: 1.67; lr: 0.00000857;   0/740 tok/s;  90113 sec
[2020-03-30 20:42:30,857 INFO] Step 54500/210000; acc:  55.58; ppl:  6.33; xent: 1.85; lr: 0.00000857;   0/659 tok/s;  90186 sec
[2020-03-30 20:42:30,860 INFO] Saving checkpoint ../models/model_step_54500.pt
[2020-03-30 20:42:59,629 INFO] Loading train dataset from ../bert_data/cnndm.train.9117.bert.pt, number of examples: 1979
[2020-03-30 20:43:45,917 INFO] Step 54550/210000; acc:  57.43; ppl:  5.63; xent: 1.73; lr: 0.00000856;   0/611 tok/s;  90261 sec
[2020-03-30 20:44:58,121 INFO] Step 54600/210000; acc:  64.01; ppl:  3.93; xent: 1.37; lr: 0.00000856;   0/458 tok/s;  90333 sec
[2020-03-30 20:45:17,255 INFO] Loading train dataset from ../bert_data/cnndm.train.9118.bert.pt, number of examples: 1980
[2020-03-30 20:46:11,986 INFO] Step 54650/210000; acc:  59.79; ppl:  4.99; xent: 1.61; lr: 0.00000856;   0/557 tok/s;  90407 sec
[2020-03-30 20:47:24,929 INFO] Step 54700/210000; acc:  58.35; ppl:  5.98; xent: 1.79; lr: 0.00000855;   0/702 tok/s;  90480 sec
[2020-03-30 20:47:37,034 INFO] Loading train dataset from ../bert_data/cnndm.train.9119.bert.pt, number of examples: 1985
[2020-03-30 20:48:37,647 INFO] Step 54750/210000; acc:  59.93; ppl:  5.57; xent: 1.72; lr: 0.00000855;   0/558 tok/s;  90552 sec
[2020-03-30 20:49:49,734 INFO] Step 54800/210000; acc:  67.29; ppl:  3.86; xent: 1.35; lr: 0.00000854;   0/541 tok/s;  90625 sec
[2020-03-30 20:49:52,790 INFO] Loading train dataset from ../bert_data/cnndm.train.9120.bert.pt, number of examples: 1981
[2020-03-30 20:51:02,730 INFO] Step 54850/210000; acc:  66.55; ppl:  4.22; xent: 1.44; lr: 0.00000854;   0/403 tok/s;  90698 sec
[2020-03-30 20:52:10,617 INFO] Loading train dataset from ../bert_data/cnndm.train.9121.bert.pt, number of examples: 1985
[2020-03-30 20:52:15,089 INFO] Step 54900/210000; acc:  65.66; ppl:  4.23; xent: 1.44; lr: 0.00000854;   0/454 tok/s;  90770 sec
[2020-03-30 20:53:27,922 INFO] Step 54950/210000; acc:  58.39; ppl:  6.17; xent: 1.82; lr: 0.00000853;   0/633 tok/s;  90843 sec
[2020-03-30 20:54:27,724 INFO] Loading train dataset from ../bert_data/cnndm.train.9122.bert.pt, number of examples: 1986
[2020-03-30 20:54:40,917 INFO] Step 55000/210000; acc:  61.99; ppl:  4.85; xent: 1.58; lr: 0.00000853;   0/697 tok/s;  90916 sec
[2020-03-30 20:54:40,920 INFO] Saving checkpoint ../models/model_step_55000.pt
[2020-03-30 20:55:56,497 INFO] Step 55050/210000; acc:  59.46; ppl:  5.13; xent: 1.64; lr: 0.00000852;   0/484 tok/s;  90991 sec
[2020-03-30 20:56:48,808 INFO] Loading train dataset from ../bert_data/cnndm.train.9123.bert.pt, number of examples: 1983
[2020-03-30 20:57:09,543 INFO] Step 55100/210000; acc:  61.50; ppl:  5.36; xent: 1.68; lr: 0.00000852;   0/529 tok/s;  91064 sec
[2020-03-30 20:58:22,231 INFO] Step 55150/210000; acc:  65.54; ppl:  3.64; xent: 1.29; lr: 0.00000852;   0/406 tok/s;  91137 sec
[2020-03-30 20:59:07,458 INFO] Loading train dataset from ../bert_data/cnndm.train.9124.bert.pt, number of examples: 1983
[2020-03-30 20:59:35,234 INFO] Step 55200/210000; acc:  66.62; ppl:  4.21; xent: 1.44; lr: 0.00000851;   0/483 tok/s;  91210 sec
[2020-03-30 21:00:47,620 INFO] Step 55250/210000; acc:  55.45; ppl:  7.16; xent: 1.97; lr: 0.00000851;   0/819 tok/s;  91282 sec
[2020-03-30 21:01:25,449 INFO] Loading train dataset from ../bert_data/cnndm.train.9125.bert.pt, number of examples: 1991
[2020-03-30 21:02:00,260 INFO] Step 55300/210000; acc:  57.23; ppl:  6.16; xent: 1.82; lr: 0.00000850;   0/594 tok/s;  91355 sec
[2020-03-30 21:03:12,803 INFO] Step 55350/210000; acc:  63.85; ppl:  4.33; xent: 1.47; lr: 0.00000850;   0/559 tok/s;  91428 sec
[2020-03-30 21:03:41,707 INFO] Loading train dataset from ../bert_data/cnndm.train.9126.bert.pt, number of examples: 1985
[2020-03-30 21:04:24,792 INFO] Step 55400/210000; acc:  65.07; ppl:  4.40; xent: 1.48; lr: 0.00000850;   0/430 tok/s;  91500 sec
[2020-03-30 21:05:37,440 INFO] Step 55450/210000; acc:  57.84; ppl:  5.71; xent: 1.74; lr: 0.00000849;   0/620 tok/s;  91572 sec
[2020-03-30 21:05:58,056 INFO] Loading train dataset from ../bert_data/cnndm.train.9127.bert.pt, number of examples: 1980
[2020-03-30 21:06:50,291 INFO] Step 55500/210000; acc:  64.45; ppl:  4.24; xent: 1.45; lr: 0.00000849;   0/362 tok/s;  91645 sec
[2020-03-30 21:06:50,295 INFO] Saving checkpoint ../models/model_step_55500.pt
[2020-03-30 21:08:05,300 INFO] Step 55550/210000; acc:  56.91; ppl:  6.34; xent: 1.85; lr: 0.00000849;   0/770 tok/s;  91720 sec
[2020-03-30 21:08:19,209 INFO] Loading train dataset from ../bert_data/cnndm.train.9128.bert.pt, number of examples: 1981
[2020-03-30 21:09:18,201 INFO] Step 55600/210000; acc:  54.80; ppl:  7.08; xent: 1.96; lr: 0.00000848;   0/692 tok/s;  91793 sec
[2020-03-30 21:10:30,575 INFO] Step 55650/210000; acc:  67.60; ppl:  4.15; xent: 1.42; lr: 0.00000848;   0/496 tok/s;  91865 sec
[2020-03-30 21:10:36,748 INFO] Loading train dataset from ../bert_data/cnndm.train.9129.bert.pt, number of examples: 1987
[2020-03-30 21:11:43,489 INFO] Step 55700/210000; acc:  60.77; ppl:  5.41; xent: 1.69; lr: 0.00000847;   0/633 tok/s;  91938 sec
[2020-03-30 21:12:53,747 INFO] Loading train dataset from ../bert_data/cnndm.train.9130.bert.pt, number of examples: 1986
[2020-03-30 21:12:56,938 INFO] Step 55750/210000; acc:  65.80; ppl:  3.89; xent: 1.36; lr: 0.00000847;   0/407 tok/s;  92012 sec
[2020-03-30 21:14:09,725 INFO] Step 55800/210000; acc:  62.35; ppl:  5.44; xent: 1.69; lr: 0.00000847;   0/690 tok/s;  92085 sec
[2020-03-30 21:15:11,968 INFO] Loading train dataset from ../bert_data/cnndm.train.9131.bert.pt, number of examples: 1984
[2020-03-30 21:15:21,881 INFO] Step 55850/210000; acc:  63.17; ppl:  5.13; xent: 1.64; lr: 0.00000846;   0/558 tok/s;  92157 sec
[2020-03-30 21:16:35,558 INFO] Step 55900/210000; acc:  59.17; ppl:  5.81; xent: 1.76; lr: 0.00000846;   0/490 tok/s;  92230 sec
[2020-03-30 21:17:30,499 INFO] Loading train dataset from ../bert_data/cnndm.train.9132.bert.pt, number of examples: 1990
[2020-03-30 21:17:47,578 INFO] Step 55950/210000; acc:  60.73; ppl:  4.90; xent: 1.59; lr: 0.00000846;   0/418 tok/s;  92302 sec
[2020-03-30 21:19:00,051 INFO] Step 56000/210000; acc:  56.35; ppl:  6.17; xent: 1.82; lr: 0.00000845;   0/659 tok/s;  92375 sec
[2020-03-30 21:19:00,054 INFO] Saving checkpoint ../models/model_step_56000.pt
[2020-03-30 21:19:50,825 INFO] Loading train dataset from ../bert_data/cnndm.train.9133.bert.pt, number of examples: 1978
[2020-03-30 21:20:15,068 INFO] Step 56050/210000; acc:  62.19; ppl:  5.30; xent: 1.67; lr: 0.00000845;   0/632 tok/s;  92450 sec
[2020-03-30 21:21:27,508 INFO] Step 56100/210000; acc:  61.47; ppl:  5.07; xent: 1.62; lr: 0.00000844;   0/524 tok/s;  92522 sec
[2020-03-30 21:22:06,664 INFO] Loading train dataset from ../bert_data/cnndm.train.9134.bert.pt, number of examples: 156
[2020-03-30 21:22:17,008 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-30 21:22:43,568 INFO] Step 56150/210000; acc:  43.53; ppl: 20.84; xent: 3.04; lr: 0.00000844;   0/859 tok/s;  92598 sec
[2020-03-30 21:24:07,774 INFO] Step 56200/210000; acc:  50.63; ppl: 13.40; xent: 2.60; lr: 0.00000844;   0/846 tok/s;  92683 sec
[2020-03-30 21:25:06,273 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-30 21:25:33,348 INFO] Step 56250/210000; acc:  46.51; ppl: 15.20; xent: 2.72; lr: 0.00000843;   0/481 tok/s;  92768 sec
[2020-03-30 21:26:57,511 INFO] Step 56300/210000; acc:  47.73; ppl: 15.06; xent: 2.71; lr: 0.00000843;   0/559 tok/s;  92852 sec
[2020-03-30 21:27:53,622 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-30 21:28:22,376 INFO] Step 56350/210000; acc:  47.78; ppl: 13.78; xent: 2.62; lr: 0.00000843;   0/627 tok/s;  92937 sec
[2020-03-30 21:29:46,483 INFO] Step 56400/210000; acc:  46.77; ppl: 17.84; xent: 2.88; lr: 0.00000842;   0/533 tok/s;  93021 sec
[2020-03-30 21:30:40,316 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-30 21:31:10,603 INFO] Step 56450/210000; acc:  43.19; ppl: 18.07; xent: 2.89; lr: 0.00000842;   0/700 tok/s;  93105 sec
[2020-03-30 21:32:34,945 INFO] Step 56500/210000; acc:  46.17; ppl: 17.35; xent: 2.85; lr: 0.00000841;   0/536 tok/s;  93190 sec
[2020-03-30 21:32:34,980 INFO] Saving checkpoint ../models/model_step_56500.pt
[2020-03-30 21:33:31,643 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-30 21:34:01,934 INFO] Step 56550/210000; acc:  51.06; ppl: 11.15; xent: 2.41; lr: 0.00000841;   0/762 tok/s;  93277 sec
[2020-03-30 21:35:25,851 INFO] Step 56600/210000; acc:  48.30; ppl: 12.27; xent: 2.51; lr: 0.00000841;   0/695 tok/s;  93361 sec
[2020-03-30 21:36:18,710 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-30 21:36:50,689 INFO] Step 56650/210000; acc:  49.62; ppl: 13.76; xent: 2.62; lr: 0.00000840;   0/793 tok/s;  93446 sec
[2020-03-30 21:38:15,181 INFO] Step 56700/210000; acc:  51.10; ppl: 11.16; xent: 2.41; lr: 0.00000840;   0/765 tok/s;  93530 sec
[2020-03-30 21:39:06,249 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-30 21:39:40,104 INFO] Step 56750/210000; acc:  44.29; ppl: 16.61; xent: 2.81; lr: 0.00000840;   0/916 tok/s;  93615 sec
[2020-03-30 21:41:04,083 INFO] Step 56800/210000; acc:  43.56; ppl: 18.31; xent: 2.91; lr: 0.00000839;   0/700 tok/s;  93699 sec
[2020-03-30 21:41:55,305 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-30 21:42:29,362 INFO] Step 56850/210000; acc:  47.30; ppl: 13.16; xent: 2.58; lr: 0.00000839;   0/913 tok/s;  93784 sec
[2020-03-30 21:43:53,030 INFO] Step 56900/210000; acc:  45.35; ppl: 18.18; xent: 2.90; lr: 0.00000838;   0/822 tok/s;  93868 sec
[2020-03-30 21:44:42,708 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-30 21:45:18,176 INFO] Step 56950/210000; acc:  50.41; ppl: 11.20; xent: 2.42; lr: 0.00000838;   0/522 tok/s;  93953 sec
[2020-03-30 21:46:42,409 INFO] Step 57000/210000; acc:  45.41; ppl: 15.38; xent: 2.73; lr: 0.00000838;   0/887 tok/s;  94037 sec
[2020-03-30 21:46:42,432 INFO] Saving checkpoint ../models/model_step_57000.pt
[2020-03-30 21:47:31,904 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-30 21:48:09,171 INFO] Step 57050/210000; acc:  47.87; ppl: 12.77; xent: 2.55; lr: 0.00000837;   0/516 tok/s;  94124 sec
[2020-03-30 21:49:32,962 INFO] Step 57100/210000; acc:  47.67; ppl: 14.37; xent: 2.66; lr: 0.00000837;   0/1014 tok/s;  94208 sec
[2020-03-30 21:50:18,598 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-30 21:50:57,083 INFO] Step 57150/210000; acc:  50.81; ppl: 11.92; xent: 2.48; lr: 0.00000837;   0/563 tok/s;  94292 sec
[2020-03-30 21:52:21,370 INFO] Step 57200/210000; acc:  53.16; ppl:  9.67; xent: 2.27; lr: 0.00000836;   0/658 tok/s;  94376 sec
[2020-03-30 21:53:07,351 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-30 21:53:46,159 INFO] Step 57250/210000; acc:  50.24; ppl: 12.25; xent: 2.51; lr: 0.00000836;   0/744 tok/s;  94461 sec
[2020-03-30 21:55:10,437 INFO] Step 57300/210000; acc:  51.25; ppl: 11.95; xent: 2.48; lr: 0.00000836;   0/701 tok/s;  94545 sec
[2020-03-30 21:55:55,163 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-30 21:56:35,430 INFO] Step 57350/210000; acc:  48.29; ppl: 13.76; xent: 2.62; lr: 0.00000835;   0/756 tok/s;  94630 sec
[2020-03-30 21:57:59,554 INFO] Step 57400/210000; acc:  48.81; ppl: 11.54; xent: 2.45; lr: 0.00000835;   0/635 tok/s;  94714 sec
[2020-03-30 21:58:42,146 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-30 21:59:24,368 INFO] Step 57450/210000; acc:  53.08; ppl: 12.01; xent: 2.49; lr: 0.00000834;   0/967 tok/s;  94799 sec
[2020-03-30 22:00:49,018 INFO] Step 57500/210000; acc:  46.11; ppl: 15.09; xent: 2.71; lr: 0.00000834;   0/887 tok/s;  94884 sec
[2020-03-30 22:00:49,042 INFO] Saving checkpoint ../models/model_step_57500.pt
[2020-03-30 22:01:32,135 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-30 22:02:16,150 INFO] Step 57550/210000; acc:  51.60; ppl: 11.34; xent: 2.43; lr: 0.00000834;   0/989 tok/s;  94971 sec
[2020-03-30 22:03:40,609 INFO] Step 57600/210000; acc:  45.91; ppl: 16.17; xent: 2.78; lr: 0.00000833;   0/869 tok/s;  95055 sec
[2020-03-30 22:04:21,833 INFO] Loading train dataset from ../bert_data/cnndm.train.103.bert.pt, number of examples: 2000
[2020-03-30 22:05:05,653 INFO] Step 57650/210000; acc:  49.37; ppl: 11.75; xent: 2.46; lr: 0.00000833;   0/787 tok/s;  95140 sec
[2020-03-30 22:06:30,263 INFO] Step 57700/210000; acc:  47.66; ppl: 15.14; xent: 2.72; lr: 0.00000833;   0/795 tok/s;  95225 sec
[2020-03-30 22:07:09,553 INFO] Loading train dataset from ../bert_data/cnndm.train.104.bert.pt, number of examples: 2000
[2020-03-30 22:07:55,262 INFO] Step 57750/210000; acc:  48.92; ppl: 12.74; xent: 2.54; lr: 0.00000832;   0/566 tok/s;  95310 sec
[2020-03-30 22:09:19,910 INFO] Step 57800/210000; acc:  54.08; ppl:  9.42; xent: 2.24; lr: 0.00000832;   0/447 tok/s;  95395 sec
[2020-03-30 22:09:57,406 INFO] Loading train dataset from ../bert_data/cnndm.train.105.bert.pt, number of examples: 2001
[2020-03-30 22:10:44,601 INFO] Step 57850/210000; acc:  51.93; ppl: 10.75; xent: 2.37; lr: 0.00000832;   0/695 tok/s;  95479 sec
[2020-03-30 22:12:08,538 INFO] Step 57900/210000; acc:  47.58; ppl: 14.49; xent: 2.67; lr: 0.00000831;   0/643 tok/s;  95563 sec
[2020-03-30 22:12:44,362 INFO] Loading train dataset from ../bert_data/cnndm.train.106.bert.pt, number of examples: 1999
[2020-03-30 22:13:33,280 INFO] Step 57950/210000; acc:  50.04; ppl: 13.49; xent: 2.60; lr: 0.00000831;   0/749 tok/s;  95648 sec
[2020-03-30 22:14:57,941 INFO] Step 58000/210000; acc:  48.44; ppl: 13.49; xent: 2.60; lr: 0.00000830;   0/640 tok/s;  95733 sec
[2020-03-30 22:14:57,965 INFO] Saving checkpoint ../models/model_step_58000.pt
[2020-03-30 22:15:36,185 INFO] Loading train dataset from ../bert_data/cnndm.train.107.bert.pt, number of examples: 1999
[2020-03-30 22:16:24,871 INFO] Step 58050/210000; acc:  46.60; ppl: 15.10; xent: 2.71; lr: 0.00000830;   0/1062 tok/s;  95820 sec
[2020-03-30 22:17:49,091 INFO] Step 58100/210000; acc:  52.71; ppl:  9.96; xent: 2.30; lr: 0.00000830;   0/928 tok/s;  95904 sec
[2020-03-30 22:18:21,514 INFO] Loading train dataset from ../bert_data/cnndm.train.108.bert.pt, number of examples: 2000
[2020-03-30 22:19:13,535 INFO] Step 58150/210000; acc:  50.12; ppl: 11.41; xent: 2.43; lr: 0.00000829;   0/757 tok/s;  95988 sec
[2020-03-30 22:20:37,101 INFO] Step 58200/210000; acc:  51.65; ppl: 10.82; xent: 2.38; lr: 0.00000829;   0/680 tok/s;  96072 sec
[2020-03-30 22:21:09,843 INFO] Loading train dataset from ../bert_data/cnndm.train.109.bert.pt, number of examples: 2000
[2020-03-30 22:22:01,930 INFO] Step 58250/210000; acc:  38.19; ppl: 22.45; xent: 3.11; lr: 0.00000829;   0/645 tok/s;  96157 sec
[2020-03-30 22:23:26,055 INFO] Step 58300/210000; acc:  51.55; ppl: 11.14; xent: 2.41; lr: 0.00000828;   0/979 tok/s;  96241 sec
[2020-03-30 22:23:56,886 INFO] Loading train dataset from ../bert_data/cnndm.train.11.bert.pt, number of examples: 1999
[2020-03-30 22:24:50,875 INFO] Step 58350/210000; acc:  54.09; ppl:  9.75; xent: 2.28; lr: 0.00000828;   0/567 tok/s;  96326 sec
[2020-03-30 22:26:15,392 INFO] Step 58400/210000; acc:  52.12; ppl: 10.90; xent: 2.39; lr: 0.00000828;   0/590 tok/s;  96410 sec
[2020-03-30 22:26:44,292 INFO] Loading train dataset from ../bert_data/cnndm.train.110.bert.pt, number of examples: 2000
[2020-03-30 22:27:40,025 INFO] Step 58450/210000; acc:  48.70; ppl: 12.11; xent: 2.49; lr: 0.00000827;   0/583 tok/s;  96495 sec
[2020-03-30 22:29:03,998 INFO] Step 58500/210000; acc:  55.46; ppl:  9.01; xent: 2.20; lr: 0.00000827;   0/571 tok/s;  96579 sec
[2020-03-30 22:29:04,024 INFO] Saving checkpoint ../models/model_step_58500.pt
[2020-03-30 22:29:33,315 INFO] Loading train dataset from ../bert_data/cnndm.train.111.bert.pt, number of examples: 2000
[2020-03-30 22:30:30,235 INFO] Step 58550/210000; acc:  47.98; ppl: 12.30; xent: 2.51; lr: 0.00000827;   0/721 tok/s;  96665 sec
[2020-03-30 22:31:54,283 INFO] Step 58600/210000; acc:  52.44; ppl: 10.81; xent: 2.38; lr: 0.00000826;   0/703 tok/s;  96749 sec
[2020-03-30 22:32:20,022 INFO] Loading train dataset from ../bert_data/cnndm.train.112.bert.pt, number of examples: 1999
[2020-03-30 22:33:18,550 INFO] Step 58650/210000; acc:  52.05; ppl: 10.33; xent: 2.33; lr: 0.00000826;   0/986 tok/s;  96833 sec
[2020-03-30 22:34:42,891 INFO] Step 58700/210000; acc:  52.75; ppl: 10.07; xent: 2.31; lr: 0.00000825;   0/1053 tok/s;  96918 sec
[2020-03-30 22:35:09,400 INFO] Loading train dataset from ../bert_data/cnndm.train.113.bert.pt, number of examples: 1999
[2020-03-30 22:36:08,447 INFO] Step 58750/210000; acc:  48.37; ppl: 12.39; xent: 2.52; lr: 0.00000825;   0/928 tok/s;  97003 sec
[2020-03-30 22:37:32,480 INFO] Step 58800/210000; acc:  53.19; ppl: 11.28; xent: 2.42; lr: 0.00000825;   0/961 tok/s;  97087 sec
[2020-03-30 22:37:56,575 INFO] Loading train dataset from ../bert_data/cnndm.train.114.bert.pt, number of examples: 1998
[2020-03-30 22:38:57,101 INFO] Step 58850/210000; acc:  50.57; ppl: 10.92; xent: 2.39; lr: 0.00000824;   0/672 tok/s;  97172 sec
[2020-03-30 22:40:21,042 INFO] Step 58900/210000; acc:  53.37; ppl: 10.68; xent: 2.37; lr: 0.00000824;   0/864 tok/s;  97256 sec
[2020-03-30 22:40:43,630 INFO] Loading train dataset from ../bert_data/cnndm.train.115.bert.pt, number of examples: 2000
[2020-03-30 22:41:46,095 INFO] Step 58950/210000; acc:  51.74; ppl: 11.44; xent: 2.44; lr: 0.00000824;   0/538 tok/s;  97341 sec
[2020-03-30 22:43:10,403 INFO] Step 59000/210000; acc:  52.46; ppl: 10.45; xent: 2.35; lr: 0.00000823;   0/523 tok/s;  97425 sec
[2020-03-30 22:43:10,406 INFO] Saving checkpoint ../models/model_step_59000.pt
[2020-03-30 22:43:33,416 INFO] Loading train dataset from ../bert_data/cnndm.train.116.bert.pt, number of examples: 2000
[2020-03-30 22:44:37,530 INFO] Step 59050/210000; acc:  52.48; ppl: 10.41; xent: 2.34; lr: 0.00000823;   0/675 tok/s;  97512 sec
[2020-03-30 22:46:01,801 INFO] Step 59100/210000; acc:  50.14; ppl: 11.99; xent: 2.48; lr: 0.00000823;   0/649 tok/s;  97597 sec
[2020-03-30 22:46:20,462 INFO] Loading train dataset from ../bert_data/cnndm.train.117.bert.pt, number of examples: 2000
[2020-03-30 22:47:26,281 INFO] Step 59150/210000; acc:  50.52; ppl: 11.71; xent: 2.46; lr: 0.00000822;   0/846 tok/s;  97681 sec
[2020-03-30 22:48:50,611 INFO] Step 59200/210000; acc:  52.27; ppl: 10.69; xent: 2.37; lr: 0.00000822;   0/794 tok/s;  97765 sec
[2020-03-30 22:49:08,027 INFO] Loading train dataset from ../bert_data/cnndm.train.118.bert.pt, number of examples: 2001
[2020-03-30 22:50:15,338 INFO] Step 59250/210000; acc:  53.99; ppl:  9.33; xent: 2.23; lr: 0.00000822;   0/1010 tok/s;  97850 sec
[2020-03-30 22:51:39,505 INFO] Step 59300/210000; acc:  48.34; ppl: 11.35; xent: 2.43; lr: 0.00000821;   0/923 tok/s;  97934 sec
[2020-03-30 22:51:57,102 INFO] Loading train dataset from ../bert_data/cnndm.train.119.bert.pt, number of examples: 2000
[2020-03-30 22:53:04,394 INFO] Step 59350/210000; acc:  51.61; ppl:  9.48; xent: 2.25; lr: 0.00000821;   0/891 tok/s;  98019 sec
[2020-03-30 22:54:28,606 INFO] Step 59400/210000; acc:  52.41; ppl:  9.59; xent: 2.26; lr: 0.00000821;   0/804 tok/s;  98103 sec
[2020-03-30 22:54:44,622 INFO] Loading train dataset from ../bert_data/cnndm.train.12.bert.pt, number of examples: 2001
[2020-03-30 22:55:53,694 INFO] Step 59450/210000; acc:  50.80; ppl: 11.19; xent: 2.42; lr: 0.00000820;   0/919 tok/s;  98189 sec
[2020-03-30 22:57:17,854 INFO] Step 59500/210000; acc:  51.26; ppl: 10.50; xent: 2.35; lr: 0.00000820;   0/884 tok/s;  98273 sec
[2020-03-30 22:57:17,858 INFO] Saving checkpoint ../models/model_step_59500.pt
[2020-03-30 22:57:33,884 INFO] Loading train dataset from ../bert_data/cnndm.train.120.bert.pt, number of examples: 2001
[2020-03-30 22:58:44,700 INFO] Step 59550/210000; acc:  50.47; ppl: 10.81; xent: 2.38; lr: 0.00000820;   0/516 tok/s;  98360 sec
[2020-03-30 23:00:08,847 INFO] Step 59600/210000; acc:  54.35; ppl:  8.40; xent: 2.13; lr: 0.00000819;   0/487 tok/s;  98444 sec
[2020-03-30 23:00:23,300 INFO] Loading train dataset from ../bert_data/cnndm.train.121.bert.pt, number of examples: 2001
[2020-03-30 23:01:33,997 INFO] Step 59650/210000; acc:  51.75; ppl:  9.72; xent: 2.27; lr: 0.00000819;   0/588 tok/s;  98529 sec
[2020-03-30 23:02:58,160 INFO] Step 59700/210000; acc:  46.76; ppl: 14.54; xent: 2.68; lr: 0.00000819;   0/632 tok/s;  98613 sec
[2020-03-30 23:03:10,608 INFO] Loading train dataset from ../bert_data/cnndm.train.122.bert.pt, number of examples: 2000
[2020-03-30 23:04:23,235 INFO] Step 59750/210000; acc:  52.09; ppl: 10.33; xent: 2.34; lr: 0.00000818;   0/752 tok/s;  98698 sec
[2020-03-30 23:05:47,595 INFO] Step 59800/210000; acc:  56.00; ppl:  8.44; xent: 2.13; lr: 0.00000818;   0/823 tok/s;  98782 sec
[2020-03-30 23:05:56,347 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001
[2020-03-30 23:07:12,061 INFO] Step 59850/210000; acc:  53.02; ppl:  9.97; xent: 2.30; lr: 0.00000818;   0/906 tok/s;  98867 sec
[2020-03-30 23:08:35,775 INFO] Step 59900/210000; acc:  52.51; ppl:  9.77; xent: 2.28; lr: 0.00000817;   0/825 tok/s;  98951 sec
[2020-03-30 23:08:44,584 INFO] Loading train dataset from ../bert_data/cnndm.train.124.bert.pt, number of examples: 2001
[2020-03-30 23:10:00,783 INFO] Step 59950/210000; acc:  49.28; ppl: 11.33; xent: 2.43; lr: 0.00000817;   0/911 tok/s;  99036 sec
[2020-03-30 23:11:24,964 INFO] Step 60000/210000; acc:  50.56; ppl: 10.92; xent: 2.39; lr: 0.00000816;   0/801 tok/s;  99120 sec
[2020-03-30 23:11:24,990 INFO] Saving checkpoint ../models/model_step_60000.pt
[2020-03-30 23:11:34,143 INFO] Loading train dataset from ../bert_data/cnndm.train.125.bert.pt, number of examples: 2000
[2020-03-30 23:12:52,258 INFO] Step 60050/210000; acc:  49.96; ppl: 12.21; xent: 2.50; lr: 0.00000816;   0/814 tok/s;  99207 sec
[2020-03-30 23:14:16,427 INFO] Step 60100/210000; acc:  45.96; ppl: 14.67; xent: 2.69; lr: 0.00000816;   0/656 tok/s;  99291 sec
[2020-03-30 23:14:21,778 INFO] Loading train dataset from ../bert_data/cnndm.train.126.bert.pt, number of examples: 1999
[2020-03-30 23:15:40,740 INFO] Step 60150/210000; acc:  57.40; ppl:  7.20; xent: 1.97; lr: 0.00000815;   0/522 tok/s;  99376 sec
[2020-03-30 23:17:05,224 INFO] Step 60200/210000; acc:  51.90; ppl: 10.04; xent: 2.31; lr: 0.00000815;   0/557 tok/s;  99460 sec
[2020-03-30 23:17:08,943 INFO] Loading train dataset from ../bert_data/cnndm.train.127.bert.pt, number of examples: 2000
[2020-03-30 23:18:29,729 INFO] Step 60250/210000; acc:  50.93; ppl: 12.02; xent: 2.49; lr: 0.00000815;   0/630 tok/s;  99545 sec
[2020-03-30 23:19:54,063 INFO] Step 60300/210000; acc:  55.02; ppl:  8.41; xent: 2.13; lr: 0.00000814;   0/713 tok/s;  99629 sec
[2020-03-30 23:19:56,117 INFO] Loading train dataset from ../bert_data/cnndm.train.128.bert.pt, number of examples: 2001
[2020-03-30 23:21:18,607 INFO] Step 60350/210000; acc:  49.52; ppl: 11.47; xent: 2.44; lr: 0.00000814;   0/695 tok/s;  99713 sec
[2020-03-30 23:22:42,895 INFO] Step 60400/210000; acc:  48.62; ppl: 12.12; xent: 2.50; lr: 0.00000814;   0/774 tok/s;  99798 sec
[2020-03-30 23:22:45,295 INFO] Loading train dataset from ../bert_data/cnndm.train.129.bert.pt, number of examples: 2000
[2020-03-30 23:24:07,972 INFO] Step 60450/210000; acc:  49.88; ppl: 11.33; xent: 2.43; lr: 0.00000813;   0/941 tok/s;  99883 sec
[2020-03-30 23:25:32,503 INFO] Step 60500/210000; acc:  52.88; ppl: 11.33; xent: 2.43; lr: 0.00000813;   0/680 tok/s;  99967 sec
[2020-03-30 23:25:32,524 INFO] Saving checkpoint ../models/model_step_60500.pt
[2020-03-30 23:25:34,975 INFO] Loading train dataset from ../bert_data/cnndm.train.13.bert.pt, number of examples: 2001
[2020-03-30 23:26:59,293 INFO] Step 60550/210000; acc:  49.17; ppl: 11.83; xent: 2.47; lr: 0.00000813;   0/907 tok/s; 100054 sec
[2020-03-30 23:28:22,574 INFO] Loading train dataset from ../bert_data/cnndm.train.130.bert.pt, number of examples: 2000
[2020-03-30 23:28:24,289 INFO] Step 60600/210000; acc:  49.86; ppl: 11.41; xent: 2.43; lr: 0.00000812;   0/531 tok/s; 100139 sec
[2020-03-30 23:29:48,326 INFO] Step 60650/210000; acc:  50.48; ppl: 12.51; xent: 2.53; lr: 0.00000812;   0/499 tok/s; 100223 sec
[2020-03-30 23:31:11,131 INFO] Loading train dataset from ../bert_data/cnndm.train.131.bert.pt, number of examples: 2001
[2020-03-30 23:31:12,813 INFO] Step 60700/210000; acc:  50.45; ppl: 10.63; xent: 2.36; lr: 0.00000812;   0/390 tok/s; 100308 sec
[2020-03-30 23:32:36,375 INFO] Step 60750/210000; acc:  47.24; ppl: 12.82; xent: 2.55; lr: 0.00000811;   0/506 tok/s; 100391 sec
[2020-03-30 23:33:57,973 INFO] Loading train dataset from ../bert_data/cnndm.train.132.bert.pt, number of examples: 2001
[2020-03-30 23:34:01,285 INFO] Step 60800/210000; acc:  45.69; ppl: 14.88; xent: 2.70; lr: 0.00000811;   0/602 tok/s; 100476 sec
[2020-03-30 23:35:25,226 INFO] Step 60850/210000; acc:  50.85; ppl: 10.31; xent: 2.33; lr: 0.00000811;   0/647 tok/s; 100560 sec
[2020-03-30 23:36:44,337 INFO] Loading train dataset from ../bert_data/cnndm.train.133.bert.pt, number of examples: 1999
[2020-03-30 23:36:49,455 INFO] Step 60900/210000; acc:  53.69; ppl:  8.47; xent: 2.14; lr: 0.00000810;   0/645 tok/s; 100644 sec
[2020-03-30 23:38:13,330 INFO] Step 60950/210000; acc:  48.63; ppl: 11.22; xent: 2.42; lr: 0.00000810;   0/677 tok/s; 100728 sec
[2020-03-30 23:39:33,381 INFO] Loading train dataset from ../bert_data/cnndm.train.134.bert.pt, number of examples: 2001
[2020-03-30 23:39:38,387 INFO] Step 61000/210000; acc:  49.27; ppl: 12.11; xent: 2.49; lr: 0.00000810;   0/784 tok/s; 100813 sec
[2020-03-30 23:39:38,389 INFO] Saving checkpoint ../models/model_step_61000.pt
[2020-03-30 23:41:04,691 INFO] Step 61050/210000; acc:  47.43; ppl: 14.33; xent: 2.66; lr: 0.00000809;   0/724 tok/s; 100900 sec
[2020-03-30 23:42:23,114 INFO] Loading train dataset from ../bert_data/cnndm.train.135.bert.pt, number of examples: 1999
[2020-03-30 23:42:29,804 INFO] Step 61100/210000; acc:  53.29; ppl:  9.40; xent: 2.24; lr: 0.00000809;   0/902 tok/s; 100985 sec
[2020-03-30 23:43:53,672 INFO] Step 61150/210000; acc:  52.49; ppl:  9.36; xent: 2.24; lr: 0.00000809;   0/926 tok/s; 101068 sec
[2020-03-30 23:45:10,034 INFO] Loading train dataset from ../bert_data/cnndm.train.136.bert.pt, number of examples: 2001
[2020-03-30 23:45:18,404 INFO] Step 61200/210000; acc:  52.68; ppl:  9.67; xent: 2.27; lr: 0.00000808;   0/998 tok/s; 101153 sec
[2020-03-30 23:46:42,161 INFO] Step 61250/210000; acc:  49.40; ppl: 12.72; xent: 2.54; lr: 0.00000808;   0/889 tok/s; 101237 sec
[2020-03-30 23:47:56,683 INFO] Loading train dataset from ../bert_data/cnndm.train.137.bert.pt, number of examples: 2000
[2020-03-30 23:48:06,826 INFO] Step 61300/210000; acc:  49.26; ppl: 11.39; xent: 2.43; lr: 0.00000808;   0/942 tok/s; 101322 sec
[2020-03-30 23:49:30,678 INFO] Step 61350/210000; acc:  49.71; ppl: 11.15; xent: 2.41; lr: 0.00000807;   0/807 tok/s; 101406 sec
[2020-03-30 23:50:45,359 INFO] Loading train dataset from ../bert_data/cnndm.train.138.bert.pt, number of examples: 2000
[2020-03-30 23:50:55,367 INFO] Step 61400/210000; acc:  49.86; ppl: 11.03; xent: 2.40; lr: 0.00000807;   0/648 tok/s; 101490 sec
[2020-03-30 23:52:19,548 INFO] Step 61450/210000; acc:  49.14; ppl: 11.78; xent: 2.47; lr: 0.00000807;   0/1047 tok/s; 101574 sec
[2020-03-30 23:53:32,208 INFO] Loading train dataset from ../bert_data/cnndm.train.139.bert.pt, number of examples: 2001
[2020-03-30 23:53:43,824 INFO] Step 61500/210000; acc:  54.47; ppl:  8.77; xent: 2.17; lr: 0.00000806;   0/609 tok/s; 101659 sec
[2020-03-30 23:53:43,827 INFO] Saving checkpoint ../models/model_step_61500.pt
[2020-03-30 23:55:09,904 INFO] Step 61550/210000; acc:  51.44; ppl: 10.87; xent: 2.39; lr: 0.00000806;   0/584 tok/s; 101745 sec
[2020-03-30 23:56:22,634 INFO] Loading train dataset from ../bert_data/cnndm.train.14.bert.pt, number of examples: 1998
[2020-03-30 23:56:34,250 INFO] Step 61600/210000; acc:  53.15; ppl:  9.35; xent: 2.24; lr: 0.00000806;   0/599 tok/s; 101829 sec
[2020-03-30 23:57:58,733 INFO] Step 61650/210000; acc:  46.69; ppl: 12.12; xent: 2.50; lr: 0.00000805;   0/446 tok/s; 101914 sec
[2020-03-30 23:59:10,482 INFO] Loading train dataset from ../bert_data/cnndm.train.140.bert.pt, number of examples: 2000
[2020-03-30 23:59:23,937 INFO] Step 61700/210000; acc:  50.28; ppl: 12.69; xent: 2.54; lr: 0.00000805;   0/738 tok/s; 101999 sec
[2020-03-31 00:00:48,409 INFO] Step 61750/210000; acc:  54.55; ppl:  8.52; xent: 2.14; lr: 0.00000805;   0/760 tok/s; 102083 sec
[2020-03-31 00:01:58,187 INFO] Loading train dataset from ../bert_data/cnndm.train.141.bert.pt, number of examples: 1999
[2020-03-31 00:02:13,380 INFO] Step 61800/210000; acc:  52.83; ppl: 10.35; xent: 2.34; lr: 0.00000805;   0/842 tok/s; 102168 sec
[2020-03-31 00:03:37,401 INFO] Step 61850/210000; acc:  50.08; ppl: 11.11; xent: 2.41; lr: 0.00000804;   0/760 tok/s; 102252 sec
[2020-03-31 00:04:44,973 INFO] Loading train dataset from ../bert_data/cnndm.train.142.bert.pt, number of examples: 2000
[2020-03-31 00:05:01,899 INFO] Step 61900/210000; acc:  44.61; ppl: 16.33; xent: 2.79; lr: 0.00000804;   0/858 tok/s; 102337 sec
[2020-03-31 00:06:25,745 INFO] Step 61950/210000; acc:  52.19; ppl:  9.42; xent: 2.24; lr: 0.00000804;   0/761 tok/s; 102421 sec
[2020-03-31 00:07:32,189 INFO] Loading train dataset from ../bert_data/cnndm.train.143.bert.pt, number of examples: 1084
[2020-03-31 00:07:50,793 INFO] Step 62000/210000; acc:  53.50; ppl: 10.18; xent: 2.32; lr: 0.00000803;   0/911 tok/s; 102506 sec
[2020-03-31 00:07:50,825 INFO] Saving checkpoint ../models/model_step_62000.pt
[2020-03-31 00:09:05,310 INFO] Loading train dataset from ../bert_data/cnndm.train.15.bert.pt, number of examples: 1999
[2020-03-31 00:09:17,011 INFO] Step 62050/210000; acc:  48.28; ppl: 12.37; xent: 2.52; lr: 0.00000803;   0/544 tok/s; 102592 sec
[2020-03-31 00:10:40,551 INFO] Step 62100/210000; acc:  46.19; ppl: 13.54; xent: 2.61; lr: 0.00000803;   0/949 tok/s; 102675 sec
[2020-03-31 00:11:52,987 INFO] Loading train dataset from ../bert_data/cnndm.train.150.bert.pt, number of examples: 1985
[2020-03-31 00:12:03,487 INFO] Step 62150/210000; acc:  52.82; ppl:  8.38; xent: 2.13; lr: 0.00000802;   0/640 tok/s; 102758 sec
[2020-03-31 00:13:16,235 INFO] Step 62200/210000; acc:  65.40; ppl:  4.28; xent: 1.45; lr: 0.00000802;   0/403 tok/s; 102831 sec
[2020-03-31 00:14:10,318 INFO] Loading train dataset from ../bert_data/cnndm.train.151.bert.pt, number of examples: 1990
[2020-03-31 00:14:29,303 INFO] Step 62250/210000; acc:  66.30; ppl:  4.09; xent: 1.41; lr: 0.00000802;   0/459 tok/s; 102904 sec
[2020-03-31 00:15:43,370 INFO] Step 62300/210000; acc:  58.28; ppl:  5.62; xent: 1.73; lr: 0.00000801;   0/397 tok/s; 102978 sec
[2020-03-31 00:16:30,334 INFO] Loading train dataset from ../bert_data/cnndm.train.152.bert.pt, number of examples: 993
[2020-03-31 00:16:56,704 INFO] Step 62350/210000; acc:  68.87; ppl:  3.55; xent: 1.27; lr: 0.00000801;   0/413 tok/s; 103052 sec
[2020-03-31 00:17:41,184 INFO] Loading train dataset from ../bert_data/cnndm.train.153.bert.pt, number of examples: 1982
[2020-03-31 00:18:14,254 INFO] Step 62400/210000; acc:  48.01; ppl:  8.47; xent: 2.14; lr: 0.00000801;   0/461 tok/s; 103129 sec
[2020-03-31 00:19:36,973 INFO] Step 62450/210000; acc:  45.42; ppl: 11.57; xent: 2.45; lr: 0.00000800;   0/929 tok/s; 103212 sec
[2020-03-31 00:20:17,225 INFO] Loading train dataset from ../bert_data/cnndm.train.154.bert.pt, number of examples: 1978
[2020-03-31 00:21:00,557 INFO] Step 62500/210000; acc:  50.74; ppl:  7.63; xent: 2.03; lr: 0.00000800;   0/565 tok/s; 103295 sec
[2020-03-31 00:21:00,560 INFO] Saving checkpoint ../models/model_step_62500.pt
[2020-03-31 00:22:25,273 INFO] Step 62550/210000; acc:  41.01; ppl: 15.79; xent: 2.76; lr: 0.00000800;   0/1340 tok/s; 103380 sec
[2020-03-31 00:22:55,925 INFO] Loading train dataset from ../bert_data/cnndm.train.155.bert.pt, number of examples: 1988
[2020-03-31 00:23:48,686 INFO] Step 62600/210000; acc:  49.41; ppl: 10.16; xent: 2.32; lr: 0.00000799;   0/830 tok/s; 103464 sec
[2020-03-31 00:25:11,339 INFO] Step 62650/210000; acc:  49.69; ppl: 10.24; xent: 2.33; lr: 0.00000799;   0/880 tok/s; 103546 sec
[2020-03-31 00:25:33,671 INFO] Loading train dataset from ../bert_data/cnndm.train.156.bert.pt, number of examples: 1993
[2020-03-31 00:26:34,473 INFO] Step 62700/210000; acc:  47.96; ppl: 10.49; xent: 2.35; lr: 0.00000799;   0/820 tok/s; 103629 sec
[2020-03-31 00:27:57,185 INFO] Step 62750/210000; acc:  58.03; ppl:  6.48; xent: 1.87; lr: 0.00000798;   0/393 tok/s; 103712 sec
[2020-03-31 00:28:10,778 INFO] Loading train dataset from ../bert_data/cnndm.train.157.bert.pt, number of examples: 1981
[2020-03-31 00:29:21,000 INFO] Step 62800/210000; acc:  47.65; ppl: 10.51; xent: 2.35; lr: 0.00000798;   0/987 tok/s; 103796 sec
[2020-03-31 00:30:45,372 INFO] Step 62850/210000; acc:  52.40; ppl:  7.87; xent: 2.06; lr: 0.00000798;   0/545 tok/s; 103880 sec
[2020-03-31 00:30:48,881 INFO] Loading train dataset from ../bert_data/cnndm.train.158.bert.pt, number of examples: 1987
[2020-03-31 00:32:07,782 INFO] Step 62900/210000; acc:  42.04; ppl: 16.10; xent: 2.78; lr: 0.00000797;   0/1405 tok/s; 103963 sec
[2020-03-31 00:33:26,286 INFO] Loading train dataset from ../bert_data/cnndm.train.159.bert.pt, number of examples: 1986
[2020-03-31 00:33:31,130 INFO] Step 62950/210000; acc:  49.09; ppl:  8.35; xent: 2.12; lr: 0.00000797;   0/625 tok/s; 104046 sec
[2020-03-31 00:34:53,700 INFO] Step 63000/210000; acc:  44.47; ppl: 12.38; xent: 2.52; lr: 0.00000797;   0/1314 tok/s; 104129 sec
[2020-03-31 00:34:53,704 INFO] Saving checkpoint ../models/model_step_63000.pt
[2020-03-31 00:36:06,335 INFO] Loading train dataset from ../bert_data/cnndm.train.16.bert.pt, number of examples: 2001
[2020-03-31 00:36:19,581 INFO] Step 63050/210000; acc:  53.09; ppl:  9.68; xent: 2.27; lr: 0.00000797;   0/751 tok/s; 104214 sec
[2020-03-31 00:37:43,509 INFO] Step 63100/210000; acc:  50.64; ppl: 10.97; xent: 2.39; lr: 0.00000796;   0/652 tok/s; 104298 sec
[2020-03-31 00:38:53,149 INFO] Loading train dataset from ../bert_data/cnndm.train.160.bert.pt, number of examples: 1983
[2020-03-31 00:39:08,045 INFO] Step 63150/210000; acc:  44.60; ppl: 13.99; xent: 2.64; lr: 0.00000796;   0/1295 tok/s; 104383 sec
[2020-03-31 00:40:31,025 INFO] Step 63200/210000; acc:  56.49; ppl:  6.39; xent: 1.85; lr: 0.00000796;   0/453 tok/s; 104466 sec
[2020-03-31 00:41:31,710 INFO] Loading train dataset from ../bert_data/cnndm.train.161.bert.pt, number of examples: 1990
[2020-03-31 00:41:54,844 INFO] Step 63250/210000; acc:  45.25; ppl: 12.57; xent: 2.53; lr: 0.00000795;   0/1178 tok/s; 104550 sec
[2020-03-31 00:43:17,462 INFO] Step 63300/210000; acc:  54.60; ppl:  6.36; xent: 1.85; lr: 0.00000795;   0/463 tok/s; 104632 sec
[2020-03-31 00:44:08,907 INFO] Loading train dataset from ../bert_data/cnndm.train.162.bert.pt, number of examples: 1986
[2020-03-31 00:44:40,529 INFO] Step 63350/210000; acc:  45.75; ppl: 12.04; xent: 2.49; lr: 0.00000795;   0/1146 tok/s; 104715 sec
[2020-03-31 00:46:03,246 INFO] Step 63400/210000; acc:  55.89; ppl:  6.88; xent: 1.93; lr: 0.00000794;   0/464 tok/s; 104798 sec
[2020-03-31 00:46:47,014 INFO] Loading train dataset from ../bert_data/cnndm.train.163.bert.pt, number of examples: 1982
[2020-03-31 00:47:26,714 INFO] Step 63450/210000; acc:  43.38; ppl: 15.14; xent: 2.72; lr: 0.00000794;   0/1313 tok/s; 104882 sec
[2020-03-31 00:48:49,169 INFO] Step 63500/210000; acc:  57.33; ppl:  5.93; xent: 1.78; lr: 0.00000794;   0/611 tok/s; 104964 sec
[2020-03-31 00:48:49,172 INFO] Saving checkpoint ../models/model_step_63500.pt
[2020-03-31 00:49:24,198 INFO] Loading train dataset from ../bert_data/cnndm.train.164.bert.pt, number of examples: 1984
[2020-03-31 00:50:13,770 INFO] Step 63550/210000; acc:  44.24; ppl: 13.00; xent: 2.57; lr: 0.00000793;   0/755 tok/s; 105049 sec
[2020-03-31 00:51:36,324 INFO] Step 63600/210000; acc:  47.38; ppl: 10.79; xent: 2.38; lr: 0.00000793;   0/827 tok/s; 105131 sec
[2020-03-31 00:52:01,443 INFO] Loading train dataset from ../bert_data/cnndm.train.165.bert.pt, number of examples: 1984
[2020-03-31 00:52:59,035 INFO] Step 63650/210000; acc:  56.24; ppl:  6.69; xent: 1.90; lr: 0.00000793;   0/382 tok/s; 105214 sec
[2020-03-31 00:54:21,568 INFO] Step 63700/210000; acc:  46.67; ppl: 11.69; xent: 2.46; lr: 0.00000792;   0/1064 tok/s; 105296 sec
[2020-03-31 00:54:38,337 INFO] Loading train dataset from ../bert_data/cnndm.train.166.bert.pt, number of examples: 1370
[2020-03-31 00:55:44,809 INFO] Step 63750/210000; acc:  59.86; ppl:  5.44; xent: 1.69; lr: 0.00000792;   0/440 tok/s; 105380 sec
[2020-03-31 00:56:26,598 INFO] Loading train dataset from ../bert_data/cnndm.train.167.bert.pt, number of examples: 1089
[2020-03-31 00:57:06,759 INFO] Step 63800/210000; acc:  46.76; ppl:  9.95; xent: 2.30; lr: 0.00000792;   0/1307 tok/s; 105462 sec
[2020-03-31 00:57:51,306 INFO] Loading train dataset from ../bert_data/cnndm.train.168.bert.pt, number of examples: 1991
[2020-03-31 00:58:26,676 INFO] Step 63850/210000; acc:  54.31; ppl:  6.56; xent: 1.88; lr: 0.00000791;   0/625 tok/s; 105541 sec
[2020-03-31 00:59:47,195 INFO] Step 63900/210000; acc:  45.28; ppl: 12.20; xent: 2.50; lr: 0.00000791;   0/1327 tok/s; 105622 sec
[2020-03-31 01:00:26,576 INFO] Loading train dataset from ../bert_data/cnndm.train.169.bert.pt, number of examples: 1995
[2020-03-31 01:01:08,383 INFO] Step 63950/210000; acc:  56.11; ppl:  5.94; xent: 1.78; lr: 0.00000791;   0/625 tok/s; 105703 sec
[2020-03-31 01:02:29,593 INFO] Step 64000/210000; acc:  47.56; ppl:  9.97; xent: 2.30; lr: 0.00000791;   0/1161 tok/s; 105784 sec
[2020-03-31 01:02:29,595 INFO] Saving checkpoint ../models/model_step_64000.pt
[2020-03-31 01:03:03,173 INFO] Loading train dataset from ../bert_data/cnndm.train.17.bert.pt, number of examples: 2000
[2020-03-31 01:03:55,378 INFO] Step 64050/210000; acc:  53.25; ppl:  9.11; xent: 2.21; lr: 0.00000790;   0/467 tok/s; 105870 sec
[2020-03-31 01:05:19,555 INFO] Step 64100/210000; acc:  50.43; ppl: 10.36; xent: 2.34; lr: 0.00000790;   0/499 tok/s; 105954 sec
[2020-03-31 01:05:50,397 INFO] Loading train dataset from ../bert_data/cnndm.train.170.bert.pt, number of examples: 1981
[2020-03-31 01:06:42,233 INFO] Step 64150/210000; acc:  58.47; ppl:  5.53; xent: 1.71; lr: 0.00000790;   0/832 tok/s; 106037 sec
[2020-03-31 01:08:04,325 INFO] Step 64200/210000; acc:  59.42; ppl:  5.30; xent: 1.67; lr: 0.00000789;   0/420 tok/s; 106119 sec
[2020-03-31 01:08:24,660 INFO] Loading train dataset from ../bert_data/cnndm.train.171.bert.pt, number of examples: 1990
[2020-03-31 01:09:26,392 INFO] Step 64250/210000; acc:  43.17; ppl: 15.12; xent: 2.72; lr: 0.00000789;   0/1348 tok/s; 106201 sec
[2020-03-31 01:10:47,010 INFO] Step 64300/210000; acc:  53.42; ppl:  6.44; xent: 1.86; lr: 0.00000789;   0/540 tok/s; 106282 sec
[2020-03-31 01:10:58,793 INFO] Loading train dataset from ../bert_data/cnndm.train.172.bert.pt, number of examples: 1983
[2020-03-31 01:12:08,323 INFO] Step 64350/210000; acc:  44.31; ppl: 13.27; xent: 2.59; lr: 0.00000788;   0/1389 tok/s; 106363 sec
[2020-03-31 01:13:28,634 INFO] Step 64400/210000; acc:  57.93; ppl:  4.92; xent: 1.59; lr: 0.00000788;   0/558 tok/s; 106443 sec
[2020-03-31 01:13:32,579 INFO] Loading train dataset from ../bert_data/cnndm.train.18.bert.pt, number of examples: 1998
[2020-03-31 01:14:53,020 INFO] Step 64450/210000; acc:  48.40; ppl: 11.50; xent: 2.44; lr: 0.00000788;   0/789 tok/s; 106528 sec
[2020-03-31 01:16:17,316 INFO] Step 64500/210000; acc:  49.64; ppl: 10.60; xent: 2.36; lr: 0.00000787;   0/762 tok/s; 106612 sec
[2020-03-31 01:16:17,338 INFO] Saving checkpoint ../models/model_step_64500.pt
[2020-03-31 01:16:21,531 INFO] Loading train dataset from ../bert_data/cnndm.train.19.bert.pt, number of examples: 2000
[2020-03-31 01:17:43,855 INFO] Step 64550/210000; acc:  56.19; ppl:  7.68; xent: 2.04; lr: 0.00000787;   0/685 tok/s; 106699 sec
[2020-03-31 01:19:08,745 INFO] Step 64600/210000; acc:  48.31; ppl: 12.51; xent: 2.53; lr: 0.00000787;   0/789 tok/s; 106784 sec
[2020-03-31 01:19:09,088 INFO] Loading train dataset from ../bert_data/cnndm.train.2.bert.pt, number of examples: 2001
[2020-03-31 01:20:33,124 INFO] Step 64650/210000; acc:  53.79; ppl:  9.97; xent: 2.30; lr: 0.00000787;   0/722 tok/s; 106868 sec
[2020-03-31 01:21:56,633 INFO] Loading train dataset from ../bert_data/cnndm.train.20.bert.pt, number of examples: 2000
[2020-03-31 01:21:58,346 INFO] Step 64700/210000; acc:  49.21; ppl: 12.28; xent: 2.51; lr: 0.00000786;   0/583 tok/s; 106953 sec
[2020-03-31 01:23:22,946 INFO] Step 64750/210000; acc:  49.94; ppl: 12.06; xent: 2.49; lr: 0.00000786;   0/882 tok/s; 107038 sec
[2020-03-31 01:24:44,006 INFO] Loading train dataset from ../bert_data/cnndm.train.21.bert.pt, number of examples: 2001
[2020-03-31 01:24:47,385 INFO] Step 64800/210000; acc:  48.57; ppl: 11.39; xent: 2.43; lr: 0.00000786;   0/534 tok/s; 107122 sec
[2020-03-31 01:26:11,708 INFO] Step 64850/210000; acc:  51.22; ppl: 11.62; xent: 2.45; lr: 0.00000785;   0/901 tok/s; 107207 sec
[2020-03-31 01:27:32,902 INFO] Loading train dataset from ../bert_data/cnndm.train.22.bert.pt, number of examples: 1999
[2020-03-31 01:27:36,245 INFO] Step 64900/210000; acc:  49.64; ppl: 12.50; xent: 2.53; lr: 0.00000785;   0/673 tok/s; 107291 sec
[2020-03-31 01:29:00,612 INFO] Step 64950/210000; acc:  56.90; ppl:  8.10; xent: 2.09; lr: 0.00000785;   0/651 tok/s; 107375 sec
[2020-03-31 01:30:20,624 INFO] Loading train dataset from ../bert_data/cnndm.train.23.bert.pt, number of examples: 2001
[2020-03-31 01:30:25,644 INFO] Step 65000/210000; acc:  45.88; ppl: 14.24; xent: 2.66; lr: 0.00000784;   0/706 tok/s; 107460 sec
[2020-03-31 01:30:25,646 INFO] Saving checkpoint ../models/model_step_65000.pt
[2020-03-31 01:31:51,675 INFO] Step 65050/210000; acc:  51.26; ppl: 13.21; xent: 2.58; lr: 0.00000784;   0/527 tok/s; 107546 sec
[2020-03-31 01:33:09,455 INFO] Loading train dataset from ../bert_data/cnndm.train.24.bert.pt, number of examples: 2001
[2020-03-31 01:33:16,137 INFO] Step 65100/210000; acc:  51.88; ppl: 10.49; xent: 2.35; lr: 0.00000784;   0/738 tok/s; 107631 sec
[2020-03-31 01:34:40,103 INFO] Step 65150/210000; acc:  56.73; ppl:  8.20; xent: 2.10; lr: 0.00000784;   0/743 tok/s; 107715 sec
[2020-03-31 01:35:58,421 INFO] Loading train dataset from ../bert_data/cnndm.train.25.bert.pt, number of examples: 2001
[2020-03-31 01:36:05,122 INFO] Step 65200/210000; acc:  50.22; ppl: 10.43; xent: 2.35; lr: 0.00000783;   0/813 tok/s; 107800 sec
[2020-03-31 01:37:29,057 INFO] Step 65250/210000; acc:  56.19; ppl:  8.13; xent: 2.10; lr: 0.00000783;   0/811 tok/s; 107884 sec
[2020-03-31 01:38:44,998 INFO] Loading train dataset from ../bert_data/cnndm.train.26.bert.pt, number of examples: 2000
[2020-03-31 01:38:53,549 INFO] Step 65300/210000; acc:  50.45; ppl: 10.60; xent: 2.36; lr: 0.00000783;   0/937 tok/s; 107968 sec
[2020-03-31 01:40:17,518 INFO] Step 65350/210000; acc:  47.69; ppl: 13.82; xent: 2.63; lr: 0.00000782;   0/785 tok/s; 108052 sec
[2020-03-31 01:41:33,858 INFO] Loading train dataset from ../bert_data/cnndm.train.27.bert.pt, number of examples: 2001
[2020-03-31 01:41:42,260 INFO] Step 65400/210000; acc:  56.16; ppl:  9.11; xent: 2.21; lr: 0.00000782;   0/1048 tok/s; 108137 sec
[2020-03-31 01:43:06,827 INFO] Step 65450/210000; acc:  51.96; ppl: 10.62; xent: 2.36; lr: 0.00000782;   0/822 tok/s; 108222 sec
[2020-03-31 01:44:21,561 INFO] Loading train dataset from ../bert_data/cnndm.train.28.bert.pt, number of examples: 2000
[2020-03-31 01:44:31,758 INFO] Step 65500/210000; acc:  50.79; ppl: 11.01; xent: 2.40; lr: 0.00000781;   0/588 tok/s; 108307 sec
[2020-03-31 01:44:31,762 INFO] Saving checkpoint ../models/model_step_65500.pt
[2020-03-31 01:45:58,217 INFO] Step 65550/210000; acc:  46.57; ppl: 12.44; xent: 2.52; lr: 0.00000781;   0/634 tok/s; 108393 sec
[2020-03-31 01:47:11,532 INFO] Loading train dataset from ../bert_data/cnndm.train.29.bert.pt, number of examples: 1999
[2020-03-31 01:47:23,303 INFO] Step 65600/210000; acc:  51.03; ppl:  9.67; xent: 2.27; lr: 0.00000781;   0/539 tok/s; 108478 sec
[2020-03-31 01:48:47,566 INFO] Step 65650/210000; acc:  49.71; ppl: 11.54; xent: 2.45; lr: 0.00000781;   0/949 tok/s; 108562 sec
[2020-03-31 01:49:58,613 INFO] Loading train dataset from ../bert_data/cnndm.train.3.bert.pt, number of examples: 2001
[2020-03-31 01:50:11,882 INFO] Step 65700/210000; acc:  57.46; ppl:  8.01; xent: 2.08; lr: 0.00000780;   0/675 tok/s; 108647 sec
[2020-03-31 01:51:35,780 INFO] Step 65750/210000; acc:  56.19; ppl:  7.42; xent: 2.00; lr: 0.00000780;   0/481 tok/s; 108731 sec
[2020-03-31 01:52:46,869 INFO] Loading train dataset from ../bert_data/cnndm.train.30.bert.pt, number of examples: 1996
[2020-03-31 01:53:00,084 INFO] Step 65800/210000; acc:  55.75; ppl:  8.85; xent: 2.18; lr: 0.00000780;   0/718 tok/s; 108815 sec
[2020-03-31 01:54:24,167 INFO] Step 65850/210000; acc:  49.59; ppl: 10.48; xent: 2.35; lr: 0.00000779;   0/663 tok/s; 108899 sec
[2020-03-31 01:55:34,243 INFO] Loading train dataset from ../bert_data/cnndm.train.31.bert.pt, number of examples: 2000
[2020-03-31 01:55:49,339 INFO] Step 65900/210000; acc:  52.15; ppl: 10.09; xent: 2.31; lr: 0.00000779;   0/827 tok/s; 108984 sec
[2020-03-31 01:57:13,632 INFO] Step 65950/210000; acc:  47.03; ppl: 12.91; xent: 2.56; lr: 0.00000779;   0/883 tok/s; 109068 sec
[2020-03-31 01:58:21,290 INFO] Loading train dataset from ../bert_data/cnndm.train.32.bert.pt, number of examples: 1998
[2020-03-31 01:58:38,284 INFO] Step 66000/210000; acc:  55.39; ppl:  8.49; xent: 2.14; lr: 0.00000778;   0/887 tok/s; 109153 sec
[2020-03-31 01:58:38,288 INFO] Saving checkpoint ../models/model_step_66000.pt
[2020-03-31 02:00:04,175 INFO] Step 66050/210000; acc:  54.49; ppl:  9.73; xent: 2.28; lr: 0.00000778;   0/850 tok/s; 109239 sec
[2020-03-31 02:01:10,197 INFO] Loading train dataset from ../bert_data/cnndm.train.33.bert.pt, number of examples: 1999
[2020-03-31 02:01:28,787 INFO] Step 66100/210000; acc:  53.27; ppl:  9.61; xent: 2.26; lr: 0.00000778;   0/949 tok/s; 109324 sec
[2020-03-31 02:02:52,621 INFO] Step 66150/210000; acc:  48.84; ppl: 11.83; xent: 2.47; lr: 0.00000778;   0/1019 tok/s; 109407 sec
[2020-03-31 02:03:56,541 INFO] Loading train dataset from ../bert_data/cnndm.train.34.bert.pt, number of examples: 2000
[2020-03-31 02:04:16,533 INFO] Step 66200/210000; acc:  48.81; ppl: 12.48; xent: 2.52; lr: 0.00000777;   0/789 tok/s; 109491 sec
[2020-03-31 02:05:40,884 INFO] Step 66250/210000; acc:  53.63; ppl:  8.63; xent: 2.16; lr: 0.00000777;   0/1065 tok/s; 109576 sec
[2020-03-31 02:06:45,409 INFO] Loading train dataset from ../bert_data/cnndm.train.35.bert.pt, number of examples: 1998
[2020-03-31 02:07:05,622 INFO] Step 66300/210000; acc:  60.80; ppl:  5.92; xent: 1.78; lr: 0.00000777;   0/603 tok/s; 109660 sec
[2020-03-31 02:08:29,533 INFO] Step 66350/210000; acc:  54.63; ppl:  8.42; xent: 2.13; lr: 0.00000776;   0/557 tok/s; 109744 sec
[2020-03-31 02:09:32,052 INFO] Loading train dataset from ../bert_data/cnndm.train.36.bert.pt, number of examples: 2000
[2020-03-31 02:09:54,135 INFO] Step 66400/210000; acc:  51.71; ppl: 10.79; xent: 2.38; lr: 0.00000776;   0/687 tok/s; 109829 sec
[2020-03-31 02:11:18,381 INFO] Step 66450/210000; acc:  52.10; ppl:  9.46; xent: 2.25; lr: 0.00000776;   0/783 tok/s; 109913 sec
[2020-03-31 02:12:19,697 INFO] Loading train dataset from ../bert_data/cnndm.train.37.bert.pt, number of examples: 1999
[2020-03-31 02:12:43,249 INFO] Step 66500/210000; acc:  50.73; ppl: 10.94; xent: 2.39; lr: 0.00000776;   0/861 tok/s; 109998 sec
[2020-03-31 02:12:43,253 INFO] Saving checkpoint ../models/model_step_66500.pt
[2020-03-31 02:14:09,436 INFO] Step 66550/210000; acc:  54.41; ppl:  8.47; xent: 2.14; lr: 0.00000775;   0/717 tok/s; 110084 sec
[2020-03-31 02:15:08,536 INFO] Loading train dataset from ../bert_data/cnndm.train.38.bert.pt, number of examples: 2001
[2020-03-31 02:15:33,751 INFO] Step 66600/210000; acc:  51.07; ppl: 10.90; xent: 2.39; lr: 0.00000775;   0/824 tok/s; 110169 sec
[2020-03-31 02:16:58,116 INFO] Step 66650/210000; acc:  52.47; ppl: 10.50; xent: 2.35; lr: 0.00000775;   0/1003 tok/s; 110253 sec
[2020-03-31 02:17:57,881 INFO] Loading train dataset from ../bert_data/cnndm.train.39.bert.pt, number of examples: 2000
[2020-03-31 02:18:23,180 INFO] Step 66700/210000; acc:  55.98; ppl:  8.48; xent: 2.14; lr: 0.00000774;   0/1117 tok/s; 110338 sec
[2020-03-31 02:19:47,135 INFO] Step 66750/210000; acc:  50.26; ppl: 10.12; xent: 2.31; lr: 0.00000774;   0/1136 tok/s; 110422 sec
[2020-03-31 02:20:44,820 INFO] Loading train dataset from ../bert_data/cnndm.train.4.bert.pt, number of examples: 2001
[2020-03-31 02:21:11,836 INFO] Step 66800/210000; acc:  44.52; ppl: 16.54; xent: 2.81; lr: 0.00000774;   0/575 tok/s; 110507 sec
[2020-03-31 02:22:35,861 INFO] Step 66850/210000; acc:  47.43; ppl: 12.09; xent: 2.49; lr: 0.00000774;   0/830 tok/s; 110591 sec
[2020-03-31 02:23:33,823 INFO] Loading train dataset from ../bert_data/cnndm.train.40.bert.pt, number of examples: 1999
[2020-03-31 02:24:00,511 INFO] Step 66900/210000; acc:  57.37; ppl:  7.60; xent: 2.03; lr: 0.00000773;   0/511 tok/s; 110675 sec
[2020-03-31 02:25:24,559 INFO] Step 66950/210000; acc:  56.28; ppl:  8.11; xent: 2.09; lr: 0.00000773;   0/561 tok/s; 110759 sec
[2020-03-31 02:26:20,741 INFO] Loading train dataset from ../bert_data/cnndm.train.41.bert.pt, number of examples: 2000
[2020-03-31 02:26:49,394 INFO] Step 67000/210000; acc:  49.47; ppl: 11.16; xent: 2.41; lr: 0.00000773;   0/704 tok/s; 110844 sec
[2020-03-31 02:26:49,398 INFO] Saving checkpoint ../models/model_step_67000.pt
[2020-03-31 02:28:15,481 INFO] Step 67050/210000; acc:  52.16; ppl: 10.11; xent: 2.31; lr: 0.00000772;   0/662 tok/s; 110930 sec
[2020-03-31 02:29:09,737 INFO] Loading train dataset from ../bert_data/cnndm.train.42.bert.pt, number of examples: 2000
[2020-03-31 02:29:39,955 INFO] Step 67100/210000; acc:  53.72; ppl:  8.83; xent: 2.18; lr: 0.00000772;   0/653 tok/s; 111015 sec
[2020-03-31 02:31:04,344 INFO] Step 67150/210000; acc:  54.48; ppl:  9.13; xent: 2.21; lr: 0.00000772;   0/717 tok/s; 111099 sec
[2020-03-31 02:31:57,029 INFO] Loading train dataset from ../bert_data/cnndm.train.43.bert.pt, number of examples: 2001
[2020-03-31 02:32:28,855 INFO] Step 67200/210000; acc:  47.55; ppl: 13.21; xent: 2.58; lr: 0.00000772;   0/608 tok/s; 111184 sec
[2020-03-31 02:33:52,891 INFO] Step 67250/210000; acc:  50.98; ppl: 12.28; xent: 2.51; lr: 0.00000771;   0/512 tok/s; 111268 sec
[2020-03-31 02:34:43,982 INFO] Loading train dataset from ../bert_data/cnndm.train.44.bert.pt, number of examples: 1998
[2020-03-31 02:35:17,779 INFO] Step 67300/210000; acc:  55.88; ppl:  8.34; xent: 2.12; lr: 0.00000771;   0/856 tok/s; 111353 sec
[2020-03-31 02:36:42,319 INFO] Step 67350/210000; acc:  51.52; ppl: 10.02; xent: 2.30; lr: 0.00000771;   0/830 tok/s; 111437 sec
[2020-03-31 02:37:31,799 INFO] Loading train dataset from ../bert_data/cnndm.train.45.bert.pt, number of examples: 2001
[2020-03-31 02:38:07,276 INFO] Step 67400/210000; acc:  57.17; ppl:  7.84; xent: 2.06; lr: 0.00000770;   0/800 tok/s; 111522 sec
[2020-03-31 02:39:31,569 INFO] Step 67450/210000; acc:  53.24; ppl:  9.41; xent: 2.24; lr: 0.00000770;   0/980 tok/s; 111606 sec
[2020-03-31 02:40:21,377 INFO] Loading train dataset from ../bert_data/cnndm.train.46.bert.pt, number of examples: 2001
[2020-03-31 02:40:56,701 INFO] Step 67500/210000; acc:  48.57; ppl: 11.30; xent: 2.42; lr: 0.00000770;   0/827 tok/s; 111692 sec
[2020-03-31 02:40:56,705 INFO] Saving checkpoint ../models/model_step_67500.pt
[2020-03-31 02:42:22,395 INFO] Step 67550/210000; acc:  51.58; ppl: 11.26; xent: 2.42; lr: 0.00000770;   0/888 tok/s; 111777 sec
[2020-03-31 02:43:09,562 INFO] Loading train dataset from ../bert_data/cnndm.train.47.bert.pt, number of examples: 2000
[2020-03-31 02:43:46,885 INFO] Step 67600/210000; acc:  48.93; ppl: 11.97; xent: 2.48; lr: 0.00000769;   0/538 tok/s; 111862 sec
[2020-03-31 02:45:10,584 INFO] Step 67650/210000; acc:  52.01; ppl: 10.48; xent: 2.35; lr: 0.00000769;   0/839 tok/s; 111945 sec
[2020-03-31 02:45:58,453 INFO] Loading train dataset from ../bert_data/cnndm.train.48.bert.pt, number of examples: 2000
[2020-03-31 02:46:35,354 INFO] Step 67700/210000; acc:  51.86; ppl:  8.98; xent: 2.19; lr: 0.00000769;   0/575 tok/s; 112030 sec
[2020-03-31 02:47:59,063 INFO] Step 67750/210000; acc:  50.80; ppl:  9.22; xent: 2.22; lr: 0.00000768;   0/585 tok/s; 112114 sec
[2020-03-31 02:48:45,731 INFO] Loading train dataset from ../bert_data/cnndm.train.49.bert.pt, number of examples: 2001
[2020-03-31 02:49:24,599 INFO] Step 67800/210000; acc:  50.14; ppl: 11.75; xent: 2.46; lr: 0.00000768;   0/669 tok/s; 112199 sec
[2020-03-31 02:50:48,795 INFO] Step 67850/210000; acc:  49.89; ppl: 11.15; xent: 2.41; lr: 0.00000768;   0/545 tok/s; 112284 sec
[2020-03-31 02:51:32,730 INFO] Loading train dataset from ../bert_data/cnndm.train.5.bert.pt, number of examples: 2001
[2020-03-31 02:52:12,903 INFO] Step 67900/210000; acc:  53.06; ppl:  9.74; xent: 2.28; lr: 0.00000768;   0/762 tok/s; 112368 sec
[2020-03-31 02:53:37,075 INFO] Step 67950/210000; acc:  52.08; ppl: 10.47; xent: 2.35; lr: 0.00000767;   0/758 tok/s; 112452 sec
[2020-03-31 02:54:21,613 INFO] Loading train dataset from ../bert_data/cnndm.train.50.bert.pt, number of examples: 2001
[2020-03-31 02:55:01,717 INFO] Step 68000/210000; acc:  54.99; ppl:  8.74; xent: 2.17; lr: 0.00000767;   0/789 tok/s; 112537 sec
[2020-03-31 02:55:01,720 INFO] Saving checkpoint ../models/model_step_68000.pt
[2020-03-31 02:56:27,674 INFO] Step 68050/210000; acc:  53.26; ppl:  9.03; xent: 2.20; lr: 0.00000767;   0/582 tok/s; 112622 sec
[2020-03-31 02:57:10,009 INFO] Loading train dataset from ../bert_data/cnndm.train.51.bert.pt, number of examples: 2000
[2020-03-31 02:57:52,075 INFO] Step 68100/210000; acc:  54.32; ppl:  8.46; xent: 2.14; lr: 0.00000766;   0/819 tok/s; 112707 sec
[2020-03-31 02:59:16,184 INFO] Step 68150/210000; acc:  57.72; ppl:  7.85; xent: 2.06; lr: 0.00000766;   0/689 tok/s; 112791 sec
[2020-03-31 02:59:59,005 INFO] Loading train dataset from ../bert_data/cnndm.train.52.bert.pt, number of examples: 2001
[2020-03-31 03:00:41,317 INFO] Step 68200/210000; acc:  52.01; ppl: 10.29; xent: 2.33; lr: 0.00000766;   0/991 tok/s; 112876 sec
[2020-03-31 03:02:05,310 INFO] Step 68250/210000; acc:  53.16; ppl:  9.37; xent: 2.24; lr: 0.00000766;   0/1076 tok/s; 112960 sec
[2020-03-31 03:02:45,969 INFO] Loading train dataset from ../bert_data/cnndm.train.53.bert.pt, number of examples: 1999
[2020-03-31 03:03:29,573 INFO] Step 68300/210000; acc:  49.58; ppl: 10.24; xent: 2.33; lr: 0.00000765;   0/848 tok/s; 113044 sec
[2020-03-31 03:04:53,289 INFO] Step 68350/210000; acc:  50.17; ppl: 11.08; xent: 2.40; lr: 0.00000765;   0/865 tok/s; 113128 sec
[2020-03-31 03:05:32,552 INFO] Loading train dataset from ../bert_data/cnndm.train.54.bert.pt, number of examples: 2001
[2020-03-31 03:06:17,875 INFO] Step 68400/210000; acc:  56.00; ppl:  7.70; xent: 2.04; lr: 0.00000765;   0/492 tok/s; 113213 sec
[2020-03-31 03:07:42,581 INFO] Step 68450/210000; acc:  55.35; ppl:  8.29; xent: 2.11; lr: 0.00000764;   0/886 tok/s; 113297 sec
[2020-03-31 03:08:21,892 INFO] Loading train dataset from ../bert_data/cnndm.train.55.bert.pt, number of examples: 1999
[2020-03-31 03:09:07,078 INFO] Step 68500/210000; acc:  54.55; ppl:  7.52; xent: 2.02; lr: 0.00000764;   0/581 tok/s; 113382 sec
[2020-03-31 03:09:07,081 INFO] Saving checkpoint ../models/model_step_68500.pt
[2020-03-31 03:10:33,310 INFO] Step 68550/210000; acc:  48.91; ppl: 12.14; xent: 2.50; lr: 0.00000764;   0/729 tok/s; 113468 sec
[2020-03-31 03:11:10,533 INFO] Loading train dataset from ../bert_data/cnndm.train.56.bert.pt, number of examples: 2001
[2020-03-31 03:11:57,715 INFO] Step 68600/210000; acc:  58.87; ppl:  6.89; xent: 1.93; lr: 0.00000764;   0/569 tok/s; 113553 sec
[2020-03-31 03:13:22,037 INFO] Step 68650/210000; acc:  54.51; ppl:  8.82; xent: 2.18; lr: 0.00000763;   0/506 tok/s; 113637 sec
[2020-03-31 03:13:57,748 INFO] Loading train dataset from ../bert_data/cnndm.train.57.bert.pt, number of examples: 1999
[2020-03-31 03:14:46,468 INFO] Step 68700/210000; acc:  58.19; ppl:  7.52; xent: 2.02; lr: 0.00000763;   0/780 tok/s; 113721 sec
[2020-03-31 03:16:10,933 INFO] Step 68750/210000; acc:  51.01; ppl: 10.64; xent: 2.36; lr: 0.00000763;   0/790 tok/s; 113806 sec
[2020-03-31 03:16:44,839 INFO] Loading train dataset from ../bert_data/cnndm.train.58.bert.pt, number of examples: 2000
[2020-03-31 03:17:35,480 INFO] Step 68800/210000; acc:  51.03; ppl: 10.06; xent: 2.31; lr: 0.00000762;   0/847 tok/s; 113890 sec
[2020-03-31 03:18:59,727 INFO] Step 68850/210000; acc:  49.75; ppl: 11.04; xent: 2.40; lr: 0.00000762;   0/817 tok/s; 113975 sec
[2020-03-31 03:19:34,580 INFO] Loading train dataset from ../bert_data/cnndm.train.59.bert.pt, number of examples: 2000
[2020-03-31 03:20:25,405 INFO] Step 68900/210000; acc:  44.81; ppl: 14.46; xent: 2.67; lr: 0.00000762;   0/1073 tok/s; 114060 sec
[2020-03-31 03:21:49,704 INFO] Step 68950/210000; acc:  48.34; ppl: 11.64; xent: 2.45; lr: 0.00000762;   0/909 tok/s; 114145 sec
[2020-03-31 03:22:22,361 INFO] Loading train dataset from ../bert_data/cnndm.train.6.bert.pt, number of examples: 2001
[2020-03-31 03:23:14,698 INFO] Step 69000/210000; acc:  54.26; ppl:  9.14; xent: 2.21; lr: 0.00000761;   0/516 tok/s; 114230 sec
[2020-03-31 03:23:14,702 INFO] Saving checkpoint ../models/model_step_69000.pt
[2020-03-31 03:24:41,205 INFO] Step 69050/210000; acc:  55.45; ppl:  8.62; xent: 2.15; lr: 0.00000761;   0/560 tok/s; 114316 sec
[2020-03-31 03:25:12,337 INFO] Loading train dataset from ../bert_data/cnndm.train.60.bert.pt, number of examples: 2000
[2020-03-31 03:26:06,011 INFO] Step 69100/210000; acc:  54.99; ppl:  7.72; xent: 2.04; lr: 0.00000761;   0/565 tok/s; 114401 sec
[2020-03-31 03:27:30,292 INFO] Step 69150/210000; acc:  53.17; ppl:  9.50; xent: 2.25; lr: 0.00000761;   0/538 tok/s; 114485 sec
[2020-03-31 03:27:59,685 INFO] Loading train dataset from ../bert_data/cnndm.train.61.bert.pt, number of examples: 2001
[2020-03-31 03:28:55,128 INFO] Step 69200/210000; acc:  46.90; ppl: 13.73; xent: 2.62; lr: 0.00000760;   0/730 tok/s; 114570 sec
[2020-03-31 03:30:19,466 INFO] Step 69250/210000; acc:  53.36; ppl:  9.25; xent: 2.22; lr: 0.00000760;   0/709 tok/s; 114654 sec
[2020-03-31 03:30:46,624 INFO] Loading train dataset from ../bert_data/cnndm.train.62.bert.pt, number of examples: 2001
[2020-03-31 03:31:43,759 INFO] Step 69300/210000; acc:  50.11; ppl: 10.65; xent: 2.37; lr: 0.00000760;   0/813 tok/s; 114739 sec
[2020-03-31 03:33:08,237 INFO] Step 69350/210000; acc:  53.08; ppl: 10.40; xent: 2.34; lr: 0.00000759;   0/798 tok/s; 114823 sec
[2020-03-31 03:33:35,830 INFO] Loading train dataset from ../bert_data/cnndm.train.63.bert.pt, number of examples: 2001
[2020-03-31 03:34:33,022 INFO] Step 69400/210000; acc:  53.35; ppl:  9.25; xent: 2.22; lr: 0.00000759;   0/811 tok/s; 114908 sec
[2020-03-31 03:35:57,119 INFO] Step 69450/210000; acc:  54.56; ppl:  8.42; xent: 2.13; lr: 0.00000759;   0/527 tok/s; 114992 sec
[2020-03-31 03:36:22,702 INFO] Loading train dataset from ../bert_data/cnndm.train.64.bert.pt, number of examples: 2001
[2020-03-31 03:37:21,665 INFO] Step 69500/210000; acc:  46.97; ppl: 13.15; xent: 2.58; lr: 0.00000759;   0/887 tok/s; 115076 sec
[2020-03-31 03:37:21,669 INFO] Saving checkpoint ../models/model_step_69500.pt
[2020-03-31 03:38:48,026 INFO] Step 69550/210000; acc:  51.89; ppl: 10.03; xent: 2.31; lr: 0.00000758;   0/682 tok/s; 115163 sec
[2020-03-31 03:39:11,904 INFO] Loading train dataset from ../bert_data/cnndm.train.65.bert.pt, number of examples: 2000
[2020-03-31 03:40:12,515 INFO] Step 69600/210000; acc:  51.77; ppl: 10.06; xent: 2.31; lr: 0.00000758;   0/916 tok/s; 115247 sec
[2020-03-31 03:41:37,105 INFO] Step 69650/210000; acc:  51.47; ppl: 10.17; xent: 2.32; lr: 0.00000758;   0/1040 tok/s; 115332 sec
[2020-03-31 03:42:01,541 INFO] Loading train dataset from ../bert_data/cnndm.train.66.bert.pt, number of examples: 2001
[2020-03-31 03:43:02,066 INFO] Step 69700/210000; acc:  55.35; ppl:  8.15; xent: 2.10; lr: 0.00000758;   0/574 tok/s; 115417 sec
[2020-03-31 03:44:26,202 INFO] Step 69750/210000; acc:  49.61; ppl: 10.81; xent: 2.38; lr: 0.00000757;   0/921 tok/s; 115501 sec
[2020-03-31 03:44:48,646 INFO] Loading train dataset from ../bert_data/cnndm.train.67.bert.pt, number of examples: 1999
[2020-03-31 03:45:51,238 INFO] Step 69800/210000; acc:  57.06; ppl:  6.68; xent: 1.90; lr: 0.00000757;   0/450 tok/s; 115586 sec
[2020-03-31 03:47:15,276 INFO] Step 69850/210000; acc:  48.65; ppl: 12.36; xent: 2.51; lr: 0.00000757;   0/802 tok/s; 115670 sec
[2020-03-31 03:47:35,951 INFO] Loading train dataset from ../bert_data/cnndm.train.68.bert.pt, number of examples: 1999
[2020-03-31 03:48:39,924 INFO] Step 69900/210000; acc:  54.41; ppl:  8.52; xent: 2.14; lr: 0.00000756;   0/637 tok/s; 115755 sec
[2020-03-31 03:50:04,332 INFO] Step 69950/210000; acc:  54.17; ppl:  9.03; xent: 2.20; lr: 0.00000756;   0/530 tok/s; 115839 sec
[2020-03-31 03:50:23,276 INFO] Loading train dataset from ../bert_data/cnndm.train.69.bert.pt, number of examples: 2000
[2020-03-31 03:51:28,862 INFO] Step 70000/210000; acc:  44.08; ppl: 15.89; xent: 2.77; lr: 0.00000756;   0/873 tok/s; 115924 sec
[2020-03-31 03:51:28,865 INFO] Saving checkpoint ../models/model_step_70000.pt
[2020-03-31 03:52:55,588 INFO] Step 70050/210000; acc:  52.97; ppl:  8.57; xent: 2.15; lr: 0.00000756;   0/967 tok/s; 116010 sec
[2020-03-31 03:53:12,714 INFO] Loading train dataset from ../bert_data/cnndm.train.7.bert.pt, number of examples: 2001
[2020-03-31 03:54:19,984 INFO] Step 70100/210000; acc:  50.39; ppl: 10.62; xent: 2.36; lr: 0.00000755;   0/903 tok/s; 116095 sec
[2020-03-31 03:55:44,060 INFO] Step 70150/210000; acc:  56.43; ppl:  7.62; xent: 2.03; lr: 0.00000755;   0/996 tok/s; 116179 sec
[2020-03-31 03:56:01,729 INFO] Loading train dataset from ../bert_data/cnndm.train.70.bert.pt, number of examples: 2000
[2020-03-31 03:57:09,322 INFO] Step 70200/210000; acc:  47.71; ppl: 11.83; xent: 2.47; lr: 0.00000755;   0/1026 tok/s; 116264 sec
[2020-03-31 03:58:33,265 INFO] Step 70250/210000; acc:  48.76; ppl: 12.03; xent: 2.49; lr: 0.00000755;   0/1039 tok/s; 116348 sec
[2020-03-31 03:58:48,637 INFO] Loading train dataset from ../bert_data/cnndm.train.71.bert.pt, number of examples: 1999
[2020-03-31 03:59:57,726 INFO] Step 70300/210000; acc:  47.25; ppl: 12.53; xent: 2.53; lr: 0.00000754;   0/810 tok/s; 116433 sec
[2020-03-31 04:01:21,584 INFO] Step 70350/210000; acc:  55.35; ppl:  8.51; xent: 2.14; lr: 0.00000754;   0/739 tok/s; 116516 sec
[2020-03-31 04:01:37,482 INFO] Loading train dataset from ../bert_data/cnndm.train.72.bert.pt, number of examples: 2000
[2020-03-31 04:02:46,867 INFO] Step 70400/210000; acc:  53.72; ppl:  8.95; xent: 2.19; lr: 0.00000754;   0/885 tok/s; 116602 sec
[2020-03-31 04:04:11,056 INFO] Step 70450/210000; acc:  50.31; ppl: 10.55; xent: 2.36; lr: 0.00000754;   0/997 tok/s; 116686 sec
[2020-03-31 04:04:25,180 INFO] Loading train dataset from ../bert_data/cnndm.train.73.bert.pt, number of examples: 2000
[2020-03-31 04:05:35,642 INFO] Step 70500/210000; acc:  54.72; ppl:  8.51; xent: 2.14; lr: 0.00000753;   0/661 tok/s; 116770 sec
[2020-03-31 04:05:35,645 INFO] Saving checkpoint ../models/model_step_70500.pt
[2020-03-31 04:07:01,896 INFO] Step 70550/210000; acc:  47.87; ppl: 11.15; xent: 2.41; lr: 0.00000753;   0/525 tok/s; 116857 sec
[2020-03-31 04:07:14,360 INFO] Loading train dataset from ../bert_data/cnndm.train.74.bert.pt, number of examples: 2000
[2020-03-31 04:08:26,774 INFO] Step 70600/210000; acc:  53.64; ppl:  9.03; xent: 2.20; lr: 0.00000753;   0/558 tok/s; 116942 sec
[2020-03-31 04:09:50,762 INFO] Step 70650/210000; acc:  42.74; ppl: 15.75; xent: 2.76; lr: 0.00000752;   0/547 tok/s; 117026 sec
[2020-03-31 04:10:01,184 INFO] Loading train dataset from ../bert_data/cnndm.train.75.bert.pt, number of examples: 1999
[2020-03-31 04:11:15,550 INFO] Step 70700/210000; acc:  50.34; ppl: 11.59; xent: 2.45; lr: 0.00000752;   0/704 tok/s; 117110 sec
[2020-03-31 04:12:39,413 INFO] Step 70750/210000; acc:  50.67; ppl: 11.01; xent: 2.40; lr: 0.00000752;   0/581 tok/s; 117194 sec
[2020-03-31 04:12:48,166 INFO] Loading train dataset from ../bert_data/cnndm.train.76.bert.pt, number of examples: 1999
[2020-03-31 04:14:04,012 INFO] Step 70800/210000; acc:  49.82; ppl: 11.09; xent: 2.41; lr: 0.00000752;   0/979 tok/s; 117279 sec
[2020-03-31 04:15:27,970 INFO] Step 70850/210000; acc:  52.68; ppl: 11.13; xent: 2.41; lr: 0.00000751;   0/748 tok/s; 117363 sec
[2020-03-31 04:15:37,126 INFO] Loading train dataset from ../bert_data/cnndm.train.77.bert.pt, number of examples: 2001
[2020-03-31 04:16:53,509 INFO] Step 70900/210000; acc:  52.75; ppl:  9.56; xent: 2.26; lr: 0.00000751;   0/983 tok/s; 117448 sec
[2020-03-31 04:18:17,708 INFO] Step 70950/210000; acc:  51.84; ppl: 11.35; xent: 2.43; lr: 0.00000751;   0/1047 tok/s; 117533 sec
[2020-03-31 04:18:25,242 INFO] Loading train dataset from ../bert_data/cnndm.train.78.bert.pt, number of examples: 2001
[2020-03-31 04:19:42,519 INFO] Step 71000/210000; acc:  47.31; ppl: 14.05; xent: 2.64; lr: 0.00000751;   0/657 tok/s; 117617 sec
[2020-03-31 04:19:42,523 INFO] Saving checkpoint ../models/model_step_71000.pt
[2020-03-31 04:21:08,911 INFO] Step 71050/210000; acc:  46.89; ppl: 11.64; xent: 2.45; lr: 0.00000750;   0/649 tok/s; 117704 sec
[2020-03-31 04:21:14,326 INFO] Loading train dataset from ../bert_data/cnndm.train.79.bert.pt, number of examples: 1999
[2020-03-31 04:22:33,801 INFO] Step 71100/210000; acc:  53.14; ppl:  9.16; xent: 2.21; lr: 0.00000750;   0/485 tok/s; 117789 sec
[2020-03-31 04:23:57,910 INFO] Step 71150/210000; acc:  50.22; ppl: 10.09; xent: 2.31; lr: 0.00000750;   0/555 tok/s; 117873 sec
[2020-03-31 04:24:03,858 INFO] Loading train dataset from ../bert_data/cnndm.train.8.bert.pt, number of examples: 2000
[2020-03-31 04:25:23,270 INFO] Step 71200/210000; acc:  56.57; ppl:  7.92; xent: 2.07; lr: 0.00000750;   0/575 tok/s; 117958 sec
[2020-03-31 04:26:47,882 INFO] Step 71250/210000; acc:  49.64; ppl: 10.78; xent: 2.38; lr: 0.00000749;   0/816 tok/s; 118043 sec
[2020-03-31 04:26:49,899 INFO] Loading train dataset from ../bert_data/cnndm.train.80.bert.pt, number of examples: 1999
[2020-03-31 04:28:12,531 INFO] Step 71300/210000; acc:  51.32; ppl:  9.95; xent: 2.30; lr: 0.00000749;   0/794 tok/s; 118127 sec
[2020-03-31 04:29:36,744 INFO] Step 71350/210000; acc:  50.59; ppl: 10.91; xent: 2.39; lr: 0.00000749;   0/755 tok/s; 118212 sec
[2020-03-31 04:29:37,119 INFO] Loading train dataset from ../bert_data/cnndm.train.81.bert.pt, number of examples: 2000
[2020-03-31 04:31:01,329 INFO] Step 71400/210000; acc:  51.85; ppl:  8.86; xent: 2.18; lr: 0.00000748;   0/767 tok/s; 118296 sec
[2020-03-31 04:32:25,626 INFO] Step 71450/210000; acc:  53.21; ppl:  8.94; xent: 2.19; lr: 0.00000748;   0/775 tok/s; 118380 sec
[2020-03-31 04:32:25,987 INFO] Loading train dataset from ../bert_data/cnndm.train.82.bert.pt, number of examples: 2001
[2020-03-31 04:33:50,112 INFO] Step 71500/210000; acc:  56.81; ppl:  8.48; xent: 2.14; lr: 0.00000748;   0/883 tok/s; 118465 sec
[2020-03-31 04:33:50,115 INFO] Saving checkpoint ../models/model_step_71500.pt
[2020-03-31 04:35:15,435 INFO] Loading train dataset from ../bert_data/cnndm.train.83.bert.pt, number of examples: 1999
[2020-03-31 04:35:17,323 INFO] Step 71550/210000; acc:  47.80; ppl: 12.70; xent: 2.54; lr: 0.00000748;   0/561 tok/s; 118552 sec
[2020-03-31 04:36:41,788 INFO] Step 71600/210000; acc:  50.04; ppl: 10.94; xent: 2.39; lr: 0.00000747;   0/792 tok/s; 118637 sec
[2020-03-31 04:38:03,633 INFO] Loading train dataset from ../bert_data/cnndm.train.84.bert.pt, number of examples: 2000
[2020-03-31 04:38:07,034 INFO] Step 71650/210000; acc:  51.05; ppl:  9.81; xent: 2.28; lr: 0.00000747;   0/579 tok/s; 118722 sec
[2020-03-31 04:39:31,319 INFO] Step 71700/210000; acc:  48.55; ppl:  9.05; xent: 2.20; lr: 0.00000747;   0/572 tok/s; 118806 sec
[2020-03-31 04:40:50,847 INFO] Loading train dataset from ../bert_data/cnndm.train.85.bert.pt, number of examples: 2001
[2020-03-31 04:40:56,037 INFO] Step 71750/210000; acc:  55.27; ppl:  8.62; xent: 2.15; lr: 0.00000747;   0/655 tok/s; 118891 sec
[2020-03-31 04:42:20,408 INFO] Step 71800/210000; acc:  54.57; ppl:  8.67; xent: 2.16; lr: 0.00000746;   0/600 tok/s; 118975 sec
[2020-03-31 04:43:38,043 INFO] Loading train dataset from ../bert_data/cnndm.train.86.bert.pt, number of examples: 1999
[2020-03-31 04:43:44,878 INFO] Step 71850/210000; acc:  56.69; ppl:  7.37; xent: 2.00; lr: 0.00000746;   0/789 tok/s; 119060 sec
[2020-03-31 04:45:08,867 INFO] Step 71900/210000; acc:  54.68; ppl:  8.89; xent: 2.18; lr: 0.00000746;   0/688 tok/s; 119144 sec
[2020-03-31 04:46:25,506 INFO] Loading train dataset from ../bert_data/cnndm.train.87.bert.pt, number of examples: 1999
[2020-03-31 04:46:34,193 INFO] Step 71950/210000; acc:  52.84; ppl:  9.70; xent: 2.27; lr: 0.00000746;   0/923 tok/s; 119229 sec
[2020-03-31 04:47:58,701 INFO] Step 72000/210000; acc:  52.42; ppl: 10.49; xent: 2.35; lr: 0.00000745;   0/896 tok/s; 119314 sec
[2020-03-31 04:47:58,705 INFO] Saving checkpoint ../models/model_step_72000.pt
[2020-03-31 04:49:15,317 INFO] Loading train dataset from ../bert_data/cnndm.train.88.bert.pt, number of examples: 1999
[2020-03-31 04:49:25,481 INFO] Step 72050/210000; acc:  53.49; ppl:  9.00; xent: 2.20; lr: 0.00000745;   0/838 tok/s; 119400 sec
[2020-03-31 04:50:49,493 INFO] Step 72100/210000; acc:  54.44; ppl:  8.36; xent: 2.12; lr: 0.00000745;   0/974 tok/s; 119484 sec
[2020-03-31 04:52:03,744 INFO] Loading train dataset from ../bert_data/cnndm.train.89.bert.pt, number of examples: 2001
[2020-03-31 04:52:13,760 INFO] Step 72150/210000; acc:  51.23; ppl:  9.62; xent: 2.26; lr: 0.00000745;   0/561 tok/s; 119569 sec
[2020-03-31 04:53:37,646 INFO] Step 72200/210000; acc:  51.07; ppl:  9.73; xent: 2.28; lr: 0.00000744;   0/770 tok/s; 119652 sec
[2020-03-31 04:54:50,107 INFO] Loading train dataset from ../bert_data/cnndm.train.9.bert.pt, number of examples: 1999
[2020-03-31 04:55:01,886 INFO] Step 72250/210000; acc:  51.27; ppl:  9.40; xent: 2.24; lr: 0.00000744;   0/547 tok/s; 119737 sec
[2020-03-31 04:56:26,080 INFO] Step 72300/210000; acc:  52.32; ppl:  9.60; xent: 2.26; lr: 0.00000744;   0/544 tok/s; 119821 sec
[2020-03-31 04:57:39,501 INFO] Loading train dataset from ../bert_data/cnndm.train.90.bert.pt, number of examples: 2000
[2020-03-31 04:57:51,249 INFO] Step 72350/210000; acc:  51.75; ppl: 10.17; xent: 2.32; lr: 0.00000744;   0/653 tok/s; 119906 sec
[2020-03-31 04:59:15,698 INFO] Step 72400/210000; acc:  48.83; ppl: 11.61; xent: 2.45; lr: 0.00000743;   0/575 tok/s; 119991 sec
[2020-03-31 05:00:27,357 INFO] Loading train dataset from ../bert_data/cnndm.train.9000.bert.pt, number of examples: 1984
[2020-03-31 05:00:40,559 INFO] Step 72450/210000; acc:  42.03; ppl: 14.79; xent: 2.69; lr: 0.00000743;   0/989 tok/s; 120075 sec
[2020-03-31 05:02:02,829 INFO] Step 72500/210000; acc:  59.57; ppl:  5.55; xent: 1.71; lr: 0.00000743;   0/444 tok/s; 120158 sec
[2020-03-31 05:02:02,832 INFO] Saving checkpoint ../models/model_step_72500.pt
[2020-03-31 05:03:05,132 INFO] Loading train dataset from ../bert_data/cnndm.train.9001.bert.pt, number of examples: 1983
[2020-03-31 05:03:27,954 INFO] Step 72550/210000; acc:  37.72; ppl: 19.26; xent: 2.96; lr: 0.00000743;   0/1508 tok/s; 120243 sec
[2020-03-31 05:04:50,153 INFO] Step 72600/210000; acc:  50.00; ppl:  8.29; xent: 2.12; lr: 0.00000742;   0/459 tok/s; 120325 sec
[2020-03-31 05:05:41,159 INFO] Loading train dataset from ../bert_data/cnndm.train.9002.bert.pt, number of examples: 1990
[2020-03-31 05:06:12,341 INFO] Step 72650/210000; acc:  40.85; ppl: 15.41; xent: 2.73; lr: 0.00000742;   0/1382 tok/s; 120407 sec
[2020-03-31 05:07:34,302 INFO] Step 72700/210000; acc:  57.60; ppl:  6.72; xent: 1.91; lr: 0.00000742;   0/538 tok/s; 120489 sec
[2020-03-31 05:08:17,205 INFO] Loading train dataset from ../bert_data/cnndm.train.9003.bert.pt, number of examples: 1985
[2020-03-31 05:08:56,874 INFO] Step 72750/210000; acc:  45.26; ppl: 13.66; xent: 2.61; lr: 0.00000742;   0/1281 tok/s; 120572 sec
[2020-03-31 05:10:18,751 INFO] Step 72800/210000; acc:  52.46; ppl:  8.12; xent: 2.09; lr: 0.00000741;   0/498 tok/s; 120654 sec
[2020-03-31 05:10:53,738 INFO] Loading train dataset from ../bert_data/cnndm.train.9004.bert.pt, number of examples: 1981
[2020-03-31 05:11:41,534 INFO] Step 72850/210000; acc:  42.48; ppl: 13.92; xent: 2.63; lr: 0.00000741;   0/1366 tok/s; 120736 sec
[2020-03-31 05:13:04,365 INFO] Step 72900/210000; acc:  51.75; ppl:  8.48; xent: 2.14; lr: 0.00000741;   0/615 tok/s; 120819 sec
[2020-03-31 05:13:31,134 INFO] Loading train dataset from ../bert_data/cnndm.train.9005.bert.pt, number of examples: 1986
[2020-03-31 05:14:26,999 INFO] Step 72950/210000; acc:  47.66; ppl: 10.79; xent: 2.38; lr: 0.00000740;   0/876 tok/s; 120902 sec
[2020-03-31 05:15:49,173 INFO] Step 73000/210000; acc:  48.08; ppl: 10.31; xent: 2.33; lr: 0.00000740;   0/717 tok/s; 120984 sec
[2020-03-31 05:15:49,176 INFO] Saving checkpoint ../models/model_step_73000.pt
[2020-03-31 05:16:10,283 INFO] Loading train dataset from ../bert_data/cnndm.train.9006.bert.pt, number of examples: 1993
[2020-03-31 05:17:13,946 INFO] Step 73050/210000; acc:  49.12; ppl:  9.11; xent: 2.21; lr: 0.00000740;   0/593 tok/s; 121069 sec
[2020-03-31 05:18:36,676 INFO] Step 73100/210000; acc:  45.14; ppl: 13.91; xent: 2.63; lr: 0.00000740;   0/831 tok/s; 121151 sec
[2020-03-31 05:18:47,392 INFO] Loading train dataset from ../bert_data/cnndm.train.9007.bert.pt, number of examples: 1983
[2020-03-31 05:20:00,298 INFO] Step 73150/210000; acc:  54.10; ppl:  7.29; xent: 1.99; lr: 0.00000739;   0/485 tok/s; 121235 sec
[2020-03-31 05:21:22,973 INFO] Step 73200/210000; acc:  43.87; ppl: 13.85; xent: 2.63; lr: 0.00000739;   0/1278 tok/s; 121318 sec
[2020-03-31 05:21:23,298 INFO] Loading train dataset from ../bert_data/cnndm.train.9008.bert.pt, number of examples: 1990
[2020-03-31 05:22:45,262 INFO] Step 73250/210000; acc:  49.93; ppl:  8.47; xent: 2.14; lr: 0.00000739;   0/486 tok/s; 121400 sec
[2020-03-31 05:23:59,891 INFO] Loading train dataset from ../bert_data/cnndm.train.9009.bert.pt, number of examples: 1988
[2020-03-31 05:24:08,221 INFO] Step 73300/210000; acc:  44.81; ppl: 12.55; xent: 2.53; lr: 0.00000739;   0/1442 tok/s; 121483 sec
[2020-03-31 05:25:30,652 INFO] Step 73350/210000; acc:  54.58; ppl:  7.31; xent: 1.99; lr: 0.00000738;   0/552 tok/s; 121565 sec
[2020-03-31 05:26:37,714 INFO] Loading train dataset from ../bert_data/cnndm.train.9010.bert.pt, number of examples: 1984
[2020-03-31 05:26:54,260 INFO] Step 73400/210000; acc:  44.38; ppl: 13.57; xent: 2.61; lr: 0.00000738;   0/1325 tok/s; 121649 sec
[2020-03-31 05:28:16,796 INFO] Step 73450/210000; acc:  52.14; ppl:  8.53; xent: 2.14; lr: 0.00000738;   0/654 tok/s; 121732 sec
[2020-03-31 05:29:14,556 INFO] Loading train dataset from ../bert_data/cnndm.train.9011.bert.pt, number of examples: 1983
[2020-03-31 05:29:39,754 INFO] Step 73500/210000; acc:  49.51; ppl:  9.26; xent: 2.23; lr: 0.00000738;   0/678 tok/s; 121815 sec
[2020-03-31 05:29:39,757 INFO] Saving checkpoint ../models/model_step_73500.pt
[2020-03-31 05:31:04,010 INFO] Step 73550/210000; acc:  51.52; ppl:  8.33; xent: 2.12; lr: 0.00000737;   0/677 tok/s; 121899 sec
[2020-03-31 05:31:53,941 INFO] Loading train dataset from ../bert_data/cnndm.train.9012.bert.pt, number of examples: 1985
[2020-03-31 05:32:26,981 INFO] Step 73600/210000; acc:  44.75; ppl: 12.60; xent: 2.53; lr: 0.00000737;   0/870 tok/s; 121982 sec
[2020-03-31 05:33:49,726 INFO] Step 73650/210000; acc:  48.52; ppl: 11.12; xent: 2.41; lr: 0.00000737;   0/755 tok/s; 122065 sec
[2020-03-31 05:34:32,045 INFO] Loading train dataset from ../bert_data/cnndm.train.9013.bert.pt, number of examples: 1987
[2020-03-31 05:35:13,459 INFO] Step 73700/210000; acc:  49.39; ppl:  8.45; xent: 2.13; lr: 0.00000737;   0/470 tok/s; 122148 sec
[2020-03-31 05:36:35,331 INFO] Step 73750/210000; acc:  50.52; ppl: 10.10; xent: 2.31; lr: 0.00000736;   0/891 tok/s; 122230 sec
[2020-03-31 05:37:09,061 INFO] Loading train dataset from ../bert_data/cnndm.train.9014.bert.pt, number of examples: 1976
[2020-03-31 05:37:58,368 INFO] Step 73800/210000; acc:  57.80; ppl:  5.82; xent: 1.76; lr: 0.00000736;   0/484 tok/s; 122313 sec
[2020-03-31 05:39:21,308 INFO] Step 73850/210000; acc:  44.00; ppl: 13.85; xent: 2.63; lr: 0.00000736;   0/1398 tok/s; 122396 sec
[2020-03-31 05:39:43,083 INFO] Loading train dataset from ../bert_data/cnndm.train.9015.bert.pt, number of examples: 1982
[2020-03-31 05:40:44,874 INFO] Step 73900/210000; acc:  49.50; ppl:  9.78; xent: 2.28; lr: 0.00000736;   0/1065 tok/s; 122480 sec
[2020-03-31 05:42:06,857 INFO] Step 73950/210000; acc:  44.28; ppl: 13.71; xent: 2.62; lr: 0.00000735;   0/679 tok/s; 122562 sec
[2020-03-31 05:42:20,849 INFO] Loading train dataset from ../bert_data/cnndm.train.9016.bert.pt, number of examples: 1984
[2020-03-31 05:43:29,141 INFO] Step 74000/210000; acc:  43.21; ppl: 14.58; xent: 2.68; lr: 0.00000735;   0/983 tok/s; 122644 sec
[2020-03-31 05:43:29,145 INFO] Saving checkpoint ../models/model_step_74000.pt
[2020-03-31 05:44:53,549 INFO] Step 74050/210000; acc:  51.46; ppl:  8.30; xent: 2.12; lr: 0.00000735;   0/554 tok/s; 122728 sec
[2020-03-31 05:44:59,118 INFO] Loading train dataset from ../bert_data/cnndm.train.9017.bert.pt, number of examples: 1984
[2020-03-31 05:46:16,501 INFO] Step 74100/210000; acc:  45.19; ppl: 12.25; xent: 2.51; lr: 0.00000735;   0/1007 tok/s; 122811 sec
[2020-03-31 05:47:35,297 INFO] Loading train dataset from ../bert_data/cnndm.train.9018.bert.pt, number of examples: 1989
[2020-03-31 05:47:38,581 INFO] Step 74150/210000; acc:  56.41; ppl:  6.78; xent: 1.91; lr: 0.00000734;   0/495 tok/s; 122893 sec
[2020-03-31 05:49:00,716 INFO] Step 74200/210000; acc:  43.59; ppl: 12.65; xent: 2.54; lr: 0.00000734;   0/1286 tok/s; 122976 sec
[2020-03-31 05:50:11,750 INFO] Loading train dataset from ../bert_data/cnndm.train.9019.bert.pt, number of examples: 1986
[2020-03-31 05:50:23,395 INFO] Step 74250/210000; acc:  54.30; ppl:  7.19; xent: 1.97; lr: 0.00000734;   0/544 tok/s; 123058 sec
[2020-03-31 05:51:44,950 INFO] Step 74300/210000; acc:  43.21; ppl: 13.56; xent: 2.61; lr: 0.00000734;   0/1227 tok/s; 123140 sec
[2020-03-31 05:52:47,663 INFO] Loading train dataset from ../bert_data/cnndm.train.9020.bert.pt, number of examples: 1992
[2020-03-31 05:53:07,400 INFO] Step 74350/210000; acc:  53.69; ppl:  6.84; xent: 1.92; lr: 0.00000733;   0/601 tok/s; 123222 sec
[2020-03-31 05:54:28,809 INFO] Step 74400/210000; acc:  46.58; ppl: 11.99; xent: 2.48; lr: 0.00000733;   0/1248 tok/s; 123304 sec
[2020-03-31 05:55:23,579 INFO] Loading train dataset from ../bert_data/cnndm.train.9021.bert.pt, number of examples: 1988
[2020-03-31 05:55:51,853 INFO] Step 74450/210000; acc:  55.53; ppl:  7.40; xent: 2.00; lr: 0.00000733;   0/533 tok/s; 123387 sec
[2020-03-31 05:57:13,608 INFO] Step 74500/210000; acc:  46.67; ppl: 13.05; xent: 2.57; lr: 0.00000733;   0/1286 tok/s; 123468 sec
[2020-03-31 05:57:13,612 INFO] Saving checkpoint ../models/model_step_74500.pt
[2020-03-31 05:58:01,929 INFO] Loading train dataset from ../bert_data/cnndm.train.9022.bert.pt, number of examples: 1991
[2020-03-31 05:58:37,639 INFO] Step 74550/210000; acc:  54.39; ppl:  6.84; xent: 1.92; lr: 0.00000732;   0/619 tok/s; 123552 sec
[2020-03-31 05:59:59,240 INFO] Step 74600/210000; acc:  42.82; ppl: 13.76; xent: 2.62; lr: 0.00000732;   0/1237 tok/s; 123634 sec
[2020-03-31 06:00:38,974 INFO] Loading train dataset from ../bert_data/cnndm.train.9023.bert.pt, number of examples: 1986
[2020-03-31 06:01:21,035 INFO] Step 74650/210000; acc:  54.41; ppl:  6.90; xent: 1.93; lr: 0.00000732;   0/483 tok/s; 123716 sec
[2020-03-31 06:02:43,127 INFO] Step 74700/210000; acc:  44.77; ppl: 12.78; xent: 2.55; lr: 0.00000732;   0/1273 tok/s; 123798 sec
[2020-03-31 06:03:15,228 INFO] Loading train dataset from ../bert_data/cnndm.train.9024.bert.pt, number of examples: 1989
[2020-03-31 06:04:06,204 INFO] Step 74750/210000; acc:  59.13; ppl:  6.06; xent: 1.80; lr: 0.00000732;   0/584 tok/s; 123881 sec
[2020-03-31 06:05:27,990 INFO] Step 74800/210000; acc:  44.60; ppl: 11.71; xent: 2.46; lr: 0.00000731;   0/1275 tok/s; 123963 sec
[2020-03-31 06:05:51,440 INFO] Loading train dataset from ../bert_data/cnndm.train.9025.bert.pt, number of examples: 1984
[2020-03-31 06:06:50,866 INFO] Step 74850/210000; acc:  51.30; ppl:  8.05; xent: 2.09; lr: 0.00000731;   0/604 tok/s; 124046 sec
[2020-03-31 06:08:13,146 INFO] Step 74900/210000; acc:  41.90; ppl: 16.18; xent: 2.78; lr: 0.00000731;   0/1417 tok/s; 124128 sec
[2020-03-31 06:08:28,839 INFO] Loading train dataset from ../bert_data/cnndm.train.9026.bert.pt, number of examples: 1986
[2020-03-31 06:09:36,496 INFO] Step 74950/210000; acc:  49.15; ppl:  9.97; xent: 2.30; lr: 0.00000731;   0/767 tok/s; 124211 sec
[2020-03-31 06:10:59,561 INFO] Step 75000/210000; acc:  44.52; ppl: 13.96; xent: 2.64; lr: 0.00000730;   0/940 tok/s; 124294 sec
[2020-03-31 06:10:59,564 INFO] Saving checkpoint ../models/model_step_75000.pt
[2020-03-31 06:11:06,827 INFO] Loading train dataset from ../bert_data/cnndm.train.9027.bert.pt, number of examples: 1989
[2020-03-31 06:12:24,083 INFO] Step 75050/210000; acc:  50.45; ppl:  8.68; xent: 2.16; lr: 0.00000730;   0/808 tok/s; 124379 sec
[2020-03-31 06:13:45,498 INFO] Loading train dataset from ../bert_data/cnndm.train.9028.bert.pt, number of examples: 1986
[2020-03-31 06:13:47,147 INFO] Step 75100/210000; acc:  54.94; ppl:  6.69; xent: 1.90; lr: 0.00000730;   0/387 tok/s; 124462 sec
[2020-03-31 06:15:09,891 INFO] Step 75150/210000; acc:  52.65; ppl:  7.66; xent: 2.04; lr: 0.00000730;   0/839 tok/s; 124545 sec
[2020-03-31 06:16:22,361 INFO] Loading train dataset from ../bert_data/cnndm.train.9029.bert.pt, number of examples: 1981
[2020-03-31 06:16:32,302 INFO] Step 75200/210000; acc:  49.07; ppl:  9.84; xent: 2.29; lr: 0.00000729;   0/552 tok/s; 124627 sec
[2020-03-31 06:17:54,534 INFO] Step 75250/210000; acc:  47.47; ppl: 12.03; xent: 2.49; lr: 0.00000729;   0/952 tok/s; 124709 sec
[2020-03-31 06:18:58,428 INFO] Loading train dataset from ../bert_data/cnndm.train.9030.bert.pt, number of examples: 1984
[2020-03-31 06:19:18,672 INFO] Step 75300/210000; acc:  54.46; ppl:  7.36; xent: 2.00; lr: 0.00000729;   0/531 tok/s; 124793 sec
[2020-03-31 06:20:40,287 INFO] Step 75350/210000; acc:  42.38; ppl: 14.10; xent: 2.65; lr: 0.00000729;   0/1203 tok/s; 124875 sec
[2020-03-31 06:21:34,819 INFO] Loading train dataset from ../bert_data/cnndm.train.9031.bert.pt, number of examples: 1975
[2020-03-31 06:22:02,664 INFO] Step 75400/210000; acc:  52.29; ppl:  7.67; xent: 2.04; lr: 0.00000728;   0/634 tok/s; 124957 sec
[2020-03-31 06:23:25,834 INFO] Step 75450/210000; acc:  43.02; ppl: 15.83; xent: 2.76; lr: 0.00000728;   0/1212 tok/s; 125041 sec
[2020-03-31 06:24:11,250 INFO] Loading train dataset from ../bert_data/cnndm.train.9032.bert.pt, number of examples: 1986
[2020-03-31 06:24:48,369 INFO] Step 75500/210000; acc:  50.03; ppl:  9.41; xent: 2.24; lr: 0.00000728;   0/1005 tok/s; 125123 sec
[2020-03-31 06:24:48,372 INFO] Saving checkpoint ../models/model_step_75500.pt
[2020-03-31 06:26:13,135 INFO] Step 75550/210000; acc:  53.26; ppl:  7.66; xent: 2.04; lr: 0.00000728;   0/462 tok/s; 125208 sec
[2020-03-31 06:26:50,412 INFO] Loading train dataset from ../bert_data/cnndm.train.9033.bert.pt, number of examples: 1986
[2020-03-31 06:27:35,955 INFO] Step 75600/210000; acc:  44.47; ppl: 12.38; xent: 2.52; lr: 0.00000727;   0/1227 tok/s; 125291 sec
[2020-03-31 06:28:57,456 INFO] Step 75650/210000; acc:  53.90; ppl:  7.17; xent: 1.97; lr: 0.00000727;   0/530 tok/s; 125372 sec
[2020-03-31 06:29:24,200 INFO] Loading train dataset from ../bert_data/cnndm.train.9034.bert.pt, number of examples: 1989
[2020-03-31 06:30:19,010 INFO] Step 75700/210000; acc:  43.43; ppl: 15.65; xent: 2.75; lr: 0.00000727;   0/1513 tok/s; 125454 sec
[2020-03-31 06:31:41,573 INFO] Step 75750/210000; acc:  53.99; ppl:  7.26; xent: 1.98; lr: 0.00000727;   0/601 tok/s; 125536 sec
[2020-03-31 06:31:59,443 INFO] Loading train dataset from ../bert_data/cnndm.train.9035.bert.pt, number of examples: 1988
[2020-03-31 06:33:02,734 INFO] Step 75800/210000; acc:  42.69; ppl: 14.50; xent: 2.67; lr: 0.00000726;   0/1387 tok/s; 125618 sec
[2020-03-31 06:34:23,444 INFO] Step 75850/210000; acc:  56.19; ppl:  6.55; xent: 1.88; lr: 0.00000726;   0/549 tok/s; 125698 sec
[2020-03-31 06:34:33,709 INFO] Loading train dataset from ../bert_data/cnndm.train.9036.bert.pt, number of examples: 1984
[2020-03-31 06:35:47,385 INFO] Step 75900/210000; acc:  54.34; ppl:  8.31; xent: 2.12; lr: 0.00000726;   0/619 tok/s; 125782 sec
[2020-03-31 06:37:09,513 INFO] Step 75950/210000; acc:  46.58; ppl: 11.89; xent: 2.48; lr: 0.00000726;   0/1297 tok/s; 125864 sec
[2020-03-31 06:37:09,817 INFO] Loading train dataset from ../bert_data/cnndm.train.9037.bert.pt, number of examples: 1983
[2020-03-31 06:38:32,559 INFO] Step 76000/210000; acc:  54.03; ppl:  7.62; xent: 2.03; lr: 0.00000725;   0/657 tok/s; 125947 sec
[2020-03-31 06:38:32,562 INFO] Saving checkpoint ../models/model_step_76000.pt
[2020-03-31 06:39:48,426 INFO] Loading train dataset from ../bert_data/cnndm.train.9038.bert.pt, number of examples: 1982
[2020-03-31 06:39:56,838 INFO] Step 76050/210000; acc:  45.56; ppl: 12.00; xent: 2.49; lr: 0.00000725;   0/987 tok/s; 126032 sec
[2020-03-31 06:41:19,228 INFO] Step 76100/210000; acc:  58.32; ppl:  6.08; xent: 1.80; lr: 0.00000725;   0/597 tok/s; 126114 sec
[2020-03-31 06:42:26,142 INFO] Loading train dataset from ../bert_data/cnndm.train.9039.bert.pt, number of examples: 1983
[2020-03-31 06:42:42,361 INFO] Step 76150/210000; acc:  44.74; ppl: 11.81; xent: 2.47; lr: 0.00000725;   0/799 tok/s; 126197 sec
[2020-03-31 06:44:03,371 INFO] Step 76200/210000; acc:  49.46; ppl:  8.81; xent: 2.18; lr: 0.00000725;   0/766 tok/s; 126278 sec
[2020-03-31 06:44:59,816 INFO] Loading train dataset from ../bert_data/cnndm.train.9040.bert.pt, number of examples: 1982
[2020-03-31 06:45:26,663 INFO] Step 76250/210000; acc:  53.01; ppl:  7.18; xent: 1.97; lr: 0.00000724;   0/391 tok/s; 126361 sec
[2020-03-31 06:46:48,933 INFO] Step 76300/210000; acc:  43.17; ppl: 14.22; xent: 2.65; lr: 0.00000724;   0/1131 tok/s; 126444 sec
[2020-03-31 06:47:34,772 INFO] Loading train dataset from ../bert_data/cnndm.train.9041.bert.pt, number of examples: 1986
[2020-03-31 06:48:11,545 INFO] Step 76350/210000; acc:  58.56; ppl:  6.10; xent: 1.81; lr: 0.00000724;   0/588 tok/s; 126526 sec
[2020-03-31 06:49:33,400 INFO] Step 76400/210000; acc:  43.66; ppl: 12.93; xent: 2.56; lr: 0.00000724;   0/1420 tok/s; 126608 sec
[2020-03-31 06:50:13,416 INFO] Loading train dataset from ../bert_data/cnndm.train.9042.bert.pt, number of examples: 1984
[2020-03-31 06:50:55,504 INFO] Step 76450/210000; acc:  53.92; ppl:  7.47; xent: 2.01; lr: 0.00000723;   0/533 tok/s; 126690 sec
[2020-03-31 06:52:16,967 INFO] Step 76500/210000; acc:  48.39; ppl: 10.34; xent: 2.34; lr: 0.00000723;   0/1225 tok/s; 126772 sec
[2020-03-31 06:52:16,971 INFO] Saving checkpoint ../models/model_step_76500.pt
[2020-03-31 06:52:51,165 INFO] Loading train dataset from ../bert_data/cnndm.train.9043.bert.pt, number of examples: 1984
[2020-03-31 06:53:42,192 INFO] Step 76550/210000; acc:  53.66; ppl:  7.42; xent: 2.00; lr: 0.00000723;   0/688 tok/s; 126857 sec
[2020-03-31 06:55:05,160 INFO] Step 76600/210000; acc:  42.46; ppl: 15.24; xent: 2.72; lr: 0.00000723;   0/1340 tok/s; 126940 sec
[2020-03-31 06:55:26,549 INFO] Loading train dataset from ../bert_data/cnndm.train.9044.bert.pt, number of examples: 1988
[2020-03-31 06:56:27,517 INFO] Step 76650/210000; acc:  49.94; ppl:  9.36; xent: 2.24; lr: 0.00000722;   0/970 tok/s; 127022 sec
[2020-03-31 06:57:49,921 INFO] Step 76700/210000; acc:  40.96; ppl: 15.70; xent: 2.75; lr: 0.00000722;   0/1056 tok/s; 127105 sec
[2020-03-31 06:58:03,284 INFO] Loading train dataset from ../bert_data/cnndm.train.9045.bert.pt, number of examples: 1983
[2020-03-31 06:59:12,429 INFO] Step 76750/210000; acc:  48.17; ppl: 10.24; xent: 2.33; lr: 0.00000722;   0/998 tok/s; 127187 sec
[2020-03-31 07:00:35,273 INFO] Step 76800/210000; acc:  51.68; ppl:  9.23; xent: 2.22; lr: 0.00000722;   0/599 tok/s; 127270 sec
[2020-03-31 07:00:40,675 INFO] Loading train dataset from ../bert_data/cnndm.train.9046.bert.pt, number of examples: 1983
[2020-03-31 07:01:58,287 INFO] Step 76850/210000; acc:  43.95; ppl: 12.87; xent: 2.55; lr: 0.00000721;   0/1383 tok/s; 127353 sec
[2020-03-31 07:03:16,453 INFO] Loading train dataset from ../bert_data/cnndm.train.9047.bert.pt, number of examples: 1983
[2020-03-31 07:03:21,503 INFO] Step 76900/210000; acc:  51.50; ppl:  7.91; xent: 2.07; lr: 0.00000721;   0/746 tok/s; 127436 sec
[2020-03-31 07:04:43,711 INFO] Step 76950/210000; acc:  49.15; ppl: 10.08; xent: 2.31; lr: 0.00000721;   0/1344 tok/s; 127519 sec
[2020-03-31 07:05:52,991 INFO] Loading train dataset from ../bert_data/cnndm.train.9048.bert.pt, number of examples: 1984
[2020-03-31 07:06:06,120 INFO] Step 77000/210000; acc:  51.38; ppl:  7.65; xent: 2.03; lr: 0.00000721;   0/704 tok/s; 127601 sec
[2020-03-31 07:06:06,123 INFO] Saving checkpoint ../models/model_step_77000.pt
[2020-03-31 07:07:30,915 INFO] Step 77050/210000; acc:  38.88; ppl: 17.19; xent: 2.84; lr: 0.00000721;   0/1358 tok/s; 127686 sec
[2020-03-31 07:08:32,917 INFO] Loading train dataset from ../bert_data/cnndm.train.9049.bert.pt, number of examples: 1978
[2020-03-31 07:08:54,745 INFO] Step 77100/210000; acc:  50.44; ppl:  9.63; xent: 2.26; lr: 0.00000720;   0/865 tok/s; 127770 sec
[2020-03-31 07:10:17,104 INFO] Step 77150/210000; acc:  51.17; ppl:  8.61; xent: 2.15; lr: 0.00000720;   0/561 tok/s; 127852 sec
[2020-03-31 07:11:07,355 INFO] Loading train dataset from ../bert_data/cnndm.train.9050.bert.pt, number of examples: 1978
[2020-03-31 07:11:38,270 INFO] Step 77200/210000; acc:  47.14; ppl: 10.73; xent: 2.37; lr: 0.00000720;   0/1069 tok/s; 127933 sec
[2020-03-31 07:13:01,352 INFO] Step 77250/210000; acc:  59.25; ppl:  5.78; xent: 1.75; lr: 0.00000720;   0/383 tok/s; 128016 sec
[2020-03-31 07:13:44,277 INFO] Loading train dataset from ../bert_data/cnndm.train.9051.bert.pt, number of examples: 1981
[2020-03-31 07:14:23,734 INFO] Step 77300/210000; acc:  46.29; ppl: 12.18; xent: 2.50; lr: 0.00000719;   0/1315 tok/s; 128099 sec
[2020-03-31 07:15:45,648 INFO] Step 77350/210000; acc:  51.64; ppl:  8.66; xent: 2.16; lr: 0.00000719;   0/662 tok/s; 128180 sec
[2020-03-31 07:16:20,752 INFO] Loading train dataset from ../bert_data/cnndm.train.9052.bert.pt, number of examples: 1981
[2020-03-31 07:17:08,619 INFO] Step 77400/210000; acc:  49.27; ppl:  9.21; xent: 2.22; lr: 0.00000719;   0/737 tok/s; 128263 sec
[2020-03-31 07:18:30,919 INFO] Step 77450/210000; acc:  46.46; ppl: 11.33; xent: 2.43; lr: 0.00000719;   0/1050 tok/s; 128346 sec
[2020-03-31 07:18:56,533 INFO] Loading train dataset from ../bert_data/cnndm.train.9053.bert.pt, number of examples: 1981
[2020-03-31 07:19:53,586 INFO] Step 77500/210000; acc:  53.78; ppl:  7.28; xent: 1.99; lr: 0.00000718;   0/461 tok/s; 128428 sec
[2020-03-31 07:19:53,589 INFO] Saving checkpoint ../models/model_step_77500.pt
[2020-03-31 07:21:17,795 INFO] Step 77550/210000; acc:  48.32; ppl: 10.94; xent: 2.39; lr: 0.00000718;   0/1152 tok/s; 128513 sec
[2020-03-31 07:21:32,814 INFO] Loading train dataset from ../bert_data/cnndm.train.9054.bert.pt, number of examples: 1987
[2020-03-31 07:22:40,068 INFO] Step 77600/210000; acc:  57.11; ppl:  6.09; xent: 1.81; lr: 0.00000718;   0/555 tok/s; 128595 sec
[2020-03-31 07:24:02,566 INFO] Step 77650/210000; acc:  43.27; ppl: 13.67; xent: 2.62; lr: 0.00000718;   0/1423 tok/s; 128677 sec
[2020-03-31 07:24:11,690 INFO] Loading train dataset from ../bert_data/cnndm.train.9055.bert.pt, number of examples: 1984
[2020-03-31 07:25:25,892 INFO] Step 77700/210000; acc:  51.16; ppl:  8.16; xent: 2.10; lr: 0.00000717;   0/696 tok/s; 128761 sec
[2020-03-31 07:26:47,628 INFO] Loading train dataset from ../bert_data/cnndm.train.9056.bert.pt, number of examples: 1985
[2020-03-31 07:26:49,403 INFO] Step 77750/210000; acc:  52.81; ppl:  8.09; xent: 2.09; lr: 0.00000717;   0/402 tok/s; 128844 sec
[2020-03-31 07:28:11,733 INFO] Step 77800/210000; acc:  52.30; ppl:  8.25; xent: 2.11; lr: 0.00000717;   0/664 tok/s; 128927 sec
[2020-03-31 07:29:24,953 INFO] Loading train dataset from ../bert_data/cnndm.train.9057.bert.pt, number of examples: 1986
[2020-03-31 07:29:35,011 INFO] Step 77850/210000; acc:  57.55; ppl:  6.52; xent: 1.87; lr: 0.00000717;   0/397 tok/s; 129010 sec
[2020-03-31 07:30:58,101 INFO] Step 77900/210000; acc:  47.83; ppl:  9.77; xent: 2.28; lr: 0.00000717;   0/942 tok/s; 129093 sec
[2020-03-31 07:32:00,960 INFO] Loading train dataset from ../bert_data/cnndm.train.9058.bert.pt, number of examples: 1979
[2020-03-31 07:32:21,170 INFO] Step 77950/210000; acc:  52.03; ppl:  7.94; xent: 2.07; lr: 0.00000716;   0/561 tok/s; 129176 sec
[2020-03-31 07:33:43,348 INFO] Step 78000/210000; acc:  46.22; ppl: 12.61; xent: 2.53; lr: 0.00000716;   0/1289 tok/s; 129258 sec
[2020-03-31 07:33:43,351 INFO] Saving checkpoint ../models/model_step_78000.pt
[2020-03-31 07:34:40,285 INFO] Loading train dataset from ../bert_data/cnndm.train.9059.bert.pt, number of examples: 1987
[2020-03-31 07:35:08,032 INFO] Step 78050/210000; acc:  49.61; ppl:  9.75; xent: 2.28; lr: 0.00000716;   0/761 tok/s; 129343 sec
[2020-03-31 07:36:30,689 INFO] Step 78100/210000; acc:  48.09; ppl: 10.36; xent: 2.34; lr: 0.00000716;   0/898 tok/s; 129426 sec
[2020-03-31 07:37:14,702 INFO] Loading train dataset from ../bert_data/cnndm.train.9060.bert.pt, number of examples: 1992
[2020-03-31 07:37:52,677 INFO] Step 78150/210000; acc:  50.32; ppl:  8.21; xent: 2.11; lr: 0.00000715;   0/1032 tok/s; 129507 sec
[2020-03-31 07:39:14,688 INFO] Step 78200/210000; acc:  43.97; ppl: 13.24; xent: 2.58; lr: 0.00000715;   0/599 tok/s; 129590 sec
[2020-03-31 07:39:53,351 INFO] Loading train dataset from ../bert_data/cnndm.train.9061.bert.pt, number of examples: 1985
[2020-03-31 07:40:38,042 INFO] Step 78250/210000; acc:  51.50; ppl:  8.05; xent: 2.09; lr: 0.00000715;   0/883 tok/s; 129673 sec
[2020-03-31 07:41:59,727 INFO] Step 78300/210000; acc:  47.93; ppl: 11.14; xent: 2.41; lr: 0.00000715;   0/1272 tok/s; 129755 sec
[2020-03-31 07:42:30,248 INFO] Loading train dataset from ../bert_data/cnndm.train.9062.bert.pt, number of examples: 1982
[2020-03-31 07:43:22,476 INFO] Step 78350/210000; acc:  53.61; ppl:  6.83; xent: 1.92; lr: 0.00000715;   0/774 tok/s; 129837 sec
[2020-03-31 07:44:44,682 INFO] Step 78400/210000; acc:  47.50; ppl:  9.85; xent: 2.29; lr: 0.00000714;   0/1181 tok/s; 129920 sec
[2020-03-31 07:45:06,458 INFO] Loading train dataset from ../bert_data/cnndm.train.9063.bert.pt, number of examples: 1988
[2020-03-31 07:46:07,221 INFO] Step 78450/210000; acc:  45.76; ppl: 11.76; xent: 2.47; lr: 0.00000714;   0/782 tok/s; 130002 sec
[2020-03-31 07:47:29,975 INFO] Step 78500/210000; acc:  55.38; ppl:  6.93; xent: 1.94; lr: 0.00000714;   0/390 tok/s; 130085 sec
[2020-03-31 07:47:29,978 INFO] Saving checkpoint ../models/model_step_78500.pt
[2020-03-31 07:47:45,903 INFO] Loading train dataset from ../bert_data/cnndm.train.9064.bert.pt, number of examples: 1984
[2020-03-31 07:48:54,409 INFO] Step 78550/210000; acc:  46.41; ppl: 10.48; xent: 2.35; lr: 0.00000714;   0/1118 tok/s; 130169 sec
[2020-03-31 07:50:15,844 INFO] Step 78600/210000; acc:  54.74; ppl:  6.65; xent: 1.89; lr: 0.00000713;   0/470 tok/s; 130251 sec
[2020-03-31 07:50:21,238 INFO] Loading train dataset from ../bert_data/cnndm.train.9065.bert.pt, number of examples: 1986
[2020-03-31 07:51:38,875 INFO] Step 78650/210000; acc:  43.11; ppl: 12.68; xent: 2.54; lr: 0.00000713;   0/1212 tok/s; 130334 sec
[2020-03-31 07:52:57,572 INFO] Loading train dataset from ../bert_data/cnndm.train.9066.bert.pt, number of examples: 1988
[2020-03-31 07:53:00,899 INFO] Step 78700/210000; acc:  56.69; ppl:  6.58; xent: 1.88; lr: 0.00000713;   0/481 tok/s; 130416 sec
[2020-03-31 07:54:23,527 INFO] Step 78750/210000; acc:  41.42; ppl: 14.50; xent: 2.67; lr: 0.00000713;   0/1290 tok/s; 130498 sec
[2020-03-31 07:55:34,685 INFO] Loading train dataset from ../bert_data/cnndm.train.9067.bert.pt, number of examples: 1986
[2020-03-31 07:55:45,872 INFO] Step 78800/210000; acc:  58.71; ppl:  5.89; xent: 1.77; lr: 0.00000712;   0/550 tok/s; 130581 sec
[2020-03-31 07:57:07,507 INFO] Step 78850/210000; acc:  42.85; ppl: 14.96; xent: 2.71; lr: 0.00000712;   0/1329 tok/s; 130662 sec
[2020-03-31 07:58:10,686 INFO] Loading train dataset from ../bert_data/cnndm.train.9068.bert.pt, number of examples: 1988
[2020-03-31 07:58:30,492 INFO] Step 78900/210000; acc:  51.73; ppl:  7.13; xent: 1.96; lr: 0.00000712;   0/668 tok/s; 130745 sec
[2020-03-31 07:59:52,637 INFO] Step 78950/210000; acc:  43.67; ppl: 12.68; xent: 2.54; lr: 0.00000712;   0/1160 tok/s; 130827 sec
[2020-03-31 08:00:47,273 INFO] Loading train dataset from ../bert_data/cnndm.train.9069.bert.pt, number of examples: 1982
[2020-03-31 08:01:15,206 INFO] Step 79000/210000; acc:  53.38; ppl:  7.19; xent: 1.97; lr: 0.00000712;   0/651 tok/s; 130910 sec
[2020-03-31 08:01:15,210 INFO] Saving checkpoint ../models/model_step_79000.pt
[2020-03-31 08:02:40,371 INFO] Step 79050/210000; acc:  49.84; ppl:  9.60; xent: 2.26; lr: 0.00000711;   0/724 tok/s; 130995 sec
[2020-03-31 08:03:25,253 INFO] Loading train dataset from ../bert_data/cnndm.train.9070.bert.pt, number of examples: 1988
[2020-03-31 08:04:03,019 INFO] Step 79100/210000; acc:  47.15; ppl: 10.37; xent: 2.34; lr: 0.00000711;   0/1078 tok/s; 131078 sec
[2020-03-31 08:05:25,139 INFO] Step 79150/210000; acc:  56.05; ppl:  6.38; xent: 1.85; lr: 0.00000711;   0/477 tok/s; 131160 sec
[2020-03-31 08:05:59,918 INFO] Loading train dataset from ../bert_data/cnndm.train.9071.bert.pt, number of examples: 1988
[2020-03-31 08:06:47,249 INFO] Step 79200/210000; acc:  45.34; ppl: 12.47; xent: 2.52; lr: 0.00000711;   0/1347 tok/s; 131242 sec
[2020-03-31 08:08:09,424 INFO] Step 79250/210000; acc:  58.66; ppl:  5.95; xent: 1.78; lr: 0.00000710;   0/469 tok/s; 131324 sec
[2020-03-31 08:08:38,078 INFO] Loading train dataset from ../bert_data/cnndm.train.9072.bert.pt, number of examples: 1992
[2020-03-31 08:09:32,069 INFO] Step 79300/210000; acc:  48.32; ppl: 10.97; xent: 2.40; lr: 0.00000710;   0/1232 tok/s; 131407 sec
[2020-03-31 08:10:54,400 INFO] Step 79350/210000; acc:  54.75; ppl:  7.33; xent: 1.99; lr: 0.00000710;   0/609 tok/s; 131489 sec
[2020-03-31 08:11:14,985 INFO] Loading train dataset from ../bert_data/cnndm.train.9073.bert.pt, number of examples: 1987
[2020-03-31 08:12:17,611 INFO] Step 79400/210000; acc:  42.22; ppl: 14.44; xent: 2.67; lr: 0.00000710;   0/1452 tok/s; 131572 sec
[2020-03-31 08:13:39,927 INFO] Step 79450/210000; acc:  60.16; ppl:  6.09; xent: 1.81; lr: 0.00000710;   0/543 tok/s; 131655 sec
[2020-03-31 08:13:52,132 INFO] Loading train dataset from ../bert_data/cnndm.train.9074.bert.pt, number of examples: 1991
[2020-03-31 08:15:03,611 INFO] Step 79500/210000; acc:  48.00; ppl: 10.32; xent: 2.33; lr: 0.00000709;   0/1128 tok/s; 131738 sec
[2020-03-31 08:15:03,615 INFO] Saving checkpoint ../models/model_step_79500.pt
[2020-03-31 08:16:27,470 INFO] Step 79550/210000; acc:  51.40; ppl:  8.09; xent: 2.09; lr: 0.00000709;   0/786 tok/s; 131822 sec
[2020-03-31 08:16:31,491 INFO] Loading train dataset from ../bert_data/cnndm.train.9075.bert.pt, number of examples: 1983
[2020-03-31 08:17:50,332 INFO] Step 79600/210000; acc:  48.13; ppl: 10.88; xent: 2.39; lr: 0.00000709;   0/812 tok/s; 131905 sec
[2020-03-31 08:19:05,220 INFO] Loading train dataset from ../bert_data/cnndm.train.9076.bert.pt, number of examples: 1986
[2020-03-31 08:19:11,782 INFO] Step 79650/210000; acc:  46.24; ppl: 12.06; xent: 2.49; lr: 0.00000709;   0/936 tok/s; 131987 sec
[2020-03-31 08:20:33,537 INFO] Step 79700/210000; acc:  50.42; ppl:  9.28; xent: 2.23; lr: 0.00000708;   0/567 tok/s; 132068 sec
[2020-03-31 08:21:41,373 INFO] Loading train dataset from ../bert_data/cnndm.train.9077.bert.pt, number of examples: 1990
[2020-03-31 08:21:56,299 INFO] Step 79750/210000; acc:  48.19; ppl:  9.44; xent: 2.25; lr: 0.00000708;   0/1110 tok/s; 132151 sec
[2020-03-31 08:23:19,069 INFO] Step 79800/210000; acc:  57.73; ppl:  5.32; xent: 1.67; lr: 0.00000708;   0/431 tok/s; 132234 sec
[2020-03-31 08:24:18,720 INFO] Loading train dataset from ../bert_data/cnndm.train.9078.bert.pt, number of examples: 1992
[2020-03-31 08:24:41,202 INFO] Step 79850/210000; acc:  49.86; ppl:  9.39; xent: 2.24; lr: 0.00000708;   0/1386 tok/s; 132316 sec
[2020-03-31 08:26:02,963 INFO] Step 79900/210000; acc:  54.85; ppl:  5.86; xent: 1.77; lr: 0.00000708;   0/432 tok/s; 132398 sec
[2020-03-31 08:26:55,964 INFO] Loading train dataset from ../bert_data/cnndm.train.9079.bert.pt, number of examples: 1984
[2020-03-31 08:27:25,535 INFO] Step 79950/210000; acc:  49.42; ppl: 10.27; xent: 2.33; lr: 0.00000707;   0/979 tok/s; 132480 sec
[2020-03-31 08:28:48,053 INFO] Step 80000/210000; acc:  61.51; ppl:  4.16; xent: 1.43; lr: 0.00000707;   0/372 tok/s; 132563 sec
[2020-03-31 08:28:48,056 INFO] Saving checkpoint ../models/model_step_80000.pt
[2020-03-31 08:29:32,880 INFO] Loading train dataset from ../bert_data/cnndm.train.9080.bert.pt, number of examples: 1988
[2020-03-31 08:30:12,845 INFO] Step 80050/210000; acc:  45.67; ppl: 11.91; xent: 2.48; lr: 0.00000707;   0/1353 tok/s; 132648 sec
[2020-03-31 08:31:34,628 INFO] Step 80100/210000; acc:  61.47; ppl:  4.96; xent: 1.60; lr: 0.00000707;   0/501 tok/s; 132729 sec
[2020-03-31 08:32:09,495 INFO] Loading train dataset from ../bert_data/cnndm.train.9081.bert.pt, number of examples: 1991
[2020-03-31 08:32:57,271 INFO] Step 80150/210000; acc:  44.43; ppl: 13.74; xent: 2.62; lr: 0.00000706;   0/1338 tok/s; 132812 sec
[2020-03-31 08:34:19,815 INFO] Step 80200/210000; acc:  57.27; ppl:  5.53; xent: 1.71; lr: 0.00000706;   0/590 tok/s; 132895 sec
[2020-03-31 08:34:46,171 INFO] Loading train dataset from ../bert_data/cnndm.train.9082.bert.pt, number of examples: 1985
[2020-03-31 08:35:42,258 INFO] Step 80250/210000; acc:  44.84; ppl: 14.15; xent: 2.65; lr: 0.00000706;   0/1363 tok/s; 132977 sec
[2020-03-31 08:37:04,067 INFO] Step 80300/210000; acc:  54.37; ppl:  7.17; xent: 1.97; lr: 0.00000706;   0/579 tok/s; 133059 sec
[2020-03-31 08:37:22,617 INFO] Loading train dataset from ../bert_data/cnndm.train.9083.bert.pt, number of examples: 1986
[2020-03-31 08:38:26,487 INFO] Step 80350/210000; acc:  44.49; ppl: 13.17; xent: 2.58; lr: 0.00000706;   0/1041 tok/s; 133141 sec
[2020-03-31 08:39:48,909 INFO] Step 80400/210000; acc:  53.34; ppl:  7.50; xent: 2.01; lr: 0.00000705;   0/738 tok/s; 133224 sec
[2020-03-31 08:39:59,373 INFO] Loading train dataset from ../bert_data/cnndm.train.9084.bert.pt, number of examples: 1982
[2020-03-31 08:41:11,994 INFO] Step 80450/210000; acc:  45.98; ppl: 11.52; xent: 2.44; lr: 0.00000705;   0/782 tok/s; 133307 sec
[2020-03-31 08:42:33,964 INFO] Step 80500/210000; acc:  47.21; ppl: 11.01; xent: 2.40; lr: 0.00000705;   0/934 tok/s; 133389 sec
[2020-03-31 08:42:33,968 INFO] Saving checkpoint ../models/model_step_80500.pt
[2020-03-31 08:42:38,056 INFO] Loading train dataset from ../bert_data/cnndm.train.9085.bert.pt, number of examples: 1985
[2020-03-31 08:43:58,720 INFO] Step 80550/210000; acc:  54.55; ppl:  7.16; xent: 1.97; lr: 0.00000705;   0/429 tok/s; 133474 sec
[2020-03-31 08:45:15,134 INFO] Loading train dataset from ../bert_data/cnndm.train.9086.bert.pt, number of examples: 1983
[2020-03-31 08:45:21,873 INFO] Step 80600/210000; acc:  51.32; ppl:  8.40; xent: 2.13; lr: 0.00000704;   0/1062 tok/s; 133557 sec
[2020-03-31 08:46:44,825 INFO] Step 80650/210000; acc:  61.51; ppl:  5.06; xent: 1.62; lr: 0.00000704;   0/429 tok/s; 133640 sec
[2020-03-31 08:47:53,008 INFO] Loading train dataset from ../bert_data/cnndm.train.9087.bert.pt, number of examples: 1986
[2020-03-31 08:48:07,918 INFO] Step 80700/210000; acc:  42.45; ppl: 15.24; xent: 2.72; lr: 0.00000704;   0/1420 tok/s; 133723 sec
[2020-03-31 08:49:29,962 INFO] Step 80750/210000; acc:  53.83; ppl:  7.88; xent: 2.06; lr: 0.00000704;   0/524 tok/s; 133805 sec
[2020-03-31 08:50:29,439 INFO] Loading train dataset from ../bert_data/cnndm.train.9088.bert.pt, number of examples: 1986
[2020-03-31 08:50:52,565 INFO] Step 80800/210000; acc:  48.47; ppl: 10.48; xent: 2.35; lr: 0.00000704;   0/1363 tok/s; 133887 sec
[2020-03-31 08:52:14,921 INFO] Step 80850/210000; acc:  53.25; ppl:  7.72; xent: 2.04; lr: 0.00000703;   0/610 tok/s; 133970 sec
[2020-03-31 08:53:04,850 INFO] Loading train dataset from ../bert_data/cnndm.train.9089.bert.pt, number of examples: 1986
[2020-03-31 08:53:37,694 INFO] Step 80900/210000; acc:  44.19; ppl: 12.88; xent: 2.56; lr: 0.00000703;   0/1393 tok/s; 134053 sec
[2020-03-31 08:54:59,559 INFO] Step 80950/210000; acc:  55.04; ppl:  7.47; xent: 2.01; lr: 0.00000703;   0/612 tok/s; 134134 sec
[2020-03-31 08:55:40,993 INFO] Loading train dataset from ../bert_data/cnndm.train.9090.bert.pt, number of examples: 20
[2020-03-31 08:55:42,883 INFO] Loading train dataset from ../bert_data/cnndm.train.9100.bert.pt, number of examples: 1988
[2020-03-31 08:56:17,886 INFO] Step 81000/210000; acc:  47.07; ppl: 10.22; xent: 2.32; lr: 0.00000703;   0/585 tok/s; 134213 sec
[2020-03-31 08:56:17,890 INFO] Saving checkpoint ../models/model_step_81000.pt
[2020-03-31 08:57:32,384 INFO] Step 81050/210000; acc:  59.97; ppl:  5.19; xent: 1.65; lr: 0.00000703;   0/428 tok/s; 134287 sec
[2020-03-31 08:58:03,177 INFO] Loading train dataset from ../bert_data/cnndm.train.9101.bert.pt, number of examples: 1983
[2020-03-31 08:58:45,438 INFO] Step 81100/210000; acc:  60.30; ppl:  5.38; xent: 1.68; lr: 0.00000702;   0/414 tok/s; 134360 sec
[2020-03-31 08:59:56,770 INFO] Step 81150/210000; acc:  58.54; ppl:  5.78; xent: 1.75; lr: 0.00000702;   0/657 tok/s; 134432 sec
[2020-03-31 09:00:20,485 INFO] Loading train dataset from ../bert_data/cnndm.train.9102.bert.pt, number of examples: 1992
[2020-03-31 09:01:10,094 INFO] Step 81200/210000; acc:  58.90; ppl:  6.04; xent: 1.80; lr: 0.00000702;   0/565 tok/s; 134505 sec
[2020-03-31 09:02:23,523 INFO] Step 81250/210000; acc:  63.66; ppl:  5.02; xent: 1.61; lr: 0.00000702;   0/466 tok/s; 134578 sec
[2020-03-31 09:02:37,913 INFO] Loading train dataset from ../bert_data/cnndm.train.9103.bert.pt, number of examples: 1978
[2020-03-31 09:03:36,278 INFO] Step 81300/210000; acc:  60.31; ppl:  5.63; xent: 1.73; lr: 0.00000701;   0/452 tok/s; 134651 sec
[2020-03-31 09:04:49,510 INFO] Step 81350/210000; acc:  60.94; ppl:  4.62; xent: 1.53; lr: 0.00000701;   0/386 tok/s; 134724 sec
[2020-03-31 09:04:57,207 INFO] Loading train dataset from ../bert_data/cnndm.train.9104.bert.pt, number of examples: 1986
[2020-03-31 09:06:02,379 INFO] Step 81400/210000; acc:  57.06; ppl:  5.59; xent: 1.72; lr: 0.00000701;   0/744 tok/s; 134797 sec
[2020-03-31 09:07:13,709 INFO] Loading train dataset from ../bert_data/cnndm.train.9105.bert.pt, number of examples: 1988
[2020-03-31 09:07:15,447 INFO] Step 81450/210000; acc:  67.00; ppl:  4.09; xent: 1.41; lr: 0.00000701;   0/472 tok/s; 134870 sec
[2020-03-31 09:08:26,858 INFO] Step 81500/210000; acc:  58.67; ppl:  5.79; xent: 1.76; lr: 0.00000701;   0/634 tok/s; 134942 sec
[2020-03-31 09:08:26,861 INFO] Saving checkpoint ../models/model_step_81500.pt
[2020-03-31 09:09:33,501 INFO] Loading train dataset from ../bert_data/cnndm.train.9106.bert.pt, number of examples: 1978
[2020-03-31 09:09:42,427 INFO] Step 81550/210000; acc:  58.60; ppl:  5.82; xent: 1.76; lr: 0.00000700;   0/529 tok/s; 135017 sec
[2020-03-31 09:10:55,755 INFO] Step 81600/210000; acc:  64.04; ppl:  4.29; xent: 1.46; lr: 0.00000700;   0/370 tok/s; 135091 sec
[2020-03-31 09:11:51,022 INFO] Loading train dataset from ../bert_data/cnndm.train.9107.bert.pt, number of examples: 1985
[2020-03-31 09:12:09,091 INFO] Step 81650/210000; acc:  65.40; ppl:  4.31; xent: 1.46; lr: 0.00000700;   0/457 tok/s; 135164 sec
[2020-03-31 09:13:21,015 INFO] Step 81700/210000; acc:  57.50; ppl:  6.52; xent: 1.88; lr: 0.00000700;   0/668 tok/s; 135236 sec
[2020-03-31 09:14:10,390 INFO] Loading train dataset from ../bert_data/cnndm.train.9108.bert.pt, number of examples: 1989
[2020-03-31 09:14:33,099 INFO] Step 81750/210000; acc:  55.91; ppl:  6.07; xent: 1.80; lr: 0.00000699;   0/614 tok/s; 135308 sec
[2020-03-31 09:15:46,204 INFO] Step 81800/210000; acc:  67.25; ppl:  4.13; xent: 1.42; lr: 0.00000699;   0/471 tok/s; 135381 sec
[2020-03-31 09:16:26,409 INFO] Loading train dataset from ../bert_data/cnndm.train.9109.bert.pt, number of examples: 1987
[2020-03-31 09:16:58,332 INFO] Step 81850/210000; acc:  64.40; ppl:  4.49; xent: 1.50; lr: 0.00000699;   0/498 tok/s; 135453 sec
[2020-03-31 09:18:12,389 INFO] Step 81900/210000; acc:  68.38; ppl:  3.52; xent: 1.26; lr: 0.00000699;   0/373 tok/s; 135527 sec
[2020-03-31 09:18:45,988 INFO] Loading train dataset from ../bert_data/cnndm.train.9110.bert.pt, number of examples: 1985
[2020-03-31 09:19:25,165 INFO] Step 81950/210000; acc:  71.31; ppl:  3.38; xent: 1.22; lr: 0.00000699;   0/397 tok/s; 135600 sec
[2020-03-31 09:20:37,867 INFO] Step 82000/210000; acc:  60.62; ppl:  5.81; xent: 1.76; lr: 0.00000698;   0/592 tok/s; 135673 sec
[2020-03-31 09:20:37,871 INFO] Saving checkpoint ../models/model_step_82000.pt
[2020-03-31 09:21:06,535 INFO] Loading train dataset from ../bert_data/cnndm.train.9111.bert.pt, number of examples: 1985
[2020-03-31 09:21:52,689 INFO] Step 82050/210000; acc:  60.11; ppl:  5.56; xent: 1.72; lr: 0.00000698;   0/565 tok/s; 135748 sec
[2020-03-31 09:23:05,993 INFO] Step 82100/210000; acc:  65.27; ppl:  4.14; xent: 1.42; lr: 0.00000698;   0/454 tok/s; 135821 sec
[2020-03-31 09:23:23,490 INFO] Loading train dataset from ../bert_data/cnndm.train.9112.bert.pt, number of examples: 1983
[2020-03-31 09:24:19,282 INFO] Step 82150/210000; acc:  63.30; ppl:  4.55; xent: 1.52; lr: 0.00000698;   0/379 tok/s; 135894 sec
[2020-03-31 09:25:31,791 INFO] Step 82200/210000; acc:  59.09; ppl:  5.64; xent: 1.73; lr: 0.00000698;   0/529 tok/s; 135967 sec
[2020-03-31 09:25:42,238 INFO] Loading train dataset from ../bert_data/cnndm.train.9113.bert.pt, number of examples: 1979
[2020-03-31 09:26:44,285 INFO] Step 82250/210000; acc:  61.86; ppl:  5.12; xent: 1.63; lr: 0.00000697;   0/619 tok/s; 136039 sec
[2020-03-31 09:27:57,399 INFO] Step 82300/210000; acc:  60.84; ppl:  5.56; xent: 1.71; lr: 0.00000697;   0/542 tok/s; 136112 sec
[2020-03-31 09:27:58,977 INFO] Loading train dataset from ../bert_data/cnndm.train.9114.bert.pt, number of examples: 1989
[2020-03-31 09:29:10,339 INFO] Step 82350/210000; acc:  67.96; ppl:  3.90; xent: 1.36; lr: 0.00000697;   0/539 tok/s; 136185 sec
[2020-03-31 09:30:17,320 INFO] Loading train dataset from ../bert_data/cnndm.train.9115.bert.pt, number of examples: 1989
[2020-03-31 09:30:23,128 INFO] Step 82400/210000; acc:  69.34; ppl:  3.70; xent: 1.31; lr: 0.00000697;   0/496 tok/s; 136258 sec
[2020-03-31 09:31:35,363 INFO] Step 82450/210000; acc:  57.94; ppl:  6.59; xent: 1.89; lr: 0.00000697;   0/652 tok/s; 136330 sec
[2020-03-31 09:32:34,976 INFO] Loading train dataset from ../bert_data/cnndm.train.9116.bert.pt, number of examples: 1985
[2020-03-31 09:32:48,335 INFO] Step 82500/210000; acc:  56.10; ppl:  6.38; xent: 1.85; lr: 0.00000696;   0/640 tok/s; 136403 sec
[2020-03-31 09:32:48,338 INFO] Saving checkpoint ../models/model_step_82500.pt
[2020-03-31 09:34:03,936 INFO] Step 82550/210000; acc:  59.66; ppl:  5.22; xent: 1.65; lr: 0.00000696;   0/635 tok/s; 136479 sec
[2020-03-31 09:34:55,385 INFO] Loading train dataset from ../bert_data/cnndm.train.9117.bert.pt, number of examples: 1979
[2020-03-31 09:35:16,984 INFO] Step 82600/210000; acc:  65.08; ppl:  4.28; xent: 1.45; lr: 0.00000696;   0/544 tok/s; 136552 sec
[2020-03-31 09:36:29,256 INFO] Step 82650/210000; acc:  65.06; ppl:  4.10; xent: 1.41; lr: 0.00000696;   0/521 tok/s; 136624 sec
[2020-03-31 09:37:12,509 INFO] Loading train dataset from ../bert_data/cnndm.train.9118.bert.pt, number of examples: 1980
[2020-03-31 09:37:42,322 INFO] Step 82700/210000; acc:  66.29; ppl:  3.95; xent: 1.37; lr: 0.00000695;   0/378 tok/s; 136697 sec
[2020-03-31 09:38:54,926 INFO] Step 82750/210000; acc:  58.92; ppl:  5.55; xent: 1.71; lr: 0.00000695;   0/646 tok/s; 136770 sec
[2020-03-31 09:39:31,608 INFO] Loading train dataset from ../bert_data/cnndm.train.9119.bert.pt, number of examples: 1985
[2020-03-31 09:40:07,302 INFO] Step 82800/210000; acc:  64.82; ppl:  4.74; xent: 1.56; lr: 0.00000695;   0/551 tok/s; 136842 sec
[2020-03-31 09:41:20,463 INFO] Step 82850/210000; acc:  67.66; ppl:  3.93; xent: 1.37; lr: 0.00000695;   0/413 tok/s; 136915 sec
[2020-03-31 09:41:49,901 INFO] Loading train dataset from ../bert_data/cnndm.train.9120.bert.pt, number of examples: 1981
[2020-03-31 09:42:33,542 INFO] Step 82900/210000; acc:  68.85; ppl:  3.56; xent: 1.27; lr: 0.00000695;   0/438 tok/s; 136988 sec
[2020-03-31 09:43:45,942 INFO] Step 82950/210000; acc:  62.78; ppl:  4.98; xent: 1.61; lr: 0.00000694;   0/674 tok/s; 137061 sec
[2020-03-31 09:44:06,158 INFO] Loading train dataset from ../bert_data/cnndm.train.9121.bert.pt, number of examples: 1985
[2020-03-31 09:44:58,569 INFO] Step 83000/210000; acc:  59.87; ppl:  5.28; xent: 1.66; lr: 0.00000694;   0/759 tok/s; 137133 sec
[2020-03-31 09:44:58,572 INFO] Saving checkpoint ../models/model_step_83000.pt
[2020-03-31 09:46:14,155 INFO] Step 83050/210000; acc:  66.50; ppl:  4.43; xent: 1.49; lr: 0.00000694;   0/569 tok/s; 137209 sec
[2020-03-31 09:46:27,621 INFO] Loading train dataset from ../bert_data/cnndm.train.9122.bert.pt, number of examples: 1986
[2020-03-31 09:47:27,590 INFO] Step 83100/210000; acc:  62.13; ppl:  4.77; xent: 1.56; lr: 0.00000694;   0/527 tok/s; 137282 sec
[2020-03-31 09:48:40,154 INFO] Step 83150/210000; acc:  54.59; ppl:  6.68; xent: 1.90; lr: 0.00000694;   0/778 tok/s; 137355 sec
[2020-03-31 09:48:46,679 INFO] Loading train dataset from ../bert_data/cnndm.train.9123.bert.pt, number of examples: 1983
[2020-03-31 09:49:54,071 INFO] Step 83200/210000; acc:  70.18; ppl:  3.33; xent: 1.20; lr: 0.00000693;   0/451 tok/s; 137429 sec
[2020-03-31 09:51:04,276 INFO] Loading train dataset from ../bert_data/cnndm.train.9124.bert.pt, number of examples: 1983
[2020-03-31 09:51:07,442 INFO] Step 83250/210000; acc:  68.37; ppl:  3.51; xent: 1.26; lr: 0.00000693;   0/405 tok/s; 137502 sec
[2020-03-31 09:52:20,182 INFO] Step 83300/210000; acc:  61.49; ppl:  4.71; xent: 1.55; lr: 0.00000693;   0/593 tok/s; 137575 sec
[2020-03-31 09:53:22,119 INFO] Loading train dataset from ../bert_data/cnndm.train.9125.bert.pt, number of examples: 1991
[2020-03-31 09:53:32,203 INFO] Step 83350/210000; acc:  62.34; ppl:  4.68; xent: 1.54; lr: 0.00000693;   0/595 tok/s; 137647 sec
[2020-03-31 09:54:44,850 INFO] Step 83400/210000; acc:  62.28; ppl:  5.25; xent: 1.66; lr: 0.00000693;   0/521 tok/s; 137720 sec
[2020-03-31 09:55:38,141 INFO] Loading train dataset from ../bert_data/cnndm.train.9126.bert.pt, number of examples: 1985
[2020-03-31 09:55:57,245 INFO] Step 83450/210000; acc:  62.65; ppl:  4.47; xent: 1.50; lr: 0.00000692;   0/465 tok/s; 137792 sec
[2020-03-31 09:57:09,414 INFO] Step 83500/210000; acc:  61.01; ppl:  5.22; xent: 1.65; lr: 0.00000692;   0/751 tok/s; 137864 sec
[2020-03-31 09:57:09,417 INFO] Saving checkpoint ../models/model_step_83500.pt
[2020-03-31 09:57:58,190 INFO] Loading train dataset from ../bert_data/cnndm.train.9127.bert.pt, number of examples: 1980
[2020-03-31 09:58:24,873 INFO] Step 83550/210000; acc:  60.92; ppl:  5.43; xent: 1.69; lr: 0.00000692;   0/695 tok/s; 137940 sec
[2020-03-31 09:59:37,296 INFO] Step 83600/210000; acc:  63.50; ppl:  4.49; xent: 1.50; lr: 0.00000692;   0/672 tok/s; 138012 sec
[2020-03-31 10:00:15,523 INFO] Loading train dataset from ../bert_data/cnndm.train.9128.bert.pt, number of examples: 1981
[2020-03-31 10:00:50,901 INFO] Step 83650/210000; acc:  61.88; ppl:  4.72; xent: 1.55; lr: 0.00000692;   0/609 tok/s; 138086 sec
[2020-03-31 10:02:03,078 INFO] Step 83700/210000; acc:  70.96; ppl:  3.63; xent: 1.29; lr: 0.00000691;   0/505 tok/s; 138158 sec
[2020-03-31 10:02:33,776 INFO] Loading train dataset from ../bert_data/cnndm.train.9129.bert.pt, number of examples: 1987
[2020-03-31 10:03:15,971 INFO] Step 83750/210000; acc:  67.40; ppl:  3.67; xent: 1.30; lr: 0.00000691;   0/417 tok/s; 138231 sec
[2020-03-31 10:04:28,585 INFO] Step 83800/210000; acc:  63.72; ppl:  4.21; xent: 1.44; lr: 0.00000691;   0/557 tok/s; 138303 sec
[2020-03-31 10:04:52,483 INFO] Loading train dataset from ../bert_data/cnndm.train.9130.bert.pt, number of examples: 1986
[2020-03-31 10:05:42,159 INFO] Step 83850/210000; acc:  59.82; ppl:  5.37; xent: 1.68; lr: 0.00000691;   0/689 tok/s; 138377 sec
[2020-03-31 10:06:54,578 INFO] Step 83900/210000; acc:  62.61; ppl:  4.96; xent: 1.60; lr: 0.00000690;   0/600 tok/s; 138449 sec
[2020-03-31 10:07:09,338 INFO] Loading train dataset from ../bert_data/cnndm.train.9131.bert.pt, number of examples: 1984
[2020-03-31 10:08:07,578 INFO] Step 83950/210000; acc:  58.11; ppl:  6.07; xent: 1.80; lr: 0.00000690;   0/488 tok/s; 138522 sec
[2020-03-31 10:09:19,653 INFO] Step 84000/210000; acc:  62.39; ppl:  5.06; xent: 1.62; lr: 0.00000690;   0/499 tok/s; 138594 sec
[2020-03-31 10:09:19,656 INFO] Saving checkpoint ../models/model_step_84000.pt
[2020-03-31 10:09:29,093 INFO] Loading train dataset from ../bert_data/cnndm.train.9132.bert.pt, number of examples: 1990
[2020-03-31 10:10:34,458 INFO] Step 84050/210000; acc:  62.86; ppl:  4.74; xent: 1.56; lr: 0.00000690;   0/614 tok/s; 138669 sec
[2020-03-31 10:11:47,275 INFO] Step 84100/210000; acc:  59.24; ppl:  6.39; xent: 1.85; lr: 0.00000690;   0/660 tok/s; 138742 sec
[2020-03-31 10:11:47,479 INFO] Loading train dataset from ../bert_data/cnndm.train.9133.bert.pt, number of examples: 1978
[2020-03-31 10:12:59,632 INFO] Step 84150/210000; acc:  63.44; ppl:  4.93; xent: 1.59; lr: 0.00000689;   0/574 tok/s; 138814 sec
[2020-03-31 10:14:03,815 INFO] Loading train dataset from ../bert_data/cnndm.train.9134.bert.pt, number of examples: 156
[2020-03-31 10:14:12,521 INFO] Step 84200/210000; acc:  61.25; ppl:  5.33; xent: 1.67; lr: 0.00000689;   0/591 tok/s; 138887 sec
[2020-03-31 10:14:15,942 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-31 10:15:36,502 INFO] Step 84250/210000; acc:  52.59; ppl: 11.36; xent: 2.43; lr: 0.00000689;   0/637 tok/s; 138971 sec
[2020-03-31 10:17:01,277 INFO] Step 84300/210000; acc:  46.63; ppl: 15.36; xent: 2.73; lr: 0.00000689;   0/692 tok/s; 139056 sec
[2020-03-31 10:17:03,661 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-31 10:18:25,972 INFO] Step 84350/210000; acc:  53.45; ppl: 12.06; xent: 2.49; lr: 0.00000689;   0/803 tok/s; 139141 sec
[2020-03-31 10:19:49,474 INFO] Step 84400/210000; acc:  55.81; ppl: 10.14; xent: 2.32; lr: 0.00000688;   0/889 tok/s; 139224 sec
[2020-03-31 10:19:49,827 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-31 10:21:13,935 INFO] Step 84450/210000; acc:  55.04; ppl:  9.64; xent: 2.27; lr: 0.00000688;   0/897 tok/s; 139309 sec
[2020-03-31 10:22:38,254 INFO] Step 84500/210000; acc:  51.25; ppl: 11.36; xent: 2.43; lr: 0.00000688;   0/822 tok/s; 139393 sec
[2020-03-31 10:22:38,276 INFO] Saving checkpoint ../models/model_step_84500.pt
[2020-03-31 10:22:40,761 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-31 10:24:04,767 INFO] Step 84550/210000; acc:  50.54; ppl: 13.10; xent: 2.57; lr: 0.00000688;   0/777 tok/s; 139480 sec
[2020-03-31 10:25:27,830 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-31 10:25:29,672 INFO] Step 84600/210000; acc:  46.16; ppl: 15.52; xent: 2.74; lr: 0.00000688;   0/502 tok/s; 139564 sec
[2020-03-31 10:26:53,658 INFO] Step 84650/210000; acc:  44.38; ppl: 15.83; xent: 2.76; lr: 0.00000687;   0/1059 tok/s; 139648 sec
[2020-03-31 10:28:15,363 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-31 10:28:18,768 INFO] Step 84700/210000; acc:  45.36; ppl: 15.11; xent: 2.72; lr: 0.00000687;   0/503 tok/s; 139734 sec
[2020-03-31 10:29:43,312 INFO] Step 84750/210000; acc:  50.94; ppl: 11.97; xent: 2.48; lr: 0.00000687;   0/648 tok/s; 139818 sec
[2020-03-31 10:31:02,774 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-31 10:31:07,793 INFO] Step 84800/210000; acc:  48.94; ppl: 13.09; xent: 2.57; lr: 0.00000687;   0/697 tok/s; 139903 sec
[2020-03-31 10:32:31,876 INFO] Step 84850/210000; acc:  50.78; ppl: 10.85; xent: 2.38; lr: 0.00000687;   0/470 tok/s; 139987 sec
[2020-03-31 10:33:51,583 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-31 10:33:56,635 INFO] Step 84900/210000; acc:  52.26; ppl: 10.52; xent: 2.35; lr: 0.00000686;   0/736 tok/s; 140071 sec
[2020-03-31 10:35:20,986 INFO] Step 84950/210000; acc:  51.24; ppl: 10.98; xent: 2.40; lr: 0.00000686;   0/689 tok/s; 140156 sec
[2020-03-31 10:36:38,785 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-31 10:36:45,419 INFO] Step 85000/210000; acc:  45.93; ppl: 14.22; xent: 2.65; lr: 0.00000686;   0/751 tok/s; 140240 sec
[2020-03-31 10:36:45,422 INFO] Saving checkpoint ../models/model_step_85000.pt
[2020-03-31 10:38:11,992 INFO] Step 85050/210000; acc:  52.71; ppl:  9.78; xent: 2.28; lr: 0.00000686;   0/808 tok/s; 140327 sec
[2020-03-31 10:39:29,578 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-31 10:39:36,582 INFO] Step 85100/210000; acc:  54.86; ppl:  9.50; xent: 2.25; lr: 0.00000686;   0/905 tok/s; 140411 sec
[2020-03-31 10:41:00,883 INFO] Step 85150/210000; acc:  56.68; ppl:  7.87; xent: 2.06; lr: 0.00000685;   0/866 tok/s; 140496 sec
[2020-03-31 10:42:17,324 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-31 10:42:25,742 INFO] Step 85200/210000; acc:  51.24; ppl: 10.82; xent: 2.38; lr: 0.00000685;   0/931 tok/s; 140581 sec
[2020-03-31 10:43:49,705 INFO] Step 85250/210000; acc:  46.47; ppl: 13.85; xent: 2.63; lr: 0.00000685;   0/1015 tok/s; 140665 sec
[2020-03-31 10:45:04,039 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-31 10:45:14,311 INFO] Step 85300/210000; acc:  56.52; ppl:  9.24; xent: 2.22; lr: 0.00000685;   0/745 tok/s; 140749 sec
[2020-03-31 10:46:38,489 INFO] Step 85350/210000; acc:  50.08; ppl: 12.90; xent: 2.56; lr: 0.00000685;   0/757 tok/s; 140833 sec
[2020-03-31 10:47:51,561 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-31 10:48:03,414 INFO] Step 85400/210000; acc:  52.57; ppl:  9.36; xent: 2.24; lr: 0.00000684;   0/481 tok/s; 140918 sec
[2020-03-31 10:49:27,391 INFO] Step 85450/210000; acc:  54.04; ppl:  8.87; xent: 2.18; lr: 0.00000684;   0/1042 tok/s; 141002 sec
[2020-03-31 10:50:40,432 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-31 10:50:52,402 INFO] Step 85500/210000; acc:  53.04; ppl: 10.47; xent: 2.35; lr: 0.00000684;   0/632 tok/s; 141087 sec
[2020-03-31 10:50:52,406 INFO] Saving checkpoint ../models/model_step_85500.pt
[2020-03-31 10:52:18,607 INFO] Step 85550/210000; acc:  52.92; ppl:  9.94; xent: 2.30; lr: 0.00000684;   0/635 tok/s; 141173 sec
[2020-03-31 10:53:29,825 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-31 10:53:43,176 INFO] Step 85600/210000; acc:  52.07; ppl: 10.85; xent: 2.38; lr: 0.00000684;   0/693 tok/s; 141258 sec
[2020-03-31 10:55:07,651 INFO] Step 85650/210000; acc:  52.96; ppl:  9.29; xent: 2.23; lr: 0.00000683;   0/602 tok/s; 141342 sec
[2020-03-31 10:56:17,336 INFO] Loading train dataset from ../bert_data/cnndm.train.103.bert.pt, number of examples: 2000
[2020-03-31 10:56:32,530 INFO] Step 85700/210000; acc:  54.04; ppl:  9.44; xent: 2.24; lr: 0.00000683;   0/813 tok/s; 141427 sec
[2020-03-31 10:57:56,635 INFO] Step 85750/210000; acc:  49.92; ppl: 11.02; xent: 2.40; lr: 0.00000683;   0/737 tok/s; 141511 sec
[2020-03-31 10:59:04,011 INFO] Loading train dataset from ../bert_data/cnndm.train.104.bert.pt, number of examples: 2000
[2020-03-31 10:59:20,853 INFO] Step 85800/210000; acc:  49.89; ppl: 12.55; xent: 2.53; lr: 0.00000683;   0/1040 tok/s; 141596 sec
[2020-03-31 11:00:45,257 INFO] Step 85850/210000; acc:  49.03; ppl: 12.36; xent: 2.51; lr: 0.00000683;   0/901 tok/s; 141680 sec
[2020-03-31 11:01:53,741 INFO] Loading train dataset from ../bert_data/cnndm.train.105.bert.pt, number of examples: 2001
[2020-03-31 11:02:10,472 INFO] Step 85900/210000; acc:  49.75; ppl: 10.75; xent: 2.37; lr: 0.00000682;   0/902 tok/s; 141765 sec
[2020-03-31 11:03:34,298 INFO] Step 85950/210000; acc:  50.04; ppl: 13.32; xent: 2.59; lr: 0.00000682;   0/784 tok/s; 141849 sec
[2020-03-31 11:04:40,769 INFO] Loading train dataset from ../bert_data/cnndm.train.106.bert.pt, number of examples: 1999
[2020-03-31 11:04:59,428 INFO] Step 86000/210000; acc:  54.82; ppl:  9.69; xent: 2.27; lr: 0.00000682;   0/503 tok/s; 141934 sec
[2020-03-31 11:04:59,431 INFO] Saving checkpoint ../models/model_step_86000.pt
[2020-03-31 11:06:25,937 INFO] Step 86050/210000; acc:  51.75; ppl: 11.20; xent: 2.42; lr: 0.00000682;   0/1038 tok/s; 142021 sec
[2020-03-31 11:07:30,437 INFO] Loading train dataset from ../bert_data/cnndm.train.107.bert.pt, number of examples: 1999
[2020-03-31 11:07:50,693 INFO] Step 86100/210000; acc:  54.19; ppl:  9.37; xent: 2.24; lr: 0.00000682;   0/629 tok/s; 142106 sec
[2020-03-31 11:09:15,289 INFO] Step 86150/210000; acc:  49.83; ppl: 10.90; xent: 2.39; lr: 0.00000681;   0/540 tok/s; 142190 sec
[2020-03-31 11:10:18,678 INFO] Loading train dataset from ../bert_data/cnndm.train.108.bert.pt, number of examples: 2000
[2020-03-31 11:10:40,418 INFO] Step 86200/210000; acc:  48.11; ppl: 12.71; xent: 2.54; lr: 0.00000681;   0/538 tok/s; 142275 sec
[2020-03-31 11:12:04,363 INFO] Step 86250/210000; acc:  45.81; ppl: 12.84; xent: 2.55; lr: 0.00000681;   0/769 tok/s; 142359 sec
[2020-03-31 11:13:05,127 INFO] Loading train dataset from ../bert_data/cnndm.train.109.bert.pt, number of examples: 2000
[2020-03-31 11:13:28,516 INFO] Step 86300/210000; acc:  49.73; ppl: 11.74; xent: 2.46; lr: 0.00000681;   0/777 tok/s; 142443 sec
[2020-03-31 11:14:52,491 INFO] Step 86350/210000; acc:  54.29; ppl:  8.52; xent: 2.14; lr: 0.00000681;   0/647 tok/s; 142527 sec
[2020-03-31 11:15:54,284 INFO] Loading train dataset from ../bert_data/cnndm.train.11.bert.pt, number of examples: 1999
[2020-03-31 11:16:17,774 INFO] Step 86400/210000; acc:  52.08; ppl: 10.40; xent: 2.34; lr: 0.00000680;   0/1039 tok/s; 142613 sec
[2020-03-31 11:17:42,214 INFO] Step 86450/210000; acc:  46.25; ppl: 14.09; xent: 2.65; lr: 0.00000680;   0/1041 tok/s; 142697 sec
[2020-03-31 11:18:42,129 INFO] Loading train dataset from ../bert_data/cnndm.train.110.bert.pt, number of examples: 2000
[2020-03-31 11:19:07,171 INFO] Step 86500/210000; acc:  49.05; ppl: 12.68; xent: 2.54; lr: 0.00000680;   0/965 tok/s; 142782 sec
[2020-03-31 11:19:07,174 INFO] Saving checkpoint ../models/model_step_86500.pt
[2020-03-31 11:20:33,958 INFO] Step 86550/210000; acc:  51.00; ppl:  9.77; xent: 2.28; lr: 0.00000680;   0/1010 tok/s; 142869 sec
[2020-03-31 11:21:32,000 INFO] Loading train dataset from ../bert_data/cnndm.train.111.bert.pt, number of examples: 2000
[2020-03-31 11:21:58,680 INFO] Step 86600/210000; acc:  57.09; ppl:  7.45; xent: 2.01; lr: 0.00000680;   0/504 tok/s; 142954 sec
[2020-03-31 11:23:22,788 INFO] Step 86650/210000; acc:  46.03; ppl: 16.03; xent: 2.77; lr: 0.00000679;   0/676 tok/s; 143038 sec
[2020-03-31 11:24:19,125 INFO] Loading train dataset from ../bert_data/cnndm.train.112.bert.pt, number of examples: 1999
[2020-03-31 11:24:47,252 INFO] Step 86700/210000; acc:  55.90; ppl:  9.04; xent: 2.20; lr: 0.00000679;   0/578 tok/s; 143122 sec
[2020-03-31 11:26:11,304 INFO] Step 86750/210000; acc:  53.83; ppl:  9.16; xent: 2.21; lr: 0.00000679;   0/660 tok/s; 143206 sec
[2020-03-31 11:27:06,089 INFO] Loading train dataset from ../bert_data/cnndm.train.113.bert.pt, number of examples: 1999
[2020-03-31 11:27:36,766 INFO] Step 86800/210000; acc:  52.71; ppl: 11.28; xent: 2.42; lr: 0.00000679;   0/582 tok/s; 143292 sec
[2020-03-31 11:29:00,879 INFO] Step 86850/210000; acc:  52.07; ppl: 10.42; xent: 2.34; lr: 0.00000679;   0/639 tok/s; 143376 sec
[2020-03-31 11:29:53,365 INFO] Loading train dataset from ../bert_data/cnndm.train.114.bert.pt, number of examples: 1998
[2020-03-31 11:30:25,240 INFO] Step 86900/210000; acc:  54.02; ppl:  8.91; xent: 2.19; lr: 0.00000678;   0/805 tok/s; 143460 sec
[2020-03-31 11:31:49,150 INFO] Step 86950/210000; acc:  57.32; ppl:  7.16; xent: 1.97; lr: 0.00000678;   0/595 tok/s; 143544 sec
[2020-03-31 11:32:40,034 INFO] Loading train dataset from ../bert_data/cnndm.train.115.bert.pt, number of examples: 2000
[2020-03-31 11:33:13,693 INFO] Step 87000/210000; acc:  57.07; ppl:  7.83; xent: 2.06; lr: 0.00000678;   0/886 tok/s; 143629 sec
[2020-03-31 11:33:13,696 INFO] Saving checkpoint ../models/model_step_87000.pt
[2020-03-31 11:34:40,358 INFO] Step 87050/210000; acc:  50.86; ppl: 11.09; xent: 2.41; lr: 0.00000678;   0/916 tok/s; 143715 sec
[2020-03-31 11:35:29,301 INFO] Loading train dataset from ../bert_data/cnndm.train.116.bert.pt, number of examples: 2000
[2020-03-31 11:36:04,715 INFO] Step 87100/210000; acc:  50.95; ppl: 10.95; xent: 2.39; lr: 0.00000678;   0/915 tok/s; 143800 sec
[2020-03-31 11:37:28,680 INFO] Step 87150/210000; acc:  60.36; ppl:  6.35; xent: 1.85; lr: 0.00000677;   0/818 tok/s; 143884 sec
[2020-03-31 11:38:18,194 INFO] Loading train dataset from ../bert_data/cnndm.train.117.bert.pt, number of examples: 2000
[2020-03-31 11:38:53,504 INFO] Step 87200/210000; acc:  47.21; ppl: 12.76; xent: 2.55; lr: 0.00000677;   0/527 tok/s; 143968 sec
[2020-03-31 11:40:17,728 INFO] Step 87250/210000; acc:  46.48; ppl: 12.71; xent: 2.54; lr: 0.00000677;   0/529 tok/s; 144053 sec
[2020-03-31 11:41:05,722 INFO] Loading train dataset from ../bert_data/cnndm.train.118.bert.pt, number of examples: 2001
[2020-03-31 11:41:42,668 INFO] Step 87300/210000; acc:  52.27; ppl:  9.07; xent: 2.20; lr: 0.00000677;   0/655 tok/s; 144137 sec
[2020-03-31 11:43:06,780 INFO] Step 87350/210000; acc:  52.91; ppl:  8.95; xent: 2.19; lr: 0.00000677;   0/624 tok/s; 144222 sec
[2020-03-31 11:43:52,832 INFO] Loading train dataset from ../bert_data/cnndm.train.119.bert.pt, number of examples: 2000
[2020-03-31 11:44:31,535 INFO] Step 87400/210000; acc:  48.33; ppl: 12.22; xent: 2.50; lr: 0.00000677;   0/635 tok/s; 144306 sec
[2020-03-31 11:45:56,004 INFO] Step 87450/210000; acc:  52.67; ppl:  9.75; xent: 2.28; lr: 0.00000676;   0/620 tok/s; 144391 sec
[2020-03-31 11:46:40,118 INFO] Loading train dataset from ../bert_data/cnndm.train.12.bert.pt, number of examples: 2001
[2020-03-31 11:47:20,536 INFO] Step 87500/210000; acc:  48.89; ppl: 11.69; xent: 2.46; lr: 0.00000676;   0/739 tok/s; 144475 sec
[2020-03-31 11:47:20,539 INFO] Saving checkpoint ../models/model_step_87500.pt
[2020-03-31 11:48:47,174 INFO] Step 87550/210000; acc:  49.56; ppl: 11.16; xent: 2.41; lr: 0.00000676;   0/693 tok/s; 144562 sec
[2020-03-31 11:49:31,657 INFO] Loading train dataset from ../bert_data/cnndm.train.120.bert.pt, number of examples: 2001
[2020-03-31 11:50:11,922 INFO] Step 87600/210000; acc:  53.88; ppl:  9.47; xent: 2.25; lr: 0.00000676;   0/807 tok/s; 144647 sec
[2020-03-31 11:51:36,398 INFO] Step 87650/210000; acc:  50.60; ppl: 10.27; xent: 2.33; lr: 0.00000676;   0/817 tok/s; 144731 sec
[2020-03-31 11:52:19,248 INFO] Loading train dataset from ../bert_data/cnndm.train.121.bert.pt, number of examples: 2001
[2020-03-31 11:53:01,820 INFO] Step 87700/210000; acc:  55.17; ppl:  8.54; xent: 2.14; lr: 0.00000675;   0/973 tok/s; 144817 sec
[2020-03-31 11:54:26,152 INFO] Step 87750/210000; acc:  54.10; ppl:  9.88; xent: 2.29; lr: 0.00000675;   0/972 tok/s; 144901 sec
[2020-03-31 11:55:07,206 INFO] Loading train dataset from ../bert_data/cnndm.train.122.bert.pt, number of examples: 2000
[2020-03-31 11:55:51,315 INFO] Step 87800/210000; acc:  54.99; ppl:  8.58; xent: 2.15; lr: 0.00000675;   0/506 tok/s; 144986 sec
[2020-03-31 11:57:15,352 INFO] Step 87850/210000; acc:  50.53; ppl: 11.73; xent: 2.46; lr: 0.00000675;   0/493 tok/s; 145070 sec
[2020-03-31 11:57:54,632 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001
[2020-03-31 11:58:39,826 INFO] Step 87900/210000; acc:  51.61; ppl: 10.10; xent: 2.31; lr: 0.00000675;   0/619 tok/s; 145155 sec
[2020-03-31 12:00:03,600 INFO] Step 87950/210000; acc:  51.39; ppl: 10.36; xent: 2.34; lr: 0.00000674;   0/560 tok/s; 145238 sec
[2020-03-31 12:00:43,098 INFO] Loading train dataset from ../bert_data/cnndm.train.124.bert.pt, number of examples: 2001
[2020-03-31 12:01:28,744 INFO] Step 88000/210000; acc:  54.88; ppl:  8.41; xent: 2.13; lr: 0.00000674;   0/634 tok/s; 145324 sec
[2020-03-31 12:01:28,747 INFO] Saving checkpoint ../models/model_step_88000.pt
[2020-03-31 12:02:55,075 INFO] Step 88050/210000; acc:  54.84; ppl:  8.56; xent: 2.15; lr: 0.00000674;   0/529 tok/s; 145410 sec
[2020-03-31 12:03:32,810 INFO] Loading train dataset from ../bert_data/cnndm.train.125.bert.pt, number of examples: 2000
[2020-03-31 12:04:20,202 INFO] Step 88100/210000; acc:  54.59; ppl:  8.91; xent: 2.19; lr: 0.00000674;   0/722 tok/s; 145495 sec
[2020-03-31 12:05:44,572 INFO] Step 88150/210000; acc:  50.08; ppl: 11.94; xent: 2.48; lr: 0.00000674;   0/780 tok/s; 145579 sec
[2020-03-31 12:06:20,515 INFO] Loading train dataset from ../bert_data/cnndm.train.126.bert.pt, number of examples: 1999
[2020-03-31 12:07:09,200 INFO] Step 88200/210000; acc:  45.80; ppl: 14.57; xent: 2.68; lr: 0.00000673;   0/840 tok/s; 145664 sec
[2020-03-31 12:08:33,278 INFO] Step 88250/210000; acc:  50.58; ppl: 10.36; xent: 2.34; lr: 0.00000673;   0/778 tok/s; 145748 sec
[2020-03-31 12:09:08,075 INFO] Loading train dataset from ../bert_data/cnndm.train.127.bert.pt, number of examples: 2000
[2020-03-31 12:09:58,293 INFO] Step 88300/210000; acc:  49.46; ppl: 11.19; xent: 2.42; lr: 0.00000673;   0/1027 tok/s; 145833 sec
[2020-03-31 12:11:22,546 INFO] Step 88350/210000; acc:  49.35; ppl: 10.96; xent: 2.39; lr: 0.00000673;   0/896 tok/s; 145917 sec
[2020-03-31 12:11:55,086 INFO] Loading train dataset from ../bert_data/cnndm.train.128.bert.pt, number of examples: 2001
[2020-03-31 12:12:47,467 INFO] Step 88400/210000; acc:  51.20; ppl: 10.86; xent: 2.38; lr: 0.00000673;   0/936 tok/s; 146002 sec
[2020-03-31 12:14:11,833 INFO] Step 88450/210000; acc:  45.18; ppl: 15.30; xent: 2.73; lr: 0.00000672;   0/1075 tok/s; 146087 sec
[2020-03-31 12:14:42,696 INFO] Loading train dataset from ../bert_data/cnndm.train.129.bert.pt, number of examples: 2000
[2020-03-31 12:15:36,948 INFO] Step 88500/210000; acc:  50.86; ppl: 10.56; xent: 2.36; lr: 0.00000672;   0/564 tok/s; 146172 sec
[2020-03-31 12:15:36,952 INFO] Saving checkpoint ../models/model_step_88500.pt
[2020-03-31 12:17:02,912 INFO] Step 88550/210000; acc:  48.05; ppl: 11.90; xent: 2.48; lr: 0.00000672;   0/591 tok/s; 146258 sec
[2020-03-31 12:17:32,438 INFO] Loading train dataset from ../bert_data/cnndm.train.13.bert.pt, number of examples: 2001
[2020-03-31 12:18:28,007 INFO] Step 88600/210000; acc:  55.69; ppl:  8.08; xent: 2.09; lr: 0.00000672;   0/737 tok/s; 146343 sec
[2020-03-31 12:19:52,121 INFO] Step 88650/210000; acc:  52.22; ppl: 10.85; xent: 2.38; lr: 0.00000672;   0/666 tok/s; 146427 sec
[2020-03-31 12:20:19,508 INFO] Loading train dataset from ../bert_data/cnndm.train.130.bert.pt, number of examples: 2000
[2020-03-31 12:21:16,648 INFO] Step 88700/210000; acc:  50.34; ppl: 10.50; xent: 2.35; lr: 0.00000672;   0/790 tok/s; 146511 sec
[2020-03-31 12:22:40,215 INFO] Step 88750/210000; acc:  56.81; ppl:  6.90; xent: 1.93; lr: 0.00000671;   0/706 tok/s; 146595 sec
[2020-03-31 12:23:07,665 INFO] Loading train dataset from ../bert_data/cnndm.train.131.bert.pt, number of examples: 2001
[2020-03-31 12:24:04,915 INFO] Step 88800/210000; acc:  50.29; ppl: 11.60; xent: 2.45; lr: 0.00000671;   0/850 tok/s; 146680 sec
[2020-03-31 12:25:28,787 INFO] Step 88850/210000; acc:  53.68; ppl:  9.15; xent: 2.21; lr: 0.00000671;   0/752 tok/s; 146764 sec
[2020-03-31 12:25:54,849 INFO] Loading train dataset from ../bert_data/cnndm.train.132.bert.pt, number of examples: 2001
[2020-03-31 12:26:53,371 INFO] Step 88900/210000; acc:  50.22; ppl: 11.06; xent: 2.40; lr: 0.00000671;   0/811 tok/s; 146848 sec
[2020-03-31 12:28:16,998 INFO] Step 88950/210000; acc:  47.80; ppl: 11.82; xent: 2.47; lr: 0.00000671;   0/716 tok/s; 146932 sec
[2020-03-31 12:28:43,193 INFO] Loading train dataset from ../bert_data/cnndm.train.133.bert.pt, number of examples: 1999
[2020-03-31 12:29:41,975 INFO] Step 89000/210000; acc:  52.96; ppl:  9.72; xent: 2.27; lr: 0.00000670;   0/967 tok/s; 147017 sec
[2020-03-31 12:29:41,978 INFO] Saving checkpoint ../models/model_step_89000.pt
[2020-03-31 12:31:07,975 INFO] Step 89050/210000; acc:  47.67; ppl: 12.62; xent: 2.54; lr: 0.00000670;   0/852 tok/s; 147103 sec
[2020-03-31 12:31:32,163 INFO] Loading train dataset from ../bert_data/cnndm.train.134.bert.pt, number of examples: 2001
[2020-03-31 12:32:32,965 INFO] Step 89100/210000; acc:  49.43; ppl:  9.19; xent: 2.22; lr: 0.00000670;   0/479 tok/s; 147188 sec
[2020-03-31 12:33:57,196 INFO] Step 89150/210000; acc:  53.00; ppl:  8.78; xent: 2.17; lr: 0.00000670;   0/574 tok/s; 147272 sec
[2020-03-31 12:34:19,921 INFO] Loading train dataset from ../bert_data/cnndm.train.135.bert.pt, number of examples: 1999
[2020-03-31 12:35:22,170 INFO] Step 89200/210000; acc:  56.31; ppl:  7.95; xent: 2.07; lr: 0.00000670;   0/546 tok/s; 147357 sec
[2020-03-31 12:36:46,016 INFO] Step 89250/210000; acc:  55.78; ppl:  7.68; xent: 2.04; lr: 0.00000669;   0/636 tok/s; 147441 sec
[2020-03-31 12:37:06,498 INFO] Loading train dataset from ../bert_data/cnndm.train.136.bert.pt, number of examples: 2001
[2020-03-31 12:38:10,298 INFO] Step 89300/210000; acc:  53.46; ppl:  9.69; xent: 2.27; lr: 0.00000669;   0/658 tok/s; 147525 sec
[2020-03-31 12:39:34,363 INFO] Step 89350/210000; acc:  51.63; ppl:  9.17; xent: 2.22; lr: 0.00000669;   0/581 tok/s; 147609 sec
[2020-03-31 12:39:55,060 INFO] Loading train dataset from ../bert_data/cnndm.train.137.bert.pt, number of examples: 2000
[2020-03-31 12:40:58,731 INFO] Step 89400/210000; acc:  56.33; ppl:  7.78; xent: 2.05; lr: 0.00000669;   0/738 tok/s; 147694 sec
[2020-03-31 12:42:22,533 INFO] Step 89450/210000; acc:  54.67; ppl:  8.70; xent: 2.16; lr: 0.00000669;   0/700 tok/s; 147777 sec
[2020-03-31 12:42:41,639 INFO] Loading train dataset from ../bert_data/cnndm.train.138.bert.pt, number of examples: 2000
[2020-03-31 12:43:47,585 INFO] Step 89500/210000; acc:  54.97; ppl:  8.02; xent: 2.08; lr: 0.00000669;   0/703 tok/s; 147862 sec
[2020-03-31 12:43:47,588 INFO] Saving checkpoint ../models/model_step_89500.pt
[2020-03-31 12:45:13,831 INFO] Step 89550/210000; acc:  60.78; ppl:  6.11; xent: 1.81; lr: 0.00000668;   0/741 tok/s; 147949 sec
[2020-03-31 12:45:32,886 INFO] Loading train dataset from ../bert_data/cnndm.train.139.bert.pt, number of examples: 2001
[2020-03-31 12:46:38,089 INFO] Step 89600/210000; acc:  49.50; ppl: 11.91; xent: 2.48; lr: 0.00000668;   0/953 tok/s; 148033 sec
[2020-03-31 12:48:01,809 INFO] Step 89650/210000; acc:  52.84; ppl:  9.34; xent: 2.23; lr: 0.00000668;   0/865 tok/s; 148117 sec
[2020-03-31 12:48:19,395 INFO] Loading train dataset from ../bert_data/cnndm.train.14.bert.pt, number of examples: 1998
[2020-03-31 12:49:26,757 INFO] Step 89700/210000; acc:  51.72; ppl: 10.84; xent: 2.38; lr: 0.00000668;   0/976 tok/s; 148202 sec
[2020-03-31 12:50:51,367 INFO] Step 89750/210000; acc:  49.24; ppl: 11.66; xent: 2.46; lr: 0.00000668;   0/862 tok/s; 148286 sec
[2020-03-31 12:51:07,294 INFO] Loading train dataset from ../bert_data/cnndm.train.140.bert.pt, number of examples: 2000
[2020-03-31 12:52:16,639 INFO] Step 89800/210000; acc:  49.85; ppl: 10.48; xent: 2.35; lr: 0.00000667;   0/622 tok/s; 148371 sec
[2020-03-31 12:53:40,538 INFO] Step 89850/210000; acc:  48.09; ppl: 11.59; xent: 2.45; lr: 0.00000667;   0/1043 tok/s; 148455 sec
[2020-03-31 12:53:54,791 INFO] Loading train dataset from ../bert_data/cnndm.train.141.bert.pt, number of examples: 1999
[2020-03-31 12:55:05,614 INFO] Step 89900/210000; acc:  50.35; ppl: 10.27; xent: 2.33; lr: 0.00000667;   0/528 tok/s; 148540 sec
[2020-03-31 12:56:29,309 INFO] Step 89950/210000; acc:  47.52; ppl: 12.69; xent: 2.54; lr: 0.00000667;   0/810 tok/s; 148624 sec
[2020-03-31 12:56:41,339 INFO] Loading train dataset from ../bert_data/cnndm.train.142.bert.pt, number of examples: 2000
[2020-03-31 12:57:53,721 INFO] Step 90000/210000; acc:  50.10; ppl:  9.94; xent: 2.30; lr: 0.00000667;   0/616 tok/s; 148709 sec
[2020-03-31 12:57:53,745 INFO] Saving checkpoint ../models/model_step_90000.pt
[2020-03-31 12:59:20,152 INFO] Step 90050/210000; acc:  47.75; ppl: 12.63; xent: 2.54; lr: 0.00000666;   0/833 tok/s; 148795 sec
[2020-03-31 12:59:32,321 INFO] Loading train dataset from ../bert_data/cnndm.train.143.bert.pt, number of examples: 1084
[2020-03-31 13:00:44,447 INFO] Step 90100/210000; acc:  54.58; ppl:  7.52; xent: 2.02; lr: 0.00000666;   0/529 tok/s; 148879 sec
[2020-03-31 13:01:03,445 INFO] Loading train dataset from ../bert_data/cnndm.train.15.bert.pt, number of examples: 1999
[2020-03-31 13:02:08,609 INFO] Step 90150/210000; acc:  52.73; ppl: 10.34; xent: 2.34; lr: 0.00000666;   0/741 tok/s; 148963 sec
[2020-03-31 13:03:32,552 INFO] Step 90200/210000; acc:  51.95; ppl:  9.45; xent: 2.25; lr: 0.00000666;   0/663 tok/s; 149047 sec
[2020-03-31 13:03:49,613 INFO] Loading train dataset from ../bert_data/cnndm.train.150.bert.pt, number of examples: 1985
[2020-03-31 13:04:48,229 INFO] Step 90250/210000; acc:  64.81; ppl:  4.04; xent: 1.40; lr: 0.00000666;   0/448 tok/s; 149123 sec
[2020-03-31 13:06:00,868 INFO] Step 90300/210000; acc:  61.72; ppl:  4.86; xent: 1.58; lr: 0.00000666;   0/634 tok/s; 149196 sec
[2020-03-31 13:06:08,932 INFO] Loading train dataset from ../bert_data/cnndm.train.151.bert.pt, number of examples: 1990
[2020-03-31 13:07:14,706 INFO] Step 90350/210000; acc:  64.78; ppl:  4.64; xent: 1.54; lr: 0.00000665;   0/465 tok/s; 149270 sec
[2020-03-31 13:08:27,006 INFO] Loading train dataset from ../bert_data/cnndm.train.152.bert.pt, number of examples: 993
[2020-03-31 13:08:28,603 INFO] Step 90400/210000; acc:  65.03; ppl:  4.29; xent: 1.46; lr: 0.00000665;   0/487 tok/s; 149343 sec
[2020-03-31 13:09:37,331 INFO] Loading train dataset from ../bert_data/cnndm.train.153.bert.pt, number of examples: 1982
[2020-03-31 13:09:41,971 INFO] Step 90450/210000; acc:  37.61; ppl: 14.26; xent: 2.66; lr: 0.00000665;   0/724 tok/s; 149417 sec
[2020-03-31 13:11:05,456 INFO] Step 90500/210000; acc:  44.10; ppl: 13.18; xent: 2.58; lr: 0.00000665;   0/930 tok/s; 149500 sec
[2020-03-31 13:11:05,459 INFO] Saving checkpoint ../models/model_step_90500.pt
[2020-03-31 13:12:16,121 INFO] Loading train dataset from ../bert_data/cnndm.train.154.bert.pt, number of examples: 1978
[2020-03-31 13:12:31,061 INFO] Step 90550/210000; acc:  50.60; ppl:  9.74; xent: 2.28; lr: 0.00000665;   0/936 tok/s; 149586 sec
[2020-03-31 13:13:53,730 INFO] Step 90600/210000; acc:  54.18; ppl:  6.73; xent: 1.91; lr: 0.00000664;   0/462 tok/s; 149669 sec
[2020-03-31 13:14:54,565 INFO] Loading train dataset from ../bert_data/cnndm.train.155.bert.pt, number of examples: 1988
[2020-03-31 13:15:17,796 INFO] Step 90650/210000; acc:  43.42; ppl: 14.54; xent: 2.68; lr: 0.00000664;   0/1321 tok/s; 149753 sec
[2020-03-31 13:16:40,199 INFO] Step 90700/210000; acc:  57.29; ppl:  6.33; xent: 1.85; lr: 0.00000664;   0/649 tok/s; 149835 sec
[2020-03-31 13:17:30,290 INFO] Loading train dataset from ../bert_data/cnndm.train.156.bert.pt, number of examples: 1993
[2020-03-31 13:18:03,015 INFO] Step 90750/210000; acc:  44.89; ppl: 12.01; xent: 2.49; lr: 0.00000664;   0/1222 tok/s; 149918 sec
[2020-03-31 13:19:25,976 INFO] Step 90800/210000; acc:  52.72; ppl:  8.16; xent: 2.10; lr: 0.00000664;   0/791 tok/s; 150001 sec
[2020-03-31 13:20:07,663 INFO] Loading train dataset from ../bert_data/cnndm.train.157.bert.pt, number of examples: 1981
[2020-03-31 13:20:49,605 INFO] Step 90850/210000; acc:  45.92; ppl: 11.62; xent: 2.45; lr: 0.00000664;   0/788 tok/s; 150084 sec
[2020-03-31 13:22:13,275 INFO] Step 90900/210000; acc:  53.96; ppl:  7.31; xent: 1.99; lr: 0.00000663;   0/699 tok/s; 150168 sec
[2020-03-31 13:22:47,572 INFO] Loading train dataset from ../bert_data/cnndm.train.158.bert.pt, number of examples: 1987
[2020-03-31 13:23:36,751 INFO] Step 90950/210000; acc:  60.03; ppl:  5.76; xent: 1.75; lr: 0.00000663;   0/457 tok/s; 150252 sec
[2020-03-31 13:24:59,579 INFO] Step 91000/210000; acc:  43.44; ppl: 13.74; xent: 2.62; lr: 0.00000663;   0/1126 tok/s; 150334 sec
[2020-03-31 13:24:59,583 INFO] Saving checkpoint ../models/model_step_91000.pt
[2020-03-31 13:25:27,128 INFO] Loading train dataset from ../bert_data/cnndm.train.159.bert.pt, number of examples: 1986
[2020-03-31 13:26:25,032 INFO] Step 91050/210000; acc:  55.32; ppl:  6.99; xent: 1.94; lr: 0.00000663;   0/643 tok/s; 150420 sec
[2020-03-31 13:27:48,125 INFO] Step 91100/210000; acc:  46.23; ppl: 10.75; xent: 2.37; lr: 0.00000663;   0/1271 tok/s; 150503 sec
[2020-03-31 13:28:03,346 INFO] Loading train dataset from ../bert_data/cnndm.train.16.bert.pt, number of examples: 2001
[2020-03-31 13:29:12,291 INFO] Step 91150/210000; acc:  48.29; ppl: 12.73; xent: 2.54; lr: 0.00000662;   0/729 tok/s; 150587 sec
[2020-03-31 13:30:36,312 INFO] Step 91200/210000; acc:  52.15; ppl:  8.95; xent: 2.19; lr: 0.00000662;   0/964 tok/s; 150671 sec
[2020-03-31 13:30:50,160 INFO] Loading train dataset from ../bert_data/cnndm.train.160.bert.pt, number of examples: 1983
[2020-03-31 13:32:00,017 INFO] Step 91250/210000; acc:  47.80; ppl: 10.74; xent: 2.37; lr: 0.00000662;   0/931 tok/s; 150755 sec
[2020-03-31 13:33:23,075 INFO] Step 91300/210000; acc:  45.94; ppl: 11.07; xent: 2.40; lr: 0.00000662;   0/998 tok/s; 150838 sec
[2020-03-31 13:33:28,204 INFO] Loading train dataset from ../bert_data/cnndm.train.161.bert.pt, number of examples: 1990
[2020-03-31 13:34:45,882 INFO] Step 91350/210000; acc:  51.30; ppl:  9.15; xent: 2.21; lr: 0.00000662;   0/1001 tok/s; 150921 sec
[2020-03-31 13:36:06,643 INFO] Loading train dataset from ../bert_data/cnndm.train.162.bert.pt, number of examples: 1986
[2020-03-31 13:36:08,296 INFO] Step 91400/210000; acc:  55.40; ppl:  6.40; xent: 1.86; lr: 0.00000662;   0/393 tok/s; 151003 sec
[2020-03-31 13:37:31,555 INFO] Step 91450/210000; acc:  48.85; ppl:  9.80; xent: 2.28; lr: 0.00000661;   0/996 tok/s; 151086 sec
[2020-03-31 13:38:42,850 INFO] Loading train dataset from ../bert_data/cnndm.train.163.bert.pt, number of examples: 1982
[2020-03-31 13:38:54,539 INFO] Step 91500/210000; acc:  59.88; ppl:  5.50; xent: 1.70; lr: 0.00000661;   0/421 tok/s; 151169 sec
[2020-03-31 13:38:54,542 INFO] Saving checkpoint ../models/model_step_91500.pt
[2020-03-31 13:40:19,946 INFO] Step 91550/210000; acc:  45.80; ppl: 11.92; xent: 2.48; lr: 0.00000661;   0/1327 tok/s; 151255 sec
[2020-03-31 13:41:22,949 INFO] Loading train dataset from ../bert_data/cnndm.train.164.bert.pt, number of examples: 1984
[2020-03-31 13:41:42,134 INFO] Step 91600/210000; acc:  56.62; ppl:  6.12; xent: 1.81; lr: 0.00000661;   0/576 tok/s; 151337 sec
[2020-03-31 13:43:05,071 INFO] Step 91650/210000; acc:  44.24; ppl: 13.01; xent: 2.57; lr: 0.00000661;   0/1249 tok/s; 151420 sec
[2020-03-31 13:44:00,198 INFO] Loading train dataset from ../bert_data/cnndm.train.165.bert.pt, number of examples: 1984
[2020-03-31 13:44:28,458 INFO] Step 91700/210000; acc:  54.68; ppl:  7.08; xent: 1.96; lr: 0.00000660;   0/653 tok/s; 151503 sec
[2020-03-31 13:45:51,322 INFO] Step 91750/210000; acc:  47.16; ppl: 11.00; xent: 2.40; lr: 0.00000660;   0/902 tok/s; 151586 sec
[2020-03-31 13:46:36,284 INFO] Loading train dataset from ../bert_data/cnndm.train.166.bert.pt, number of examples: 1370
[2020-03-31 13:47:14,217 INFO] Step 91800/210000; acc:  47.60; ppl: 10.48; xent: 2.35; lr: 0.00000660;   0/830 tok/s; 151669 sec
[2020-03-31 13:48:26,493 INFO] Loading train dataset from ../bert_data/cnndm.train.167.bert.pt, number of examples: 1089
[2020-03-31 13:48:37,668 INFO] Step 91850/210000; acc:  59.89; ppl:  5.01; xent: 1.61; lr: 0.00000660;   0/575 tok/s; 151752 sec
[2020-03-31 13:49:51,653 INFO] Loading train dataset from ../bert_data/cnndm.train.168.bert.pt, number of examples: 1991
[2020-03-31 13:49:57,941 INFO] Step 91900/210000; acc:  47.75; ppl: 11.24; xent: 2.42; lr: 0.00000660;   0/1271 tok/s; 151833 sec
[2020-03-31 13:51:19,182 INFO] Step 91950/210000; acc:  63.28; ppl:  4.22; xent: 1.44; lr: 0.00000660;   0/540 tok/s; 151914 sec
[2020-03-31 13:52:26,041 INFO] Loading train dataset from ../bert_data/cnndm.train.169.bert.pt, number of examples: 1995
[2020-03-31 13:52:40,707 INFO] Step 92000/210000; acc:  53.83; ppl:  7.71; xent: 2.04; lr: 0.00000659;   0/1062 tok/s; 151996 sec
[2020-03-31 13:52:40,710 INFO] Saving checkpoint ../models/model_step_92000.pt
[2020-03-31 13:54:04,131 INFO] Step 92050/210000; acc:  56.55; ppl:  6.13; xent: 1.81; lr: 0.00000659;   0/453 tok/s; 152079 sec
[2020-03-31 13:55:03,271 INFO] Loading train dataset from ../bert_data/cnndm.train.17.bert.pt, number of examples: 2000
[2020-03-31 13:55:26,886 INFO] Step 92100/210000; acc:  53.70; ppl:  8.77; xent: 2.17; lr: 0.00000659;   0/788 tok/s; 152162 sec
[2020-03-31 13:56:51,090 INFO] Step 92150/210000; acc:  54.50; ppl:  8.67; xent: 2.16; lr: 0.00000659;   0/881 tok/s; 152246 sec
[2020-03-31 13:57:50,201 INFO] Loading train dataset from ../bert_data/cnndm.train.170.bert.pt, number of examples: 1981
[2020-03-31 13:58:14,450 INFO] Step 92200/210000; acc:  47.58; ppl: 10.38; xent: 2.34; lr: 0.00000659;   0/1147 tok/s; 152329 sec
[2020-03-31 13:59:35,945 INFO] Step 92250/210000; acc:  60.74; ppl:  5.09; xent: 1.63; lr: 0.00000658;   0/716 tok/s; 152411 sec
[2020-03-31 14:00:24,038 INFO] Loading train dataset from ../bert_data/cnndm.train.171.bert.pt, number of examples: 1990
[2020-03-31 14:00:58,222 INFO] Step 92300/210000; acc:  63.26; ppl:  4.45; xent: 1.49; lr: 0.00000658;   0/427 tok/s; 152493 sec
[2020-03-31 14:02:18,882 INFO] Step 92350/210000; acc:  51.71; ppl:  7.55; xent: 2.02; lr: 0.00000658;   0/1054 tok/s; 152574 sec
[2020-03-31 14:02:58,359 INFO] Loading train dataset from ../bert_data/cnndm.train.172.bert.pt, number of examples: 1983
[2020-03-31 14:03:40,171 INFO] Step 92400/210000; acc:  59.68; ppl:  5.03; xent: 1.61; lr: 0.00000658;   0/460 tok/s; 152655 sec
[2020-03-31 14:05:01,088 INFO] Step 92450/210000; acc:  51.62; ppl:  7.82; xent: 2.06; lr: 0.00000658;   0/1134 tok/s; 152736 sec
[2020-03-31 14:05:32,630 INFO] Loading train dataset from ../bert_data/cnndm.train.18.bert.pt, number of examples: 1998
[2020-03-31 14:06:24,837 INFO] Step 92500/210000; acc:  51.26; ppl: 10.52; xent: 2.35; lr: 0.00000658;   0/446 tok/s; 152820 sec
[2020-03-31 14:06:24,841 INFO] Saving checkpoint ../models/model_step_92500.pt
[2020-03-31 14:07:51,354 INFO] Step 92550/210000; acc:  47.08; ppl: 12.22; xent: 2.50; lr: 0.00000657;   0/1016 tok/s; 152906 sec
[2020-03-31 14:08:22,223 INFO] Loading train dataset from ../bert_data/cnndm.train.19.bert.pt, number of examples: 2000
[2020-03-31 14:09:16,004 INFO] Step 92600/210000; acc:  52.73; ppl:  9.17; xent: 2.22; lr: 0.00000657;   0/621 tok/s; 152991 sec
[2020-03-31 14:10:39,998 INFO] Step 92650/210000; acc:  52.91; ppl:  9.49; xent: 2.25; lr: 0.00000657;   0/945 tok/s; 153075 sec
[2020-03-31 14:11:09,647 INFO] Loading train dataset from ../bert_data/cnndm.train.2.bert.pt, number of examples: 2001
[2020-03-31 14:12:05,058 INFO] Step 92700/210000; acc:  56.52; ppl:  7.93; xent: 2.07; lr: 0.00000657;   0/639 tok/s; 153160 sec
[2020-03-31 14:13:29,020 INFO] Step 92750/210000; acc:  55.91; ppl:  8.16; xent: 2.10; lr: 0.00000657;   0/685 tok/s; 153244 sec
[2020-03-31 14:13:56,534 INFO] Loading train dataset from ../bert_data/cnndm.train.20.bert.pt, number of examples: 2000
[2020-03-31 14:14:53,539 INFO] Step 92800/210000; acc:  46.50; ppl: 13.30; xent: 2.59; lr: 0.00000657;   0/730 tok/s; 153328 sec
[2020-03-31 14:16:18,002 INFO] Step 92850/210000; acc:  58.68; ppl:  7.15; xent: 1.97; lr: 0.00000656;   0/619 tok/s; 153413 sec
[2020-03-31 14:16:45,788 INFO] Loading train dataset from ../bert_data/cnndm.train.21.bert.pt, number of examples: 2001
[2020-03-31 14:17:42,960 INFO] Step 92900/210000; acc:  53.57; ppl:  8.90; xent: 2.19; lr: 0.00000656;   0/724 tok/s; 153498 sec
[2020-03-31 14:19:07,029 INFO] Step 92950/210000; acc:  54.81; ppl:  7.97; xent: 2.08; lr: 0.00000656;   0/641 tok/s; 153582 sec
[2020-03-31 14:19:32,947 INFO] Loading train dataset from ../bert_data/cnndm.train.22.bert.pt, number of examples: 1999
[2020-03-31 14:20:31,682 INFO] Step 93000/210000; acc:  49.79; ppl: 10.30; xent: 2.33; lr: 0.00000656;   0/824 tok/s; 153667 sec
[2020-03-31 14:20:31,686 INFO] Saving checkpoint ../models/model_step_93000.pt
[2020-03-31 14:21:57,684 INFO] Step 93050/210000; acc:  56.19; ppl:  7.50; xent: 2.02; lr: 0.00000656;   0/702 tok/s; 153753 sec
[2020-03-31 14:22:21,730 INFO] Loading train dataset from ../bert_data/cnndm.train.23.bert.pt, number of examples: 2001
[2020-03-31 14:23:22,342 INFO] Step 93100/210000; acc:  52.20; ppl: 10.63; xent: 2.36; lr: 0.00000655;   0/922 tok/s; 153837 sec
[2020-03-31 14:24:46,217 INFO] Step 93150/210000; acc:  52.91; ppl: 10.93; xent: 2.39; lr: 0.00000655;   0/671 tok/s; 153921 sec
[2020-03-31 14:25:10,564 INFO] Loading train dataset from ../bert_data/cnndm.train.24.bert.pt, number of examples: 2001
[2020-03-31 14:26:10,965 INFO] Step 93200/210000; acc:  50.34; ppl: 10.63; xent: 2.36; lr: 0.00000655;   0/863 tok/s; 154006 sec
[2020-03-31 14:27:35,056 INFO] Step 93250/210000; acc:  49.34; ppl: 12.19; xent: 2.50; lr: 0.00000655;   0/981 tok/s; 154090 sec
[2020-03-31 14:27:57,326 INFO] Loading train dataset from ../bert_data/cnndm.train.25.bert.pt, number of examples: 2001
[2020-03-31 14:28:59,770 INFO] Step 93300/210000; acc:  55.12; ppl:  7.87; xent: 2.06; lr: 0.00000655;   0/485 tok/s; 154175 sec
[2020-03-31 14:30:23,914 INFO] Step 93350/210000; acc:  53.89; ppl:  7.92; xent: 2.07; lr: 0.00000655;   0/520 tok/s; 154259 sec
[2020-03-31 14:30:46,383 INFO] Loading train dataset from ../bert_data/cnndm.train.26.bert.pt, number of examples: 2000
[2020-03-31 14:31:48,705 INFO] Step 93400/210000; acc:  60.48; ppl:  6.75; xent: 1.91; lr: 0.00000654;   0/487 tok/s; 154344 sec
[2020-03-31 14:33:13,110 INFO] Step 93450/210000; acc:  57.55; ppl:  7.26; xent: 1.98; lr: 0.00000654;   0/602 tok/s; 154428 sec
[2020-03-31 14:33:34,104 INFO] Loading train dataset from ../bert_data/cnndm.train.27.bert.pt, number of examples: 2001
[2020-03-31 14:34:38,270 INFO] Step 93500/210000; acc:  55.44; ppl:  8.01; xent: 2.08; lr: 0.00000654;   0/716 tok/s; 154513 sec
[2020-03-31 14:34:38,274 INFO] Saving checkpoint ../models/model_step_93500.pt
[2020-03-31 14:36:04,776 INFO] Step 93550/210000; acc:  47.33; ppl: 11.96; xent: 2.48; lr: 0.00000654;   0/648 tok/s; 154600 sec
[2020-03-31 14:36:23,724 INFO] Loading train dataset from ../bert_data/cnndm.train.28.bert.pt, number of examples: 2000
[2020-03-31 14:37:29,667 INFO] Step 93600/210000; acc:  55.20; ppl:  6.97; xent: 1.94; lr: 0.00000654;   0/730 tok/s; 154684 sec
[2020-03-31 14:38:53,727 INFO] Step 93650/210000; acc:  55.81; ppl:  8.57; xent: 2.15; lr: 0.00000654;   0/706 tok/s; 154769 sec
[2020-03-31 14:39:10,907 INFO] Loading train dataset from ../bert_data/cnndm.train.29.bert.pt, number of examples: 1999
[2020-03-31 14:40:18,246 INFO] Step 93700/210000; acc:  53.36; ppl:  8.73; xent: 2.17; lr: 0.00000653;   0/670 tok/s; 154853 sec
[2020-03-31 14:41:42,743 INFO] Step 93750/210000; acc:  54.60; ppl:  8.52; xent: 2.14; lr: 0.00000653;   0/768 tok/s; 154938 sec
[2020-03-31 14:42:00,283 INFO] Loading train dataset from ../bert_data/cnndm.train.3.bert.pt, number of examples: 2001
[2020-03-31 14:43:07,372 INFO] Step 93800/210000; acc:  54.30; ppl:  8.56; xent: 2.15; lr: 0.00000653;   0/806 tok/s; 155022 sec
[2020-03-31 14:44:31,415 INFO] Step 93850/210000; acc:  50.59; ppl: 11.68; xent: 2.46; lr: 0.00000653;   0/838 tok/s; 155106 sec
[2020-03-31 14:44:47,191 INFO] Loading train dataset from ../bert_data/cnndm.train.30.bert.pt, number of examples: 1996
[2020-03-31 14:45:55,936 INFO] Step 93900/210000; acc:  50.94; ppl:  9.86; xent: 2.29; lr: 0.00000653;   0/972 tok/s; 155191 sec
[2020-03-31 14:47:20,424 INFO] Step 93950/210000; acc:  56.79; ppl:  7.90; xent: 2.07; lr: 0.00000653;   0/874 tok/s; 155275 sec
[2020-03-31 14:47:34,451 INFO] Loading train dataset from ../bert_data/cnndm.train.31.bert.pt, number of examples: 2000
[2020-03-31 14:48:45,321 INFO] Step 94000/210000; acc:  55.63; ppl:  7.59; xent: 2.03; lr: 0.00000652;   0/537 tok/s; 155360 sec
[2020-03-31 14:48:45,325 INFO] Saving checkpoint ../models/model_step_94000.pt
[2020-03-31 14:50:11,732 INFO] Step 94050/210000; acc:  46.99; ppl: 11.80; xent: 2.47; lr: 0.00000652;   0/811 tok/s; 155447 sec
[2020-03-31 14:50:23,830 INFO] Loading train dataset from ../bert_data/cnndm.train.32.bert.pt, number of examples: 1998
[2020-03-31 14:51:36,023 INFO] Step 94100/210000; acc:  56.10; ppl:  8.02; xent: 2.08; lr: 0.00000652;   0/572 tok/s; 155531 sec
[2020-03-31 14:53:00,195 INFO] Step 94150/210000; acc:  52.57; ppl:  9.58; xent: 2.26; lr: 0.00000652;   0/590 tok/s; 155615 sec
[2020-03-31 14:53:12,876 INFO] Loading train dataset from ../bert_data/cnndm.train.33.bert.pt, number of examples: 1999
[2020-03-31 14:54:25,398 INFO] Step 94200/210000; acc:  52.22; ppl:  8.31; xent: 2.12; lr: 0.00000652;   0/686 tok/s; 155700 sec
[2020-03-31 14:55:49,042 INFO] Step 94250/210000; acc:  51.95; ppl:  8.45; xent: 2.13; lr: 0.00000651;   0/586 tok/s; 155784 sec
[2020-03-31 14:55:59,866 INFO] Loading train dataset from ../bert_data/cnndm.train.34.bert.pt, number of examples: 2000
[2020-03-31 14:57:13,897 INFO] Step 94300/210000; acc:  56.24; ppl:  7.91; xent: 2.07; lr: 0.00000651;   0/692 tok/s; 155869 sec
[2020-03-31 14:58:38,165 INFO] Step 94350/210000; acc:  51.00; ppl: 10.70; xent: 2.37; lr: 0.00000651;   0/694 tok/s; 155953 sec
[2020-03-31 14:58:47,388 INFO] Loading train dataset from ../bert_data/cnndm.train.35.bert.pt, number of examples: 1998
[2020-03-31 15:00:03,201 INFO] Step 94400/210000; acc:  51.69; ppl: 11.09; xent: 2.41; lr: 0.00000651;   0/942 tok/s; 156038 sec
[2020-03-31 15:01:26,928 INFO] Step 94450/210000; acc:  56.93; ppl:  7.69; xent: 2.04; lr: 0.00000651;   0/767 tok/s; 156122 sec
[2020-03-31 15:01:36,240 INFO] Loading train dataset from ../bert_data/cnndm.train.36.bert.pt, number of examples: 2000
[2020-03-31 15:02:51,633 INFO] Step 94500/210000; acc:  46.24; ppl: 12.79; xent: 2.55; lr: 0.00000651;   0/846 tok/s; 156206 sec
[2020-03-31 15:02:51,636 INFO] Saving checkpoint ../models/model_step_94500.pt
[2020-03-31 15:04:18,408 INFO] Step 94550/210000; acc:  54.06; ppl:  9.00; xent: 2.20; lr: 0.00000650;   0/808 tok/s; 156293 sec
[2020-03-31 15:04:25,915 INFO] Loading train dataset from ../bert_data/cnndm.train.37.bert.pt, number of examples: 1999
[2020-03-31 15:05:43,159 INFO] Step 94600/210000; acc:  54.05; ppl:  8.36; xent: 2.12; lr: 0.00000650;   0/584 tok/s; 156378 sec
[2020-03-31 15:07:07,385 INFO] Step 94650/210000; acc:  51.68; ppl:  9.77; xent: 2.28; lr: 0.00000650;   0/996 tok/s; 156462 sec
[2020-03-31 15:07:13,161 INFO] Loading train dataset from ../bert_data/cnndm.train.38.bert.pt, number of examples: 2001
[2020-03-31 15:08:32,420 INFO] Step 94700/210000; acc:  51.80; ppl:  9.84; xent: 2.29; lr: 0.00000650;   0/605 tok/s; 156547 sec
[2020-03-31 15:09:56,523 INFO] Step 94750/210000; acc:  54.17; ppl:  8.80; xent: 2.18; lr: 0.00000650;   0/601 tok/s; 156631 sec
[2020-03-31 15:10:00,764 INFO] Loading train dataset from ../bert_data/cnndm.train.39.bert.pt, number of examples: 2000
[2020-03-31 15:11:21,320 INFO] Step 94800/210000; acc:  50.19; ppl: 11.25; xent: 2.42; lr: 0.00000650;   0/626 tok/s; 156716 sec
[2020-03-31 15:12:45,841 INFO] Step 94850/210000; acc:  50.16; ppl: 10.14; xent: 2.32; lr: 0.00000649;   0/745 tok/s; 156801 sec
[2020-03-31 15:12:47,913 INFO] Loading train dataset from ../bert_data/cnndm.train.4.bert.pt, number of examples: 2001
[2020-03-31 15:14:10,732 INFO] Step 94900/210000; acc:  56.32; ppl:  8.48; xent: 2.14; lr: 0.00000649;   0/728 tok/s; 156886 sec
[2020-03-31 15:15:34,689 INFO] Step 94950/210000; acc:  48.62; ppl: 12.42; xent: 2.52; lr: 0.00000649;   0/814 tok/s; 156970 sec
[2020-03-31 15:15:37,145 INFO] Loading train dataset from ../bert_data/cnndm.train.40.bert.pt, number of examples: 1999
[2020-03-31 15:16:59,745 INFO] Step 95000/210000; acc:  55.63; ppl:  7.91; xent: 2.07; lr: 0.00000649;   0/858 tok/s; 157055 sec
[2020-03-31 15:16:59,749 INFO] Saving checkpoint ../models/model_step_95000.pt
[2020-03-31 15:18:26,144 INFO] Step 95050/210000; acc:  51.94; ppl:  9.37; xent: 2.24; lr: 0.00000649;   0/768 tok/s; 157141 sec
[2020-03-31 15:18:26,504 INFO] Loading train dataset from ../bert_data/cnndm.train.41.bert.pt, number of examples: 2000
[2020-03-31 15:19:50,499 INFO] Step 95100/210000; acc:  49.73; ppl: 10.10; xent: 2.31; lr: 0.00000649;   0/770 tok/s; 157225 sec
[2020-03-31 15:21:15,218 INFO] Step 95150/210000; acc:  52.70; ppl: 10.03; xent: 2.31; lr: 0.00000648;   0/735 tok/s; 157310 sec
[2020-03-31 15:21:15,571 INFO] Loading train dataset from ../bert_data/cnndm.train.42.bert.pt, number of examples: 2000
[2020-03-31 15:22:39,816 INFO] Step 95200/210000; acc:  60.73; ppl:  6.87; xent: 1.93; lr: 0.00000648;   0/708 tok/s; 157395 sec
[2020-03-31 15:24:03,341 INFO] Loading train dataset from ../bert_data/cnndm.train.43.bert.pt, number of examples: 2001
[2020-03-31 15:24:05,030 INFO] Step 95250/210000; acc:  58.17; ppl:  7.14; xent: 1.97; lr: 0.00000648;   0/417 tok/s; 157480 sec
[2020-03-31 15:25:28,964 INFO] Step 95300/210000; acc:  52.21; ppl:  9.77; xent: 2.28; lr: 0.00000648;   0/867 tok/s; 157564 sec
[2020-03-31 15:26:50,670 INFO] Loading train dataset from ../bert_data/cnndm.train.44.bert.pt, number of examples: 1998
[2020-03-31 15:26:54,073 INFO] Step 95350/210000; acc:  47.05; ppl: 12.69; xent: 2.54; lr: 0.00000648;   0/576 tok/s; 157649 sec
[2020-03-31 15:28:18,725 INFO] Step 95400/210000; acc:  59.96; ppl:  7.14; xent: 1.97; lr: 0.00000648;   0/590 tok/s; 157734 sec
[2020-03-31 15:29:39,010 INFO] Loading train dataset from ../bert_data/cnndm.train.45.bert.pt, number of examples: 2001
[2020-03-31 15:29:44,137 INFO] Step 95450/210000; acc:  50.28; ppl: 12.24; xent: 2.50; lr: 0.00000647;   0/752 tok/s; 157819 sec
[2020-03-31 15:31:08,264 INFO] Step 95500/210000; acc:  52.79; ppl: 10.15; xent: 2.32; lr: 0.00000647;   0/699 tok/s; 157903 sec
[2020-03-31 15:31:08,291 INFO] Saving checkpoint ../models/model_step_95500.pt
[2020-03-31 15:32:28,624 INFO] Loading train dataset from ../bert_data/cnndm.train.46.bert.pt, number of examples: 2001
[2020-03-31 15:32:35,269 INFO] Step 95550/210000; acc:  54.31; ppl:  9.53; xent: 2.25; lr: 0.00000647;   0/777 tok/s; 157990 sec
[2020-03-31 15:33:59,251 INFO] Step 95600/210000; acc:  57.89; ppl:  7.80; xent: 2.05; lr: 0.00000647;   0/583 tok/s; 158074 sec
[2020-03-31 15:35:16,883 INFO] Loading train dataset from ../bert_data/cnndm.train.47.bert.pt, number of examples: 2000
[2020-03-31 15:35:23,555 INFO] Step 95650/210000; acc:  53.12; ppl:  9.96; xent: 2.30; lr: 0.00000647;   0/836 tok/s; 158158 sec
[2020-03-31 15:36:48,100 INFO] Step 95700/210000; acc:  56.01; ppl:  8.15; xent: 2.10; lr: 0.00000647;   0/695 tok/s; 158243 sec
[2020-03-31 15:38:04,360 INFO] Loading train dataset from ../bert_data/cnndm.train.48.bert.pt, number of examples: 2000
[2020-03-31 15:38:12,811 INFO] Step 95750/210000; acc:  53.00; ppl: 10.02; xent: 2.30; lr: 0.00000646;   0/955 tok/s; 158328 sec
[2020-03-31 15:39:36,902 INFO] Step 95800/210000; acc:  51.14; ppl: 11.32; xent: 2.43; lr: 0.00000646;   0/902 tok/s; 158412 sec
[2020-03-31 15:40:52,037 INFO] Loading train dataset from ../bert_data/cnndm.train.49.bert.pt, number of examples: 2001
[2020-03-31 15:41:02,337 INFO] Step 95850/210000; acc:  51.39; ppl: 10.13; xent: 2.32; lr: 0.00000646;   0/739 tok/s; 158497 sec
[2020-03-31 15:42:26,389 INFO] Step 95900/210000; acc:  52.29; ppl:  9.41; xent: 2.24; lr: 0.00000646;   0/999 tok/s; 158581 sec
[2020-03-31 15:43:40,832 INFO] Loading train dataset from ../bert_data/cnndm.train.5.bert.pt, number of examples: 2001
[2020-03-31 15:43:50,883 INFO] Step 95950/210000; acc:  55.39; ppl:  7.35; xent: 2.00; lr: 0.00000646;   0/571 tok/s; 158666 sec
[2020-03-31 15:45:14,654 INFO] Step 96000/210000; acc:  47.06; ppl: 12.51; xent: 2.53; lr: 0.00000645;   0/615 tok/s; 158749 sec
[2020-03-31 15:45:14,658 INFO] Saving checkpoint ../models/model_step_96000.pt
[2020-03-31 15:46:29,871 INFO] Loading train dataset from ../bert_data/cnndm.train.50.bert.pt, number of examples: 2001
[2020-03-31 15:46:41,535 INFO] Step 96050/210000; acc:  48.65; ppl: 11.77; xent: 2.47; lr: 0.00000645;   0/525 tok/s; 158836 sec
[2020-03-31 15:48:05,578 INFO] Step 96100/210000; acc:  58.42; ppl:  7.03; xent: 1.95; lr: 0.00000645;   0/698 tok/s; 158920 sec
[2020-03-31 15:49:18,534 INFO] Loading train dataset from ../bert_data/cnndm.train.51.bert.pt, number of examples: 2000
[2020-03-31 15:49:30,301 INFO] Step 96150/210000; acc:  50.20; ppl: 10.01; xent: 2.30; lr: 0.00000645;   0/601 tok/s; 159005 sec
[2020-03-31 15:50:54,209 INFO] Step 96200/210000; acc:  55.01; ppl:  8.70; xent: 2.16; lr: 0.00000645;   0/497 tok/s; 159089 sec
[2020-03-31 15:52:05,163 INFO] Loading train dataset from ../bert_data/cnndm.train.52.bert.pt, number of examples: 2001
[2020-03-31 15:52:18,782 INFO] Step 96250/210000; acc:  56.29; ppl:  6.98; xent: 1.94; lr: 0.00000645;   0/646 tok/s; 159174 sec
[2020-03-31 15:53:43,278 INFO] Step 96300/210000; acc:  52.84; ppl:  8.35; xent: 2.12; lr: 0.00000644;   0/652 tok/s; 159258 sec
[2020-03-31 15:54:54,740 INFO] Loading train dataset from ../bert_data/cnndm.train.53.bert.pt, number of examples: 1999
[2020-03-31 15:55:08,095 INFO] Step 96350/210000; acc:  52.43; ppl:  9.55; xent: 2.26; lr: 0.00000644;   0/775 tok/s; 159343 sec
[2020-03-31 15:56:32,233 INFO] Step 96400/210000; acc:  52.44; ppl:  9.51; xent: 2.25; lr: 0.00000644;   0/677 tok/s; 159427 sec
[2020-03-31 15:57:42,472 INFO] Loading train dataset from ../bert_data/cnndm.train.54.bert.pt, number of examples: 2001
[2020-03-31 15:57:57,425 INFO] Step 96450/210000; acc:  55.91; ppl:  8.15; xent: 2.10; lr: 0.00000644;   0/849 tok/s; 159512 sec
[2020-03-31 15:59:21,879 INFO] Step 96500/210000; acc:  54.65; ppl:  8.31; xent: 2.12; lr: 0.00000644;   0/722 tok/s; 159597 sec
[2020-03-31 15:59:21,907 INFO] Saving checkpoint ../models/model_step_96500.pt
[2020-03-31 16:00:32,350 INFO] Loading train dataset from ../bert_data/cnndm.train.55.bert.pt, number of examples: 1999
[2020-03-31 16:00:49,185 INFO] Step 96550/210000; acc:  56.87; ppl:  7.70; xent: 2.04; lr: 0.00000644;   0/900 tok/s; 159684 sec
[2020-03-31 16:02:13,003 INFO] Step 96600/210000; acc:  49.14; ppl: 12.51; xent: 2.53; lr: 0.00000643;   0/871 tok/s; 159768 sec
[2020-03-31 16:03:19,028 INFO] Loading train dataset from ../bert_data/cnndm.train.56.bert.pt, number of examples: 2001
[2020-03-31 16:03:37,334 INFO] Step 96650/210000; acc:  49.51; ppl: 11.75; xent: 2.46; lr: 0.00000643;   0/955 tok/s; 159852 sec
[2020-03-31 16:05:01,442 INFO] Step 96700/210000; acc:  52.31; ppl:  9.26; xent: 2.23; lr: 0.00000643;   0/797 tok/s; 159936 sec
[2020-03-31 16:06:08,003 INFO] Loading train dataset from ../bert_data/cnndm.train.57.bert.pt, number of examples: 1999
[2020-03-31 16:06:26,438 INFO] Step 96750/210000; acc:  54.29; ppl:  9.27; xent: 2.23; lr: 0.00000643;   0/498 tok/s; 160021 sec
[2020-03-31 16:07:50,480 INFO] Step 96800/210000; acc:  56.24; ppl:  7.15; xent: 1.97; lr: 0.00000643;   0/496 tok/s; 160105 sec
[2020-03-31 16:08:55,149 INFO] Loading train dataset from ../bert_data/cnndm.train.58.bert.pt, number of examples: 2000
[2020-03-31 16:09:15,440 INFO] Step 96850/210000; acc:  46.94; ppl: 11.23; xent: 2.42; lr: 0.00000643;   0/539 tok/s; 160190 sec
[2020-03-31 16:10:39,723 INFO] Step 96900/210000; acc:  55.17; ppl:  8.99; xent: 2.20; lr: 0.00000642;   0/564 tok/s; 160275 sec
[2020-03-31 16:11:43,170 INFO] Loading train dataset from ../bert_data/cnndm.train.59.bert.pt, number of examples: 2000
[2020-03-31 16:12:05,307 INFO] Step 96950/210000; acc:  55.28; ppl:  8.64; xent: 2.16; lr: 0.00000642;   0/665 tok/s; 160360 sec
[2020-03-31 16:13:29,925 INFO] Step 97000/210000; acc:  57.73; ppl:  7.85; xent: 2.06; lr: 0.00000642;   0/624 tok/s; 160445 sec
[2020-03-31 16:13:29,951 INFO] Saving checkpoint ../models/model_step_97000.pt
[2020-03-31 16:14:33,626 INFO] Loading train dataset from ../bert_data/cnndm.train.6.bert.pt, number of examples: 2001
[2020-03-31 16:14:57,296 INFO] Step 97050/210000; acc:  53.07; ppl:  9.36; xent: 2.24; lr: 0.00000642;   0/846 tok/s; 160532 sec
[2020-03-31 16:16:21,701 INFO] Step 97100/210000; acc:  49.66; ppl: 11.22; xent: 2.42; lr: 0.00000642;   0/877 tok/s; 160617 sec
[2020-03-31 16:17:21,484 INFO] Loading train dataset from ../bert_data/cnndm.train.60.bert.pt, number of examples: 2000
[2020-03-31 16:17:46,572 INFO] Step 97150/210000; acc:  53.14; ppl:  9.23; xent: 2.22; lr: 0.00000642;   0/1019 tok/s; 160701 sec
[2020-03-31 16:19:10,781 INFO] Step 97200/210000; acc:  55.10; ppl:  8.29; xent: 2.12; lr: 0.00000642;   0/902 tok/s; 160786 sec
[2020-03-31 16:20:08,286 INFO] Loading train dataset from ../bert_data/cnndm.train.61.bert.pt, number of examples: 2001
[2020-03-31 16:20:35,547 INFO] Step 97250/210000; acc:  57.34; ppl:  7.20; xent: 1.97; lr: 0.00000641;   0/701 tok/s; 160870 sec
[2020-03-31 16:21:59,908 INFO] Step 97300/210000; acc:  43.74; ppl: 13.80; xent: 2.62; lr: 0.00000641;   0/597 tok/s; 160955 sec
[2020-03-31 16:22:57,674 INFO] Loading train dataset from ../bert_data/cnndm.train.62.bert.pt, number of examples: 2001
[2020-03-31 16:23:24,529 INFO] Step 97350/210000; acc:  49.94; ppl:  9.68; xent: 2.27; lr: 0.00000641;   0/487 tok/s; 161039 sec
[2020-03-31 16:24:48,897 INFO] Step 97400/210000; acc:  50.92; ppl:  9.94; xent: 2.30; lr: 0.00000641;   0/711 tok/s; 161124 sec
[2020-03-31 16:25:45,043 INFO] Loading train dataset from ../bert_data/cnndm.train.63.bert.pt, number of examples: 2001
[2020-03-31 16:26:13,835 INFO] Step 97450/210000; acc:  57.25; ppl:  7.27; xent: 1.98; lr: 0.00000641;   0/580 tok/s; 161209 sec
[2020-03-31 16:27:37,443 INFO] Step 97500/210000; acc:  54.01; ppl:  9.28; xent: 2.23; lr: 0.00000641;   0/835 tok/s; 161292 sec
[2020-03-31 16:27:37,447 INFO] Saving checkpoint ../models/model_step_97500.pt
[2020-03-31 16:28:33,610 INFO] Loading train dataset from ../bert_data/cnndm.train.64.bert.pt, number of examples: 2001
[2020-03-31 16:29:03,837 INFO] Step 97550/210000; acc:  47.75; ppl: 10.33; xent: 2.33; lr: 0.00000640;   0/549 tok/s; 161379 sec
[2020-03-31 16:30:27,999 INFO] Step 97600/210000; acc:  55.31; ppl:  7.47; xent: 2.01; lr: 0.00000640;   0/893 tok/s; 161463 sec
[2020-03-31 16:31:22,651 INFO] Loading train dataset from ../bert_data/cnndm.train.65.bert.pt, number of examples: 2000
[2020-03-31 16:31:53,203 INFO] Step 97650/210000; acc:  52.86; ppl:  8.58; xent: 2.15; lr: 0.00000640;   0/668 tok/s; 161548 sec
[2020-03-31 16:33:17,325 INFO] Step 97700/210000; acc:  50.82; ppl: 10.29; xent: 2.33; lr: 0.00000640;   0/669 tok/s; 161632 sec
[2020-03-31 16:34:10,275 INFO] Loading train dataset from ../bert_data/cnndm.train.66.bert.pt, number of examples: 2001
[2020-03-31 16:34:42,139 INFO] Step 97750/210000; acc:  59.80; ppl:  6.34; xent: 1.85; lr: 0.00000640;   0/824 tok/s; 161717 sec
[2020-03-31 16:36:06,355 INFO] Step 97800/210000; acc:  56.62; ppl:  7.05; xent: 1.95; lr: 0.00000640;   0/689 tok/s; 161801 sec
[2020-03-31 16:36:57,466 INFO] Loading train dataset from ../bert_data/cnndm.train.67.bert.pt, number of examples: 1999
[2020-03-31 16:37:31,250 INFO] Step 97850/210000; acc:  53.79; ppl:  9.32; xent: 2.23; lr: 0.00000639;   0/835 tok/s; 161886 sec
[2020-03-31 16:38:55,510 INFO] Step 97900/210000; acc:  48.99; ppl: 10.82; xent: 2.38; lr: 0.00000639;   0/800 tok/s; 161970 sec
[2020-03-31 16:39:46,947 INFO] Loading train dataset from ../bert_data/cnndm.train.68.bert.pt, number of examples: 1999
[2020-03-31 16:40:20,539 INFO] Step 97950/210000; acc:  52.19; ppl:  9.38; xent: 2.24; lr: 0.00000639;   0/966 tok/s; 162055 sec
[2020-03-31 16:41:44,998 INFO] Step 98000/210000; acc:  57.05; ppl:  7.30; xent: 1.99; lr: 0.00000639;   0/810 tok/s; 162140 sec
[2020-03-31 16:41:45,002 INFO] Saving checkpoint ../models/model_step_98000.pt
[2020-03-31 16:42:37,018 INFO] Loading train dataset from ../bert_data/cnndm.train.69.bert.pt, number of examples: 2000
[2020-03-31 16:43:12,182 INFO] Step 98050/210000; acc:  51.79; ppl:  9.49; xent: 2.25; lr: 0.00000639;   0/546 tok/s; 162227 sec
[2020-03-31 16:44:36,370 INFO] Step 98100/210000; acc:  52.63; ppl:  8.40; xent: 2.13; lr: 0.00000639;   0/594 tok/s; 162311 sec
[2020-03-31 16:45:24,294 INFO] Loading train dataset from ../bert_data/cnndm.train.7.bert.pt, number of examples: 2001
[2020-03-31 16:46:01,321 INFO] Step 98150/210000; acc:  55.57; ppl:  7.70; xent: 2.04; lr: 0.00000638;   0/629 tok/s; 162396 sec
[2020-03-31 16:47:25,635 INFO] Step 98200/210000; acc:  53.15; ppl:  7.99; xent: 2.08; lr: 0.00000638;   0/574 tok/s; 162480 sec
[2020-03-31 16:48:12,405 INFO] Loading train dataset from ../bert_data/cnndm.train.70.bert.pt, number of examples: 2000
[2020-03-31 16:48:51,133 INFO] Step 98250/210000; acc:  54.03; ppl:  8.96; xent: 2.19; lr: 0.00000638;   0/741 tok/s; 162566 sec
[2020-03-31 16:50:15,338 INFO] Step 98300/210000; acc:  50.70; ppl: 10.90; xent: 2.39; lr: 0.00000638;   0/721 tok/s; 162650 sec
[2020-03-31 16:51:01,391 INFO] Loading train dataset from ../bert_data/cnndm.train.71.bert.pt, number of examples: 1999
[2020-03-31 16:51:40,024 INFO] Step 98350/210000; acc:  53.02; ppl:  9.91; xent: 2.29; lr: 0.00000638;   0/761 tok/s; 162735 sec
[2020-03-31 16:53:04,695 INFO] Step 98400/210000; acc:  50.27; ppl:  9.30; xent: 2.23; lr: 0.00000638;   0/763 tok/s; 162820 sec
[2020-03-31 16:53:49,176 INFO] Loading train dataset from ../bert_data/cnndm.train.72.bert.pt, number of examples: 2000
[2020-03-31 16:54:29,705 INFO] Step 98450/210000; acc:  53.78; ppl:  8.90; xent: 2.19; lr: 0.00000637;   0/711 tok/s; 162905 sec
[2020-03-31 16:55:54,651 INFO] Step 98500/210000; acc:  51.09; ppl: 10.40; xent: 2.34; lr: 0.00000637;   0/716 tok/s; 162989 sec
[2020-03-31 16:55:54,676 INFO] Saving checkpoint ../models/model_step_98500.pt
[2020-03-31 16:56:39,671 INFO] Loading train dataset from ../bert_data/cnndm.train.73.bert.pt, number of examples: 2000
[2020-03-31 16:57:21,750 INFO] Step 98550/210000; acc:  52.88; ppl: 10.19; xent: 2.32; lr: 0.00000637;   0/1094 tok/s; 163077 sec
[2020-03-31 16:58:45,619 INFO] Step 98600/210000; acc:  50.80; ppl: 10.31; xent: 2.33; lr: 0.00000637;   0/884 tok/s; 163160 sec
[2020-03-31 16:59:26,511 INFO] Loading train dataset from ../bert_data/cnndm.train.74.bert.pt, number of examples: 2000
[2020-03-31 17:00:10,205 INFO] Step 98650/210000; acc:  52.54; ppl:  9.85; xent: 2.29; lr: 0.00000637;   0/953 tok/s; 163245 sec
[2020-03-31 17:01:34,428 INFO] Step 98700/210000; acc:  55.61; ppl:  7.92; xent: 2.07; lr: 0.00000637;   0/803 tok/s; 163329 sec
[2020-03-31 17:02:15,511 INFO] Loading train dataset from ../bert_data/cnndm.train.75.bert.pt, number of examples: 1999
[2020-03-31 17:02:59,504 INFO] Step 98750/210000; acc:  50.81; ppl:  8.58; xent: 2.15; lr: 0.00000636;   0/750 tok/s; 163414 sec
[2020-03-31 17:04:23,760 INFO] Step 98800/210000; acc:  49.08; ppl: 11.28; xent: 2.42; lr: 0.00000636;   0/1019 tok/s; 163499 sec
[2020-03-31 17:05:03,266 INFO] Loading train dataset from ../bert_data/cnndm.train.76.bert.pt, number of examples: 1999
[2020-03-31 17:05:48,967 INFO] Step 98850/210000; acc:  55.43; ppl:  7.17; xent: 1.97; lr: 0.00000636;   0/584 tok/s; 163584 sec
[2020-03-31 17:07:12,827 INFO] Step 98900/210000; acc:  55.48; ppl:  8.04; xent: 2.08; lr: 0.00000636;   0/568 tok/s; 163668 sec
[2020-03-31 17:07:50,353 INFO] Loading train dataset from ../bert_data/cnndm.train.77.bert.pt, number of examples: 2001
[2020-03-31 17:08:37,846 INFO] Step 98950/210000; acc:  55.30; ppl:  7.61; xent: 2.03; lr: 0.00000636;   0/684 tok/s; 163753 sec
[2020-03-31 17:10:01,966 INFO] Step 99000/210000; acc:  52.29; ppl:  9.00; xent: 2.20; lr: 0.00000636;   0/687 tok/s; 163837 sec
[2020-03-31 17:10:01,991 INFO] Saving checkpoint ../models/model_step_99000.pt
[2020-03-31 17:10:39,832 INFO] Loading train dataset from ../bert_data/cnndm.train.78.bert.pt, number of examples: 2001
[2020-03-31 17:11:28,200 INFO] Step 99050/210000; acc:  54.38; ppl:  8.58; xent: 2.15; lr: 0.00000635;   0/774 tok/s; 163923 sec
[2020-03-31 17:12:52,420 INFO] Step 99100/210000; acc:  49.92; ppl: 12.17; xent: 2.50; lr: 0.00000635;   0/766 tok/s; 164007 sec
[2020-03-31 17:13:28,728 INFO] Loading train dataset from ../bert_data/cnndm.train.79.bert.pt, number of examples: 1999
[2020-03-31 17:14:17,031 INFO] Step 99150/210000; acc:  58.22; ppl:  6.51; xent: 1.87; lr: 0.00000635;   0/788 tok/s; 164092 sec
[2020-03-31 17:15:41,902 INFO] Step 99200/210000; acc:  49.41; ppl: 10.37; xent: 2.34; lr: 0.00000635;   0/956 tok/s; 164177 sec
[2020-03-31 17:16:16,085 INFO] Loading train dataset from ../bert_data/cnndm.train.8.bert.pt, number of examples: 2000
[2020-03-31 17:17:06,693 INFO] Step 99250/210000; acc:  55.84; ppl:  7.86; xent: 2.06; lr: 0.00000635;   0/909 tok/s; 164262 sec
[2020-03-31 17:18:31,160 INFO] Step 99300/210000; acc:  54.59; ppl:  9.14; xent: 2.21; lr: 0.00000635;   0/896 tok/s; 164346 sec
[2020-03-31 17:19:03,992 INFO] Loading train dataset from ../bert_data/cnndm.train.80.bert.pt, number of examples: 1999
[2020-03-31 17:19:56,599 INFO] Step 99350/210000; acc:  53.16; ppl:  8.17; xent: 2.10; lr: 0.00000635;   0/521 tok/s; 164431 sec
[2020-03-31 17:21:20,770 INFO] Step 99400/210000; acc:  53.40; ppl:  9.46; xent: 2.25; lr: 0.00000634;   0/1006 tok/s; 164516 sec
[2020-03-31 17:21:51,727 INFO] Loading train dataset from ../bert_data/cnndm.train.81.bert.pt, number of examples: 2000
[2020-03-31 17:22:45,949 INFO] Step 99450/210000; acc:  58.39; ppl:  6.92; xent: 1.94; lr: 0.00000634;   0/572 tok/s; 164601 sec
[2020-03-31 17:24:10,081 INFO] Step 99500/210000; acc:  54.59; ppl:  8.53; xent: 2.14; lr: 0.00000634;   0/461 tok/s; 164685 sec
[2020-03-31 17:24:10,084 INFO] Saving checkpoint ../models/model_step_99500.pt
[2020-03-31 17:24:41,148 INFO] Loading train dataset from ../bert_data/cnndm.train.82.bert.pt, number of examples: 2001
[2020-03-31 17:25:36,602 INFO] Step 99550/210000; acc:  54.71; ppl:  8.02; xent: 2.08; lr: 0.00000634;   0/629 tok/s; 164771 sec
[2020-03-31 17:27:01,127 INFO] Step 99600/210000; acc:  56.13; ppl:  7.95; xent: 2.07; lr: 0.00000634;   0/630 tok/s; 164856 sec
[2020-03-31 17:27:30,730 INFO] Loading train dataset from ../bert_data/cnndm.train.83.bert.pt, number of examples: 1999
[2020-03-31 17:28:26,286 INFO] Step 99650/210000; acc:  53.62; ppl:  8.87; xent: 2.18; lr: 0.00000634;   0/695 tok/s; 164941 sec
[2020-03-31 17:29:50,315 INFO] Step 99700/210000; acc:  55.81; ppl:  7.91; xent: 2.07; lr: 0.00000633;   0/790 tok/s; 165025 sec
[2020-03-31 17:30:16,266 INFO] Loading train dataset from ../bert_data/cnndm.train.84.bert.pt, number of examples: 2000
[2020-03-31 17:31:15,188 INFO] Step 99750/210000; acc:  52.06; ppl: 10.12; xent: 2.31; lr: 0.00000633;   0/943 tok/s; 165110 sec
[2020-03-31 17:32:39,338 INFO] Step 99800/210000; acc:  54.40; ppl:  9.07; xent: 2.21; lr: 0.00000633;   0/965 tok/s; 165194 sec
[2020-03-31 17:33:05,370 INFO] Loading train dataset from ../bert_data/cnndm.train.85.bert.pt, number of examples: 2001
[2020-03-31 17:34:04,341 INFO] Step 99850/210000; acc:  46.13; ppl: 14.29; xent: 2.66; lr: 0.00000633;   0/846 tok/s; 165279 sec
[2020-03-31 17:35:28,685 INFO] Step 99900/210000; acc:  58.88; ppl:  6.93; xent: 1.94; lr: 0.00000633;   0/962 tok/s; 165364 sec
[2020-03-31 17:35:52,910 INFO] Loading train dataset from ../bert_data/cnndm.train.86.bert.pt, number of examples: 1999
[2020-03-31 17:36:53,544 INFO] Step 99950/210000; acc:  51.01; ppl:  9.69; xent: 2.27; lr: 0.00000633;   0/997 tok/s; 165448 sec
[2020-03-31 17:38:18,536 INFO] Step 100000/210000; acc:  56.66; ppl:  6.97; xent: 1.94; lr: 0.00000632;   0/833 tok/s; 165533 sec
[2020-03-31 17:38:18,558 INFO] Saving checkpoint ../models/model_step_100000.pt
[2020-03-31 17:38:43,241 INFO] Loading train dataset from ../bert_data/cnndm.train.87.bert.pt, number of examples: 1999
[2020-03-31 17:39:45,925 INFO] Step 100050/210000; acc:  52.69; ppl:  9.14; xent: 2.21; lr: 0.00000632;   0/632 tok/s; 165621 sec
[2020-03-31 17:41:10,490 INFO] Step 100100/210000; acc:  46.69; ppl: 11.53; xent: 2.44; lr: 0.00000632;   0/563 tok/s; 165705 sec
[2020-03-31 17:41:31,430 INFO] Loading train dataset from ../bert_data/cnndm.train.88.bert.pt, number of examples: 1999
[2020-03-31 17:42:35,345 INFO] Step 100150/210000; acc:  57.85; ppl:  7.81; xent: 2.06; lr: 0.00000632;   0/708 tok/s; 165790 sec
[2020-03-31 17:43:59,386 INFO] Step 100200/210000; acc:  56.05; ppl:  7.05; xent: 1.95; lr: 0.00000632;   0/671 tok/s; 165874 sec
[2020-03-31 17:44:18,774 INFO] Loading train dataset from ../bert_data/cnndm.train.89.bert.pt, number of examples: 2001
[2020-03-31 17:45:23,853 INFO] Step 100250/210000; acc:  53.17; ppl:  9.27; xent: 2.23; lr: 0.00000632;   0/659 tok/s; 165959 sec
[2020-03-31 17:46:48,352 INFO] Step 100300/210000; acc:  58.82; ppl:  6.47; xent: 1.87; lr: 0.00000632;   0/749 tok/s; 166043 sec
[2020-03-31 17:47:07,625 INFO] Loading train dataset from ../bert_data/cnndm.train.9.bert.pt, number of examples: 1999
[2020-03-31 17:48:13,364 INFO] Step 100350/210000; acc:  53.19; ppl:  8.74; xent: 2.17; lr: 0.00000631;   0/949 tok/s; 166128 sec
[2020-03-31 17:49:37,860 INFO] Step 100400/210000; acc:  51.54; ppl: 10.65; xent: 2.37; lr: 0.00000631;   0/846 tok/s; 166213 sec
[2020-03-31 17:49:55,491 INFO] Loading train dataset from ../bert_data/cnndm.train.90.bert.pt, number of examples: 2000
[2020-03-31 17:51:03,240 INFO] Step 100450/210000; acc:  55.54; ppl:  7.57; xent: 2.02; lr: 0.00000631;   0/1005 tok/s; 166298 sec
[2020-03-31 17:52:27,830 INFO] Step 100500/210000; acc:  47.70; ppl: 13.07; xent: 2.57; lr: 0.00000631;   0/832 tok/s; 166383 sec
[2020-03-31 17:52:27,854 INFO] Saving checkpoint ../models/model_step_100500.pt
[2020-03-31 17:52:46,032 INFO] Loading train dataset from ../bert_data/cnndm.train.9000.bert.pt, number of examples: 1984
[2020-03-31 17:53:53,727 INFO] Step 100550/210000; acc:  51.34; ppl:  8.90; xent: 2.19; lr: 0.00000631;   0/741 tok/s; 166469 sec
[2020-03-31 17:55:16,234 INFO] Step 100600/210000; acc:  47.32; ppl: 11.18; xent: 2.41; lr: 0.00000631;   0/822 tok/s; 166551 sec
[2020-03-31 17:55:21,834 INFO] Loading train dataset from ../bert_data/cnndm.train.9001.bert.pt, number of examples: 1983
[2020-03-31 17:56:38,748 INFO] Step 100650/210000; acc:  47.89; ppl: 10.17; xent: 2.32; lr: 0.00000630;   0/963 tok/s; 166634 sec
[2020-03-31 17:57:57,834 INFO] Loading train dataset from ../bert_data/cnndm.train.9002.bert.pt, number of examples: 1990
[2020-03-31 17:58:01,387 INFO] Step 100700/210000; acc:  57.90; ppl:  5.21; xent: 1.65; lr: 0.00000630;   0/436 tok/s; 166716 sec
[2020-03-31 17:59:23,535 INFO] Step 100750/210000; acc:  49.62; ppl:  9.97; xent: 2.30; lr: 0.00000630;   0/1058 tok/s; 166798 sec
[2020-03-31 18:00:34,641 INFO] Loading train dataset from ../bert_data/cnndm.train.9003.bert.pt, number of examples: 1985
[2020-03-31 18:00:46,236 INFO] Step 100800/210000; acc:  57.23; ppl:  6.77; xent: 1.91; lr: 0.00000630;   0/523 tok/s; 166881 sec
[2020-03-31 18:02:08,805 INFO] Step 100850/210000; acc:  43.54; ppl: 13.87; xent: 2.63; lr: 0.00000630;   0/1171 tok/s; 166964 sec
[2020-03-31 18:03:11,585 INFO] Loading train dataset from ../bert_data/cnndm.train.9004.bert.pt, number of examples: 1981
[2020-03-31 18:03:31,320 INFO] Step 100900/210000; acc:  56.42; ppl:  6.70; xent: 1.90; lr: 0.00000630;   0/631 tok/s; 167046 sec
[2020-03-31 18:04:53,935 INFO] Step 100950/210000; acc:  45.31; ppl: 12.67; xent: 2.54; lr: 0.00000629;   0/1060 tok/s; 167129 sec
[2020-03-31 18:05:48,902 INFO] Loading train dataset from ../bert_data/cnndm.train.9005.bert.pt, number of examples: 1986
[2020-03-31 18:06:16,859 INFO] Step 101000/210000; acc:  50.93; ppl:  7.92; xent: 2.07; lr: 0.00000629;   0/589 tok/s; 167212 sec
[2020-03-31 18:06:16,863 INFO] Saving checkpoint ../models/model_step_101000.pt
[2020-03-31 18:07:41,952 INFO] Step 101050/210000; acc:  43.58; ppl: 14.73; xent: 2.69; lr: 0.00000629;   0/1362 tok/s; 167297 sec
[2020-03-31 18:08:29,209 INFO] Loading train dataset from ../bert_data/cnndm.train.9006.bert.pt, number of examples: 1993
[2020-03-31 18:09:05,132 INFO] Step 101100/210000; acc:  53.36; ppl:  8.01; xent: 2.08; lr: 0.00000629;   0/733 tok/s; 167380 sec
[2020-03-31 18:10:27,727 INFO] Step 101150/210000; acc:  42.50; ppl: 14.85; xent: 2.70; lr: 0.00000629;   0/1292 tok/s; 167463 sec
[2020-03-31 18:11:06,719 INFO] Loading train dataset from ../bert_data/cnndm.train.9007.bert.pt, number of examples: 1983
[2020-03-31 18:11:51,491 INFO] Step 101200/210000; acc:  48.09; ppl:  9.31; xent: 2.23; lr: 0.00000629;   0/772 tok/s; 167546 sec
[2020-03-31 18:13:13,786 INFO] Step 101250/210000; acc:  43.86; ppl: 14.08; xent: 2.64; lr: 0.00000629;   0/1360 tok/s; 167629 sec
[2020-03-31 18:13:44,284 INFO] Loading train dataset from ../bert_data/cnndm.train.9008.bert.pt, number of examples: 1990
[2020-03-31 18:14:36,778 INFO] Step 101300/210000; acc:  48.47; ppl: 10.49; xent: 2.35; lr: 0.00000628;   0/954 tok/s; 167712 sec
[2020-03-31 18:15:59,139 INFO] Step 101350/210000; acc:  48.60; ppl: 12.47; xent: 2.52; lr: 0.00000628;   0/725 tok/s; 167794 sec
[2020-03-31 18:16:21,252 INFO] Loading train dataset from ../bert_data/cnndm.train.9009.bert.pt, number of examples: 1988
[2020-03-31 18:17:22,357 INFO] Step 101400/210000; acc:  47.98; ppl: 10.05; xent: 2.31; lr: 0.00000628;   0/919 tok/s; 167877 sec
[2020-03-31 18:18:45,654 INFO] Step 101450/210000; acc:  55.98; ppl:  6.65; xent: 1.90; lr: 0.00000628;   0/431 tok/s; 167960 sec
[2020-03-31 18:18:57,435 INFO] Loading train dataset from ../bert_data/cnndm.train.9010.bert.pt, number of examples: 1984
[2020-03-31 18:20:08,911 INFO] Step 101500/210000; acc:  41.23; ppl: 14.96; xent: 2.71; lr: 0.00000628;   0/1255 tok/s; 168044 sec
[2020-03-31 18:20:08,915 INFO] Saving checkpoint ../models/model_step_101500.pt
[2020-03-31 18:21:33,850 INFO] Step 101550/210000; acc:  51.52; ppl:  8.01; xent: 2.08; lr: 0.00000628;   0/553 tok/s; 168129 sec
[2020-03-31 18:21:37,796 INFO] Loading train dataset from ../bert_data/cnndm.train.9011.bert.pt, number of examples: 1983
[2020-03-31 18:22:56,991 INFO] Step 101600/210000; acc:  44.36; ppl: 13.57; xent: 2.61; lr: 0.00000627;   0/1386 tok/s; 168212 sec
[2020-03-31 18:24:14,025 INFO] Loading train dataset from ../bert_data/cnndm.train.9012.bert.pt, number of examples: 1985
[2020-03-31 18:24:19,000 INFO] Step 101650/210000; acc:  56.98; ppl:  6.67; xent: 1.90; lr: 0.00000627;   0/627 tok/s; 168294 sec
[2020-03-31 18:25:41,504 INFO] Step 101700/210000; acc:  45.96; ppl: 11.75; xent: 2.46; lr: 0.00000627;   0/1351 tok/s; 168376 sec
[2020-03-31 18:26:52,316 INFO] Loading train dataset from ../bert_data/cnndm.train.9013.bert.pt, number of examples: 1987
[2020-03-31 18:27:05,706 INFO] Step 101750/210000; acc:  47.83; ppl: 10.04; xent: 2.31; lr: 0.00000627;   0/899 tok/s; 168461 sec
[2020-03-31 18:28:28,377 INFO] Step 101800/210000; acc:  46.55; ppl: 11.58; xent: 2.45; lr: 0.00000627;   0/892 tok/s; 168543 sec
[2020-03-31 18:29:29,172 INFO] Loading train dataset from ../bert_data/cnndm.train.9014.bert.pt, number of examples: 1976
[2020-03-31 18:29:50,649 INFO] Step 101850/210000; acc:  49.05; ppl:  9.15; xent: 2.21; lr: 0.00000627;   0/887 tok/s; 168625 sec
[2020-03-31 18:31:14,051 INFO] Step 101900/210000; acc:  55.26; ppl:  6.54; xent: 1.88; lr: 0.00000627;   0/446 tok/s; 168709 sec
[2020-03-31 18:32:06,012 INFO] Loading train dataset from ../bert_data/cnndm.train.9015.bert.pt, number of examples: 1982
[2020-03-31 18:32:37,953 INFO] Step 101950/210000; acc:  44.75; ppl: 13.42; xent: 2.60; lr: 0.00000626;   0/1106 tok/s; 168793 sec
[2020-03-31 18:33:59,829 INFO] Step 102000/210000; acc:  51.39; ppl:  8.18; xent: 2.10; lr: 0.00000626;   0/628 tok/s; 168875 sec
[2020-03-31 18:33:59,831 INFO] Saving checkpoint ../models/model_step_102000.pt
[2020-03-31 18:34:44,135 INFO] Loading train dataset from ../bert_data/cnndm.train.9016.bert.pt, number of examples: 1984
[2020-03-31 18:35:25,193 INFO] Step 102050/210000; acc:  46.61; ppl: 10.49; xent: 2.35; lr: 0.00000626;   0/813 tok/s; 168960 sec
[2020-03-31 18:36:47,407 INFO] Step 102100/210000; acc:  49.95; ppl:  8.70; xent: 2.16; lr: 0.00000626;   0/679 tok/s; 169042 sec
[2020-03-31 18:37:20,623 INFO] Loading train dataset from ../bert_data/cnndm.train.9017.bert.pt, number of examples: 1984
[2020-03-31 18:38:10,215 INFO] Step 102150/210000; acc:  50.16; ppl: 10.29; xent: 2.33; lr: 0.00000626;   0/571 tok/s; 169125 sec
[2020-03-31 18:39:32,202 INFO] Step 102200/210000; acc:  55.07; ppl:  6.49; xent: 1.87; lr: 0.00000626;   0/824 tok/s; 169207 sec
[2020-03-31 18:39:56,964 INFO] Loading train dataset from ../bert_data/cnndm.train.9018.bert.pt, number of examples: 1989
[2020-03-31 18:40:54,937 INFO] Step 102250/210000; acc:  48.98; ppl:  9.14; xent: 2.21; lr: 0.00000625;   0/660 tok/s; 169290 sec
[2020-03-31 18:42:17,185 INFO] Step 102300/210000; acc:  45.98; ppl: 10.95; xent: 2.39; lr: 0.00000625;   0/937 tok/s; 169372 sec
[2020-03-31 18:42:36,028 INFO] Loading train dataset from ../bert_data/cnndm.train.9019.bert.pt, number of examples: 1986
[2020-03-31 18:43:40,131 INFO] Step 102350/210000; acc:  53.04; ppl:  6.99; xent: 1.94; lr: 0.00000625;   0/411 tok/s; 169455 sec
[2020-03-31 18:45:02,299 INFO] Step 102400/210000; acc:  43.27; ppl: 12.25; xent: 2.51; lr: 0.00000625;   0/1073 tok/s; 169537 sec
[2020-03-31 18:45:10,799 INFO] Loading train dataset from ../bert_data/cnndm.train.9020.bert.pt, number of examples: 1992
[2020-03-31 18:46:24,432 INFO] Step 102450/210000; acc:  57.98; ppl:  5.80; xent: 1.76; lr: 0.00000625;   0/541 tok/s; 169619 sec
[2020-03-31 18:47:47,157 INFO] Step 102500/210000; acc:  50.38; ppl:  9.54; xent: 2.26; lr: 0.00000625;   0/1099 tok/s; 169702 sec
[2020-03-31 18:47:47,161 INFO] Saving checkpoint ../models/model_step_102500.pt
[2020-03-31 18:47:51,379 INFO] Loading train dataset from ../bert_data/cnndm.train.9021.bert.pt, number of examples: 1988
[2020-03-31 18:49:11,914 INFO] Step 102550/210000; acc:  52.69; ppl:  8.87; xent: 2.18; lr: 0.00000625;   0/541 tok/s; 169787 sec
[2020-03-31 18:50:27,847 INFO] Loading train dataset from ../bert_data/cnndm.train.9022.bert.pt, number of examples: 1991
[2020-03-31 18:50:34,354 INFO] Step 102600/210000; acc:  48.62; ppl: 10.23; xent: 2.33; lr: 0.00000624;   0/1217 tok/s; 169869 sec
[2020-03-31 18:51:56,714 INFO] Step 102650/210000; acc:  56.81; ppl:  5.74; xent: 1.75; lr: 0.00000624;   0/400 tok/s; 169952 sec
[2020-03-31 18:53:04,005 INFO] Loading train dataset from ../bert_data/cnndm.train.9023.bert.pt, number of examples: 1986
[2020-03-31 18:53:18,553 INFO] Step 102700/210000; acc:  47.25; ppl: 10.79; xent: 2.38; lr: 0.00000624;   0/896 tok/s; 170033 sec
[2020-03-31 18:54:40,746 INFO] Step 102750/210000; acc:  50.64; ppl:  8.37; xent: 2.13; lr: 0.00000624;   0/450 tok/s; 170116 sec
[2020-03-31 18:55:40,660 INFO] Loading train dataset from ../bert_data/cnndm.train.9024.bert.pt, number of examples: 1989
[2020-03-31 18:56:03,610 INFO] Step 102800/210000; acc:  47.40; ppl: 10.32; xent: 2.33; lr: 0.00000624;   0/1167 tok/s; 170198 sec
[2020-03-31 18:57:25,943 INFO] Step 102850/210000; acc:  55.65; ppl:  6.82; xent: 1.92; lr: 0.00000624;   0/531 tok/s; 170281 sec
[2020-03-31 18:58:19,275 INFO] Loading train dataset from ../bert_data/cnndm.train.9025.bert.pt, number of examples: 1984
[2020-03-31 18:58:48,574 INFO] Step 102900/210000; acc:  49.19; ppl:  9.78; xent: 2.28; lr: 0.00000623;   0/1162 tok/s; 170363 sec
[2020-03-31 19:00:10,510 INFO] Step 102950/210000; acc:  56.16; ppl:  6.53; xent: 1.88; lr: 0.00000623;   0/511 tok/s; 170445 sec
[2020-03-31 19:00:54,143 INFO] Loading train dataset from ../bert_data/cnndm.train.9026.bert.pt, number of examples: 1986
[2020-03-31 19:01:34,685 INFO] Step 103000/210000; acc:  44.58; ppl: 13.08; xent: 2.57; lr: 0.00000623;   0/1267 tok/s; 170530 sec
[2020-03-31 19:01:34,688 INFO] Saving checkpoint ../models/model_step_103000.pt
[2020-03-31 19:02:59,652 INFO] Step 103050/210000; acc:  52.45; ppl:  7.36; xent: 2.00; lr: 0.00000623;   0/594 tok/s; 170614 sec
[2020-03-31 19:03:35,328 INFO] Loading train dataset from ../bert_data/cnndm.train.9027.bert.pt, number of examples: 1989
[2020-03-31 19:04:23,027 INFO] Step 103100/210000; acc:  45.79; ppl: 11.46; xent: 2.44; lr: 0.00000623;   0/1287 tok/s; 170698 sec
[2020-03-31 19:05:45,428 INFO] Step 103150/210000; acc:  61.24; ppl:  5.11; xent: 1.63; lr: 0.00000623;   0/590 tok/s; 170780 sec
[2020-03-31 19:06:12,348 INFO] Loading train dataset from ../bert_data/cnndm.train.9028.bert.pt, number of examples: 1986
[2020-03-31 19:07:08,785 INFO] Step 103200/210000; acc:  42.62; ppl: 14.35; xent: 2.66; lr: 0.00000623;   0/1208 tok/s; 170864 sec
[2020-03-31 19:08:31,685 INFO] Step 103250/210000; acc:  52.58; ppl:  7.58; xent: 2.02; lr: 0.00000622;   0/526 tok/s; 170947 sec
[2020-03-31 19:08:49,981 INFO] Loading train dataset from ../bert_data/cnndm.train.9029.bert.pt, number of examples: 1981
[2020-03-31 19:09:54,524 INFO] Step 103300/210000; acc:  43.12; ppl: 12.65; xent: 2.54; lr: 0.00000622;   0/1146 tok/s; 171029 sec
[2020-03-31 19:11:17,353 INFO] Step 103350/210000; acc:  48.65; ppl:  8.85; xent: 2.18; lr: 0.00000622;   0/731 tok/s; 171112 sec
[2020-03-31 19:11:28,220 INFO] Loading train dataset from ../bert_data/cnndm.train.9030.bert.pt, number of examples: 1984
[2020-03-31 19:12:40,689 INFO] Step 103400/210000; acc:  54.97; ppl:  7.01; xent: 1.95; lr: 0.00000622;   0/600 tok/s; 171196 sec
[2020-03-31 19:14:03,200 INFO] Step 103450/210000; acc:  46.08; ppl: 10.65; xent: 2.37; lr: 0.00000622;   0/1112 tok/s; 171278 sec
[2020-03-31 19:14:03,516 INFO] Loading train dataset from ../bert_data/cnndm.train.9031.bert.pt, number of examples: 1975
[2020-03-31 19:15:26,022 INFO] Step 103500/210000; acc:  58.07; ppl:  5.21; xent: 1.65; lr: 0.00000622;   0/481 tok/s; 171361 sec
[2020-03-31 19:15:26,027 INFO] Saving checkpoint ../models/model_step_103500.pt
[2020-03-31 19:16:42,245 INFO] Loading train dataset from ../bert_data/cnndm.train.9032.bert.pt, number of examples: 1986
[2020-03-31 19:16:52,162 INFO] Step 103550/210000; acc:  49.41; ppl: 10.66; xent: 2.37; lr: 0.00000622;   0/690 tok/s; 171447 sec
[2020-03-31 19:18:14,555 INFO] Step 103600/210000; acc:  45.94; ppl: 11.48; xent: 2.44; lr: 0.00000621;   0/806 tok/s; 171529 sec
[2020-03-31 19:19:19,607 INFO] Loading train dataset from ../bert_data/cnndm.train.9033.bert.pt, number of examples: 1986
[2020-03-31 19:19:37,842 INFO] Step 103650/210000; acc:  58.21; ppl:  5.99; xent: 1.79; lr: 0.00000621;   0/448 tok/s; 171613 sec
[2020-03-31 19:20:59,109 INFO] Step 103700/210000; acc:  49.62; ppl:  9.33; xent: 2.23; lr: 0.00000621;   0/889 tok/s; 171694 sec
[2020-03-31 19:21:55,870 INFO] Loading train dataset from ../bert_data/cnndm.train.9034.bert.pt, number of examples: 1989
[2020-03-31 19:22:21,444 INFO] Step 103750/210000; acc:  55.27; ppl:  7.13; xent: 1.96; lr: 0.00000621;   0/433 tok/s; 171776 sec
[2020-03-31 19:23:44,228 INFO] Step 103800/210000; acc:  46.22; ppl: 11.41; xent: 2.43; lr: 0.00000621;   0/1179 tok/s; 171859 sec
[2020-03-31 19:24:31,918 INFO] Loading train dataset from ../bert_data/cnndm.train.9035.bert.pt, number of examples: 1988
[2020-03-31 19:25:06,364 INFO] Step 103850/210000; acc:  55.25; ppl:  6.58; xent: 1.88; lr: 0.00000621;   0/521 tok/s; 171941 sec
[2020-03-31 19:26:27,386 INFO] Step 103900/210000; acc:  43.40; ppl: 13.62; xent: 2.61; lr: 0.00000620;   0/1160 tok/s; 172022 sec
[2020-03-31 19:27:07,382 INFO] Loading train dataset from ../bert_data/cnndm.train.9036.bert.pt, number of examples: 1984
[2020-03-31 19:27:50,329 INFO] Step 103950/210000; acc:  50.80; ppl:  8.47; xent: 2.14; lr: 0.00000620;   0/673 tok/s; 172105 sec
[2020-03-31 19:29:14,037 INFO] Step 104000/210000; acc:  48.26; ppl: 10.35; xent: 2.34; lr: 0.00000620;   0/973 tok/s; 172189 sec
[2020-03-31 19:29:14,041 INFO] Saving checkpoint ../models/model_step_104000.pt
[2020-03-31 19:29:46,809 INFO] Loading train dataset from ../bert_data/cnndm.train.9037.bert.pt, number of examples: 1983
[2020-03-31 19:30:39,000 INFO] Step 104050/210000; acc:  47.48; ppl: 10.82; xent: 2.38; lr: 0.00000620;   0/1228 tok/s; 172274 sec
[2020-03-31 19:32:01,782 INFO] Step 104100/210000; acc:  58.04; ppl:  5.84; xent: 1.76; lr: 0.00000620;   0/439 tok/s; 172357 sec
[2020-03-31 19:32:21,819 INFO] Loading train dataset from ../bert_data/cnndm.train.9038.bert.pt, number of examples: 1982
[2020-03-31 19:33:25,155 INFO] Step 104150/210000; acc:  46.14; ppl: 11.32; xent: 2.43; lr: 0.00000620;   0/1204 tok/s; 172440 sec
[2020-03-31 19:34:48,595 INFO] Step 104200/210000; acc:  55.80; ppl:  6.27; xent: 1.84; lr: 0.00000620;   0/424 tok/s; 172523 sec
[2020-03-31 19:35:00,257 INFO] Loading train dataset from ../bert_data/cnndm.train.9039.bert.pt, number of examples: 1983
[2020-03-31 19:36:10,036 INFO] Step 104250/210000; acc:  46.35; ppl: 12.14; xent: 2.50; lr: 0.00000619;   0/1397 tok/s; 172605 sec
[2020-03-31 19:37:31,724 INFO] Step 104300/210000; acc:  51.59; ppl:  8.27; xent: 2.11; lr: 0.00000619;   0/742 tok/s; 172687 sec
[2020-03-31 19:37:35,986 INFO] Loading train dataset from ../bert_data/cnndm.train.9040.bert.pt, number of examples: 1982
[2020-03-31 19:38:56,008 INFO] Step 104350/210000; acc:  51.50; ppl:  8.36; xent: 2.12; lr: 0.00000619;   0/635 tok/s; 172771 sec
[2020-03-31 19:40:11,808 INFO] Loading train dataset from ../bert_data/cnndm.train.9041.bert.pt, number of examples: 1986
[2020-03-31 19:40:18,576 INFO] Step 104400/210000; acc:  43.74; ppl: 12.41; xent: 2.52; lr: 0.00000619;   0/1248 tok/s; 172853 sec
[2020-03-31 19:41:40,855 INFO] Step 104450/210000; acc:  59.48; ppl:  5.95; xent: 1.78; lr: 0.00000619;   0/428 tok/s; 172936 sec
[2020-03-31 19:42:48,580 INFO] Loading train dataset from ../bert_data/cnndm.train.9042.bert.pt, number of examples: 1984
[2020-03-31 19:43:03,265 INFO] Step 104500/210000; acc:  45.19; ppl: 11.57; xent: 2.45; lr: 0.00000619;   0/1136 tok/s; 173018 sec
[2020-03-31 19:43:03,268 INFO] Saving checkpoint ../models/model_step_104500.pt
[2020-03-31 19:44:27,014 INFO] Step 104550/210000; acc:  59.06; ppl:  5.75; xent: 1.75; lr: 0.00000619;   0/452 tok/s; 173102 sec
[2020-03-31 19:45:26,608 INFO] Loading train dataset from ../bert_data/cnndm.train.9043.bert.pt, number of examples: 1984
[2020-03-31 19:45:49,571 INFO] Step 104600/210000; acc:  44.63; ppl: 11.29; xent: 2.42; lr: 0.00000618;   0/1139 tok/s; 173184 sec
[2020-03-31 19:47:12,380 INFO] Step 104650/210000; acc:  54.06; ppl:  6.32; xent: 1.84; lr: 0.00000618;   0/541 tok/s; 173267 sec
[2020-03-31 19:48:04,309 INFO] Loading train dataset from ../bert_data/cnndm.train.9044.bert.pt, number of examples: 1988
[2020-03-31 19:48:35,736 INFO] Step 104700/210000; acc:  44.59; ppl: 12.01; xent: 2.49; lr: 0.00000618;   0/1469 tok/s; 173351 sec
[2020-03-31 19:49:58,122 INFO] Step 104750/210000; acc:  56.24; ppl:  5.77; xent: 1.75; lr: 0.00000618;   0/561 tok/s; 173433 sec
[2020-03-31 19:50:41,740 INFO] Loading train dataset from ../bert_data/cnndm.train.9045.bert.pt, number of examples: 1983
[2020-03-31 19:51:20,998 INFO] Step 104800/210000; acc:  48.41; ppl: 10.85; xent: 2.38; lr: 0.00000618;   0/1088 tok/s; 173516 sec
[2020-03-31 19:52:43,423 INFO] Step 104850/210000; acc:  53.64; ppl:  6.75; xent: 1.91; lr: 0.00000618;   0/695 tok/s; 173598 sec
[2020-03-31 19:53:17,080 INFO] Loading train dataset from ../bert_data/cnndm.train.9046.bert.pt, number of examples: 1983
[2020-03-31 19:54:07,081 INFO] Step 104900/210000; acc:  53.23; ppl:  8.22; xent: 2.11; lr: 0.00000618;   0/477 tok/s; 173682 sec
[2020-03-31 19:55:29,480 INFO] Step 104950/210000; acc:  45.72; ppl: 10.62; xent: 2.36; lr: 0.00000617;   0/979 tok/s; 173764 sec
[2020-03-31 19:55:55,360 INFO] Loading train dataset from ../bert_data/cnndm.train.9047.bert.pt, number of examples: 1983
[2020-03-31 19:56:53,287 INFO] Step 105000/210000; acc:  55.54; ppl:  6.33; xent: 1.85; lr: 0.00000617;   0/489 tok/s; 173848 sec
[2020-03-31 19:56:53,291 INFO] Saving checkpoint ../models/model_step_105000.pt
[2020-03-31 19:58:18,293 INFO] Step 105050/210000; acc:  45.64; ppl: 13.09; xent: 2.57; lr: 0.00000617;   0/1185 tok/s; 173933 sec
[2020-03-31 19:58:35,394 INFO] Loading train dataset from ../bert_data/cnndm.train.9048.bert.pt, number of examples: 1984
[2020-03-31 19:59:41,112 INFO] Step 105100/210000; acc:  56.66; ppl:  5.58; xent: 1.72; lr: 0.00000617;   0/506 tok/s; 174016 sec
[2020-03-31 20:01:03,785 INFO] Step 105150/210000; acc:  45.79; ppl: 13.26; xent: 2.58; lr: 0.00000617;   0/1204 tok/s; 174099 sec
[2020-03-31 20:01:10,916 INFO] Loading train dataset from ../bert_data/cnndm.train.9049.bert.pt, number of examples: 1978
[2020-03-31 20:02:27,537 INFO] Step 105200/210000; acc:  61.79; ppl:  4.83; xent: 1.57; lr: 0.00000617;   0/665 tok/s; 174182 sec
[2020-03-31 20:03:48,816 INFO] Loading train dataset from ../bert_data/cnndm.train.9050.bert.pt, number of examples: 1978
[2020-03-31 20:03:50,429 INFO] Step 105250/210000; acc:  54.04; ppl:  7.71; xent: 2.04; lr: 0.00000616;   0/440 tok/s; 174265 sec
[2020-03-31 20:05:12,798 INFO] Step 105300/210000; acc:  51.67; ppl:  8.77; xent: 2.17; lr: 0.00000616;   0/866 tok/s; 174348 sec
[2020-03-31 20:06:26,368 INFO] Loading train dataset from ../bert_data/cnndm.train.9051.bert.pt, number of examples: 1981
[2020-03-31 20:06:36,268 INFO] Step 105350/210000; acc:  60.98; ppl:  5.43; xent: 1.69; lr: 0.00000616;   0/433 tok/s; 174431 sec
[2020-03-31 20:07:58,624 INFO] Step 105400/210000; acc:  45.50; ppl: 12.06; xent: 2.49; lr: 0.00000616;   0/1298 tok/s; 174513 sec
[2020-03-31 20:09:01,437 INFO] Loading train dataset from ../bert_data/cnndm.train.9052.bert.pt, number of examples: 1981
[2020-03-31 20:09:21,174 INFO] Step 105450/210000; acc:  56.11; ppl:  6.35; xent: 1.85; lr: 0.00000616;   0/565 tok/s; 174596 sec
[2020-03-31 20:10:44,234 INFO] Step 105500/210000; acc:  45.23; ppl: 14.20; xent: 2.65; lr: 0.00000616;   0/1046 tok/s; 174679 sec
[2020-03-31 20:10:44,237 INFO] Saving checkpoint ../models/model_step_105500.pt
[2020-03-31 20:11:39,290 INFO] Loading train dataset from ../bert_data/cnndm.train.9053.bert.pt, number of examples: 1981
[2020-03-31 20:12:08,427 INFO] Step 105550/210000; acc:  49.51; ppl:  8.66; xent: 2.16; lr: 0.00000616;   0/883 tok/s; 174763 sec
[2020-03-31 20:13:30,710 INFO] Step 105600/210000; acc:  50.35; ppl:  9.39; xent: 2.24; lr: 0.00000615;   0/729 tok/s; 174846 sec
[2020-03-31 20:14:15,703 INFO] Loading train dataset from ../bert_data/cnndm.train.9054.bert.pt, number of examples: 1987
[2020-03-31 20:14:53,603 INFO] Step 105650/210000; acc:  51.36; ppl:  8.44; xent: 2.13; lr: 0.00000615;   0/1127 tok/s; 174928 sec
[2020-03-31 20:16:16,640 INFO] Step 105700/210000; acc:  62.38; ppl:  4.98; xent: 1.60; lr: 0.00000615;   0/402 tok/s; 175011 sec
[2020-03-31 20:16:53,393 INFO] Loading train dataset from ../bert_data/cnndm.train.9055.bert.pt, number of examples: 1984
[2020-03-31 20:17:39,690 INFO] Step 105750/210000; acc:  48.67; ppl: 10.31; xent: 2.33; lr: 0.00000615;   0/1127 tok/s; 175095 sec
[2020-03-31 20:19:02,236 INFO] Step 105800/210000; acc:  55.41; ppl:  7.25; xent: 1.98; lr: 0.00000615;   0/526 tok/s; 175177 sec
[2020-03-31 20:19:29,227 INFO] Loading train dataset from ../bert_data/cnndm.train.9056.bert.pt, number of examples: 1985
[2020-03-31 20:20:24,910 INFO] Step 105850/210000; acc:  43.18; ppl: 13.69; xent: 2.62; lr: 0.00000615;   0/1128 tok/s; 175260 sec
[2020-03-31 20:21:47,413 INFO] Step 105900/210000; acc:  52.15; ppl:  8.80; xent: 2.17; lr: 0.00000615;   0/666 tok/s; 175342 sec
[2020-03-31 20:22:06,145 INFO] Loading train dataset from ../bert_data/cnndm.train.9057.bert.pt, number of examples: 1986
[2020-03-31 20:23:10,524 INFO] Step 105950/210000; acc:  47.71; ppl: 10.66; xent: 2.37; lr: 0.00000614;   0/1228 tok/s; 175425 sec
[2020-03-31 20:24:34,226 INFO] Step 106000/210000; acc:  52.87; ppl:  7.90; xent: 2.07; lr: 0.00000614;   0/751 tok/s; 175509 sec
[2020-03-31 20:24:34,229 INFO] Saving checkpoint ../models/model_step_106000.pt
[2020-03-31 20:24:46,860 INFO] Loading train dataset from ../bert_data/cnndm.train.9058.bert.pt, number of examples: 1979
[2020-03-31 20:25:59,777 INFO] Step 106050/210000; acc:  55.78; ppl:  6.32; xent: 1.84; lr: 0.00000614;   0/497 tok/s; 175595 sec
[2020-03-31 20:27:22,265 INFO] Step 106100/210000; acc:  45.06; ppl: 11.90; xent: 2.48; lr: 0.00000614;   0/1203 tok/s; 175677 sec
[2020-03-31 20:27:22,567 INFO] Loading train dataset from ../bert_data/cnndm.train.9059.bert.pt, number of examples: 1987
[2020-03-31 20:28:45,360 INFO] Step 106150/210000; acc:  54.82; ppl:  7.39; xent: 2.00; lr: 0.00000614;   0/525 tok/s; 175760 sec
[2020-03-31 20:29:59,893 INFO] Loading train dataset from ../bert_data/cnndm.train.9060.bert.pt, number of examples: 1992
[2020-03-31 20:30:08,246 INFO] Step 106200/210000; acc:  41.32; ppl: 15.02; xent: 2.71; lr: 0.00000614;   0/1081 tok/s; 175843 sec
[2020-03-31 20:31:29,935 INFO] Step 106250/210000; acc:  52.64; ppl:  7.92; xent: 2.07; lr: 0.00000614;   0/728 tok/s; 175925 sec
[2020-03-31 20:32:37,013 INFO] Loading train dataset from ../bert_data/cnndm.train.9061.bert.pt, number of examples: 1985
[2020-03-31 20:32:53,682 INFO] Step 106300/210000; acc:  45.26; ppl: 11.46; xent: 2.44; lr: 0.00000613;   0/1125 tok/s; 176009 sec
[2020-03-31 20:34:15,991 INFO] Step 106350/210000; acc:  53.81; ppl:  6.47; xent: 1.87; lr: 0.00000613;   0/525 tok/s; 176091 sec
[2020-03-31 20:35:14,278 INFO] Loading train dataset from ../bert_data/cnndm.train.9062.bert.pt, number of examples: 1982
[2020-03-31 20:35:38,707 INFO] Step 106400/210000; acc:  49.14; ppl: 10.06; xent: 2.31; lr: 0.00000613;   0/1349 tok/s; 176174 sec
[2020-03-31 20:37:00,850 INFO] Step 106450/210000; acc:  56.65; ppl:  5.95; xent: 1.78; lr: 0.00000613;   0/537 tok/s; 176256 sec
[2020-03-31 20:37:52,352 INFO] Loading train dataset from ../bert_data/cnndm.train.9063.bert.pt, number of examples: 1988
[2020-03-31 20:38:23,702 INFO] Step 106500/210000; acc:  45.04; ppl: 12.58; xent: 2.53; lr: 0.00000613;   0/1263 tok/s; 176339 sec
[2020-03-31 20:38:23,706 INFO] Saving checkpoint ../models/model_step_106500.pt
[2020-03-31 20:39:48,290 INFO] Step 106550/210000; acc:  53.03; ppl:  6.95; xent: 1.94; lr: 0.00000613;   0/755 tok/s; 176423 sec
[2020-03-31 20:40:30,572 INFO] Loading train dataset from ../bert_data/cnndm.train.9064.bert.pt, number of examples: 1984
[2020-03-31 20:41:11,504 INFO] Step 106600/210000; acc:  51.32; ppl:  8.92; xent: 2.19; lr: 0.00000613;   0/760 tok/s; 176506 sec
[2020-03-31 20:42:32,857 INFO] Step 106650/210000; acc:  53.25; ppl:  6.76; xent: 1.91; lr: 0.00000612;   0/690 tok/s; 176588 sec
[2020-03-31 20:43:06,132 INFO] Loading train dataset from ../bert_data/cnndm.train.9065.bert.pt, number of examples: 1986
[2020-03-31 20:43:56,144 INFO] Step 106700/210000; acc:  58.80; ppl:  6.08; xent: 1.81; lr: 0.00000612;   0/509 tok/s; 176671 sec
[2020-03-31 20:45:17,743 INFO] Step 106750/210000; acc:  47.49; ppl: 11.12; xent: 2.41; lr: 0.00000612;   0/1119 tok/s; 176753 sec
[2020-03-31 20:45:42,867 INFO] Loading train dataset from ../bert_data/cnndm.train.9066.bert.pt, number of examples: 1988
[2020-03-31 20:46:40,864 INFO] Step 106800/210000; acc:  56.68; ppl:  6.07; xent: 1.80; lr: 0.00000612;   0/473 tok/s; 176836 sec
[2020-03-31 20:48:02,531 INFO] Step 106850/210000; acc:  46.96; ppl: 10.02; xent: 2.30; lr: 0.00000612;   0/868 tok/s; 176917 sec
[2020-03-31 20:48:19,750 INFO] Loading train dataset from ../bert_data/cnndm.train.9067.bert.pt, number of examples: 1986
[2020-03-31 20:49:25,091 INFO] Step 106900/210000; acc:  54.76; ppl:  6.34; xent: 1.85; lr: 0.00000612;   0/483 tok/s; 177000 sec
[2020-03-31 20:50:47,373 INFO] Step 106950/210000; acc:  51.35; ppl:  8.70; xent: 2.16; lr: 0.00000612;   0/962 tok/s; 177082 sec
[2020-03-31 20:50:56,312 INFO] Loading train dataset from ../bert_data/cnndm.train.9068.bert.pt, number of examples: 1988
[2020-03-31 20:52:10,607 INFO] Step 107000/210000; acc:  59.85; ppl:  5.25; xent: 1.66; lr: 0.00000611;   0/482 tok/s; 177165 sec
[2020-03-31 20:52:10,610 INFO] Saving checkpoint ../models/model_step_107000.pt
[2020-03-31 20:53:35,466 INFO] Step 107050/210000; acc:  44.00; ppl: 12.35; xent: 2.51; lr: 0.00000611;   0/1121 tok/s; 177250 sec
[2020-03-31 20:53:35,766 INFO] Loading train dataset from ../bert_data/cnndm.train.9069.bert.pt, number of examples: 1982
[2020-03-31 20:54:58,475 INFO] Step 107100/210000; acc:  48.39; ppl:  8.40; xent: 2.13; lr: 0.00000611;   0/707 tok/s; 177333 sec
[2020-03-31 20:56:11,542 INFO] Loading train dataset from ../bert_data/cnndm.train.9070.bert.pt, number of examples: 1988
[2020-03-31 20:56:21,550 INFO] Step 107150/210000; acc:  52.51; ppl:  7.93; xent: 2.07; lr: 0.00000611;   0/567 tok/s; 177416 sec
[2020-03-31 20:57:43,115 INFO] Step 107200/210000; acc:  51.07; ppl:  8.55; xent: 2.15; lr: 0.00000611;   0/1085 tok/s; 177498 sec
[2020-03-31 20:58:48,277 INFO] Loading train dataset from ../bert_data/cnndm.train.9071.bert.pt, number of examples: 1988
[2020-03-31 20:59:06,105 INFO] Step 107250/210000; acc:  54.55; ppl:  7.16; xent: 1.97; lr: 0.00000611;   0/437 tok/s; 177581 sec
[2020-03-31 21:00:28,691 INFO] Step 107300/210000; acc:  45.91; ppl: 11.08; xent: 2.41; lr: 0.00000611;   0/896 tok/s; 177664 sec
[2020-03-31 21:01:25,307 INFO] Loading train dataset from ../bert_data/cnndm.train.9072.bert.pt, number of examples: 1992
[2020-03-31 21:01:51,740 INFO] Step 107350/210000; acc:  61.86; ppl:  5.01; xent: 1.61; lr: 0.00000610;   0/489 tok/s; 177747 sec
[2020-03-31 21:03:14,089 INFO] Step 107400/210000; acc:  48.53; ppl:  9.45; xent: 2.25; lr: 0.00000610;   0/1078 tok/s; 177829 sec
[2020-03-31 21:04:02,516 INFO] Loading train dataset from ../bert_data/cnndm.train.9073.bert.pt, number of examples: 1987
[2020-03-31 21:04:37,209 INFO] Step 107450/210000; acc:  59.30; ppl:  5.01; xent: 1.61; lr: 0.00000610;   0/486 tok/s; 177912 sec
[2020-03-31 21:06:00,068 INFO] Step 107500/210000; acc:  44.97; ppl: 11.72; xent: 2.46; lr: 0.00000610;   0/1073 tok/s; 177995 sec
[2020-03-31 21:06:00,073 INFO] Saving checkpoint ../models/model_step_107500.pt
[2020-03-31 21:06:42,287 INFO] Loading train dataset from ../bert_data/cnndm.train.9074.bert.pt, number of examples: 1991
[2020-03-31 21:07:25,140 INFO] Step 107550/210000; acc:  62.00; ppl:  5.14; xent: 1.64; lr: 0.00000610;   0/525 tok/s; 178080 sec
[2020-03-31 21:08:47,957 INFO] Step 107600/210000; acc:  43.83; ppl: 13.53; xent: 2.61; lr: 0.00000610;   0/1441 tok/s; 178163 sec
[2020-03-31 21:09:19,576 INFO] Loading train dataset from ../bert_data/cnndm.train.9075.bert.pt, number of examples: 1983
[2020-03-31 21:10:11,111 INFO] Step 107650/210000; acc:  57.77; ppl:  6.33; xent: 1.85; lr: 0.00000610;   0/623 tok/s; 178246 sec
[2020-03-31 21:11:33,438 INFO] Step 107700/210000; acc:  46.44; ppl: 11.32; xent: 2.43; lr: 0.00000609;   0/1272 tok/s; 178328 sec
[2020-03-31 21:11:56,579 INFO] Loading train dataset from ../bert_data/cnndm.train.9076.bert.pt, number of examples: 1986
[2020-03-31 21:12:55,365 INFO] Step 107750/210000; acc:  57.74; ppl:  6.02; xent: 1.80; lr: 0.00000609;   0/602 tok/s; 178410 sec
[2020-03-31 21:14:17,800 INFO] Step 107800/210000; acc:  47.15; ppl: 11.38; xent: 2.43; lr: 0.00000609;   0/1279 tok/s; 178493 sec
[2020-03-31 21:14:33,274 INFO] Loading train dataset from ../bert_data/cnndm.train.9077.bert.pt, number of examples: 1990
[2020-03-31 21:15:40,856 INFO] Step 107850/210000; acc:  54.23; ppl:  6.48; xent: 1.87; lr: 0.00000609;   0/743 tok/s; 178576 sec
[2020-03-31 21:17:03,570 INFO] Step 107900/210000; acc:  53.85; ppl:  8.27; xent: 2.11; lr: 0.00000609;   0/699 tok/s; 178658 sec
[2020-03-31 21:17:08,611 INFO] Loading train dataset from ../bert_data/cnndm.train.9078.bert.pt, number of examples: 1992
[2020-03-31 21:18:25,413 INFO] Step 107950/210000; acc:  50.89; ppl:  8.99; xent: 2.20; lr: 0.00000609;   0/890 tok/s; 178740 sec
[2020-03-31 21:19:46,644 INFO] Loading train dataset from ../bert_data/cnndm.train.9079.bert.pt, number of examples: 1984
[2020-03-31 21:19:48,527 INFO] Step 108000/210000; acc:  51.30; ppl:  8.50; xent: 2.14; lr: 0.00000609;   0/806 tok/s; 178823 sec
[2020-03-31 21:19:48,529 INFO] Saving checkpoint ../models/model_step_108000.pt
[2020-03-31 21:21:13,366 INFO] Step 108050/210000; acc:  48.39; ppl:  9.62; xent: 2.26; lr: 0.00000608;   0/773 tok/s; 178908 sec
[2020-03-31 21:22:26,346 INFO] Loading train dataset from ../bert_data/cnndm.train.9080.bert.pt, number of examples: 1988
[2020-03-31 21:22:36,530 INFO] Step 108100/210000; acc:  59.02; ppl:  5.60; xent: 1.72; lr: 0.00000608;   0/491 tok/s; 178991 sec
[2020-03-31 21:23:57,943 INFO] Step 108150/210000; acc:  50.03; ppl:  9.45; xent: 2.25; lr: 0.00000608;   0/1212 tok/s; 179073 sec
[2020-03-31 21:25:02,717 INFO] Loading train dataset from ../bert_data/cnndm.train.9081.bert.pt, number of examples: 1991
[2020-03-31 21:25:21,014 INFO] Step 108200/210000; acc:  59.35; ppl:  5.14; xent: 1.64; lr: 0.00000608;   0/472 tok/s; 179156 sec
[2020-03-31 21:26:43,377 INFO] Step 108250/210000; acc:  45.20; ppl: 13.13; xent: 2.57; lr: 0.00000608;   0/1318 tok/s; 179238 sec
[2020-03-31 21:27:40,196 INFO] Loading train dataset from ../bert_data/cnndm.train.9082.bert.pt, number of examples: 1985
[2020-03-31 21:28:06,337 INFO] Step 108300/210000; acc:  55.00; ppl:  6.77; xent: 1.91; lr: 0.00000608;   0/566 tok/s; 179321 sec
[2020-03-31 21:29:28,487 INFO] Step 108350/210000; acc:  40.63; ppl: 16.10; xent: 2.78; lr: 0.00000608;   0/1217 tok/s; 179403 sec
[2020-03-31 21:30:14,459 INFO] Loading train dataset from ../bert_data/cnndm.train.9083.bert.pt, number of examples: 1986
[2020-03-31 21:30:50,842 INFO] Step 108400/210000; acc:  53.08; ppl:  6.86; xent: 1.93; lr: 0.00000607;   0/598 tok/s; 179486 sec
[2020-03-31 21:32:13,342 INFO] Step 108450/210000; acc:  47.08; ppl: 10.79; xent: 2.38; lr: 0.00000607;   0/1178 tok/s; 179568 sec
[2020-03-31 21:32:51,927 INFO] Loading train dataset from ../bert_data/cnndm.train.9084.bert.pt, number of examples: 1982
[2020-03-31 21:33:36,716 INFO] Step 108500/210000; acc:  52.59; ppl:  7.07; xent: 1.96; lr: 0.00000607;   0/599 tok/s; 179652 sec
[2020-03-31 21:33:36,719 INFO] Saving checkpoint ../models/model_step_108500.pt
[2020-03-31 21:35:01,017 INFO] Step 108550/210000; acc:  43.63; ppl: 14.58; xent: 2.68; lr: 0.00000607;   0/1477 tok/s; 179736 sec
[2020-03-31 21:35:30,835 INFO] Loading train dataset from ../bert_data/cnndm.train.9085.bert.pt, number of examples: 1985
[2020-03-31 21:36:23,535 INFO] Step 108600/210000; acc:  53.59; ppl:  7.26; xent: 1.98; lr: 0.00000607;   0/734 tok/s; 179818 sec
[2020-03-31 21:37:46,496 INFO] Step 108650/210000; acc:  46.59; ppl: 11.42; xent: 2.44; lr: 0.00000607;   0/1224 tok/s; 179901 sec
[2020-03-31 21:38:08,115 INFO] Loading train dataset from ../bert_data/cnndm.train.9086.bert.pt, number of examples: 1983
[2020-03-31 21:39:09,261 INFO] Step 108700/210000; acc:  54.23; ppl:  7.43; xent: 2.01; lr: 0.00000607;   0/809 tok/s; 179984 sec
[2020-03-31 21:40:32,234 INFO] Step 108750/210000; acc:  46.41; ppl: 11.14; xent: 2.41; lr: 0.00000606;   0/782 tok/s; 180067 sec
[2020-03-31 21:40:45,674 INFO] Loading train dataset from ../bert_data/cnndm.train.9087.bert.pt, number of examples: 1986
[2020-03-31 21:41:54,735 INFO] Step 108800/210000; acc:  48.98; ppl:  9.15; xent: 2.21; lr: 0.00000606;   0/968 tok/s; 180150 sec
[2020-03-31 21:43:17,504 INFO] Step 108850/210000; acc:  53.32; ppl:  7.37; xent: 2.00; lr: 0.00000606;   0/744 tok/s; 180232 sec
[2020-03-31 21:43:23,020 INFO] Loading train dataset from ../bert_data/cnndm.train.9088.bert.pt, number of examples: 1986
[2020-03-31 21:44:40,510 INFO] Step 108900/210000; acc:  47.53; ppl:  9.68; xent: 2.27; lr: 0.00000606;   0/1133 tok/s; 180315 sec
[2020-03-31 21:46:01,057 INFO] Loading train dataset from ../bert_data/cnndm.train.9089.bert.pt, number of examples: 1986
[2020-03-31 21:46:04,349 INFO] Step 108950/210000; acc:  55.92; ppl:  6.85; xent: 1.92; lr: 0.00000606;   0/526 tok/s; 180399 sec
[2020-03-31 21:47:26,947 INFO] Step 109000/210000; acc:  47.57; ppl: 10.49; xent: 2.35; lr: 0.00000606;   0/1121 tok/s; 180482 sec
[2020-03-31 21:47:26,951 INFO] Saving checkpoint ../models/model_step_109000.pt
[2020-03-31 21:48:40,514 INFO] Loading train dataset from ../bert_data/cnndm.train.9090.bert.pt, number of examples: 20
[2020-03-31 21:48:42,780 INFO] Loading train dataset from ../bert_data/cnndm.train.9100.bert.pt, number of examples: 1988
[2020-03-31 21:48:51,353 INFO] Step 109050/210000; acc:  50.44; ppl:  8.52; xent: 2.14; lr: 0.00000606;   0/606 tok/s; 180566 sec
[2020-03-31 21:50:04,070 INFO] Step 109100/210000; acc:  56.51; ppl:  5.47; xent: 1.70; lr: 0.00000606;   0/397 tok/s; 180639 sec
[2020-03-31 21:50:58,658 INFO] Loading train dataset from ../bert_data/cnndm.train.9101.bert.pt, number of examples: 1983
[2020-03-31 21:51:15,994 INFO] Step 109150/210000; acc:  65.86; ppl:  4.31; xent: 1.46; lr: 0.00000605;   0/417 tok/s; 180711 sec
[2020-03-31 21:52:27,375 INFO] Step 109200/210000; acc:  56.96; ppl:  6.09; xent: 1.81; lr: 0.00000605;   0/735 tok/s; 180782 sec
[2020-03-31 21:53:15,842 INFO] Loading train dataset from ../bert_data/cnndm.train.9102.bert.pt, number of examples: 1992
[2020-03-31 21:53:40,952 INFO] Step 109250/210000; acc:  65.27; ppl:  4.64; xent: 1.54; lr: 0.00000605;   0/587 tok/s; 180856 sec
[2020-03-31 21:54:54,543 INFO] Step 109300/210000; acc:  65.44; ppl:  4.11; xent: 1.41; lr: 0.00000605;   0/481 tok/s; 180929 sec
[2020-03-31 21:55:35,532 INFO] Loading train dataset from ../bert_data/cnndm.train.9103.bert.pt, number of examples: 1978
[2020-03-31 21:56:07,550 INFO] Step 109350/210000; acc:  62.82; ppl:  4.59; xent: 1.52; lr: 0.00000605;   0/477 tok/s; 181002 sec
[2020-03-31 21:57:21,234 INFO] Step 109400/210000; acc:  55.39; ppl:  7.05; xent: 1.95; lr: 0.00000605;   0/532 tok/s; 181076 sec
[2020-03-31 21:57:53,746 INFO] Loading train dataset from ../bert_data/cnndm.train.9104.bert.pt, number of examples: 1986
[2020-03-31 21:58:35,032 INFO] Step 109450/210000; acc:  60.14; ppl:  5.25; xent: 1.66; lr: 0.00000605;   0/466 tok/s; 181150 sec
[2020-03-31 21:59:47,446 INFO] Step 109500/210000; acc:  62.48; ppl:  5.15; xent: 1.64; lr: 0.00000604;   0/526 tok/s; 181222 sec
[2020-03-31 21:59:47,449 INFO] Saving checkpoint ../models/model_step_109500.pt
[2020-03-31 22:00:15,013 INFO] Loading train dataset from ../bert_data/cnndm.train.9105.bert.pt, number of examples: 1988
[2020-03-31 22:01:02,874 INFO] Step 109550/210000; acc:  65.13; ppl:  4.73; xent: 1.55; lr: 0.00000604;   0/586 tok/s; 181298 sec
[2020-03-31 22:02:16,107 INFO] Step 109600/210000; acc:  59.59; ppl:  5.66; xent: 1.73; lr: 0.00000604;   0/625 tok/s; 181371 sec
[2020-03-31 22:02:33,816 INFO] Loading train dataset from ../bert_data/cnndm.train.9106.bert.pt, number of examples: 1978
[2020-03-31 22:03:29,549 INFO] Step 109650/210000; acc:  65.29; ppl:  4.22; xent: 1.44; lr: 0.00000604;   0/372 tok/s; 181444 sec
[2020-03-31 22:04:42,943 INFO] Step 109700/210000; acc:  58.85; ppl:  6.17; xent: 1.82; lr: 0.00000604;   0/689 tok/s; 181518 sec
[2020-03-31 22:04:52,524 INFO] Loading train dataset from ../bert_data/cnndm.train.9107.bert.pt, number of examples: 1985
[2020-03-31 22:05:56,652 INFO] Step 109750/210000; acc:  55.22; ppl:  6.42; xent: 1.86; lr: 0.00000604;   0/650 tok/s; 181591 sec
[2020-03-31 22:07:09,173 INFO] Step 109800/210000; acc:  59.08; ppl:  5.83; xent: 1.76; lr: 0.00000604;   0/609 tok/s; 181664 sec
[2020-03-31 22:07:10,992 INFO] Loading train dataset from ../bert_data/cnndm.train.9108.bert.pt, number of examples: 1989
[2020-03-31 22:08:22,706 INFO] Step 109850/210000; acc:  71.68; ppl:  3.45; xent: 1.24; lr: 0.00000603;   0/421 tok/s; 181738 sec
[2020-03-31 22:09:29,382 INFO] Loading train dataset from ../bert_data/cnndm.train.9109.bert.pt, number of examples: 1987
[2020-03-31 22:09:35,172 INFO] Step 109900/210000; acc:  68.60; ppl:  3.64; xent: 1.29; lr: 0.00000603;   0/519 tok/s; 181810 sec
[2020-03-31 22:10:48,873 INFO] Step 109950/210000; acc:  60.92; ppl:  4.87; xent: 1.58; lr: 0.00000603;   0/439 tok/s; 181884 sec
[2020-03-31 22:11:47,095 INFO] Loading train dataset from ../bert_data/cnndm.train.9110.bert.pt, number of examples: 1985
[2020-03-31 22:12:01,771 INFO] Step 110000/210000; acc:  71.15; ppl:  3.31; xent: 1.20; lr: 0.00000603;   0/403 tok/s; 181957 sec
[2020-03-31 22:12:01,773 INFO] Saving checkpoint ../models/model_step_110000.pt
[2020-03-31 22:13:16,722 INFO] Step 110050/210000; acc:  60.23; ppl:  5.20; xent: 1.65; lr: 0.00000603;   0/740 tok/s; 182032 sec
[2020-03-31 22:14:08,449 INFO] Loading train dataset from ../bert_data/cnndm.train.9111.bert.pt, number of examples: 1985
[2020-03-31 22:14:30,513 INFO] Step 110100/210000; acc:  58.44; ppl:  5.86; xent: 1.77; lr: 0.00000603;   0/589 tok/s; 182105 sec
[2020-03-31 22:15:43,362 INFO] Step 110150/210000; acc:  66.87; ppl:  3.68; xent: 1.30; lr: 0.00000603;   0/480 tok/s; 182178 sec
[2020-03-31 22:16:27,890 INFO] Loading train dataset from ../bert_data/cnndm.train.9112.bert.pt, number of examples: 1983
[2020-03-31 22:16:57,409 INFO] Step 110200/210000; acc:  72.81; ppl:  2.95; xent: 1.08; lr: 0.00000602;   0/386 tok/s; 182252 sec
[2020-03-31 22:18:09,744 INFO] Step 110250/210000; acc:  63.33; ppl:  4.61; xent: 1.53; lr: 0.00000602;   0/611 tok/s; 182325 sec
[2020-03-31 22:18:44,761 INFO] Loading train dataset from ../bert_data/cnndm.train.9113.bert.pt, number of examples: 1979
[2020-03-31 22:19:22,390 INFO] Step 110300/210000; acc:  59.09; ppl:  5.78; xent: 1.75; lr: 0.00000602;   0/674 tok/s; 182397 sec
[2020-03-31 22:20:35,856 INFO] Step 110350/210000; acc:  67.36; ppl:  3.70; xent: 1.31; lr: 0.00000602;   0/503 tok/s; 182471 sec
[2020-03-31 22:21:04,514 INFO] Loading train dataset from ../bert_data/cnndm.train.9114.bert.pt, number of examples: 1989
[2020-03-31 22:21:50,039 INFO] Step 110400/210000; acc:  64.93; ppl:  4.42; xent: 1.49; lr: 0.00000602;   0/550 tok/s; 182545 sec
[2020-03-31 22:23:02,772 INFO] Step 110450/210000; acc:  69.59; ppl:  3.52; xent: 1.26; lr: 0.00000602;   0/452 tok/s; 182618 sec
[2020-03-31 22:23:23,464 INFO] Loading train dataset from ../bert_data/cnndm.train.9115.bert.pt, number of examples: 1989
[2020-03-31 22:24:15,433 INFO] Step 110500/210000; acc:  64.62; ppl:  4.18; xent: 1.43; lr: 0.00000602;   0/677 tok/s; 182690 sec
[2020-03-31 22:24:15,436 INFO] Saving checkpoint ../models/model_step_110500.pt
[2020-03-31 22:25:30,699 INFO] Step 110550/210000; acc:  65.56; ppl:  4.27; xent: 1.45; lr: 0.00000602;   0/543 tok/s; 182766 sec
[2020-03-31 22:25:42,195 INFO] Loading train dataset from ../bert_data/cnndm.train.9116.bert.pt, number of examples: 1985
[2020-03-31 22:26:43,849 INFO] Step 110600/210000; acc:  65.22; ppl:  4.49; xent: 1.50; lr: 0.00000601;   0/589 tok/s; 182839 sec
[2020-03-31 22:27:56,922 INFO] Step 110650/210000; acc:  62.53; ppl:  4.45; xent: 1.49; lr: 0.00000601;   0/589 tok/s; 182912 sec
[2020-03-31 22:28:01,896 INFO] Loading train dataset from ../bert_data/cnndm.train.9117.bert.pt, number of examples: 1979
[2020-03-31 22:29:10,070 INFO] Step 110700/210000; acc:  60.55; ppl:  5.12; xent: 1.63; lr: 0.00000601;   0/518 tok/s; 182985 sec
[2020-03-31 22:30:19,846 INFO] Loading train dataset from ../bert_data/cnndm.train.9118.bert.pt, number of examples: 1980
[2020-03-31 22:30:22,592 INFO] Step 110750/210000; acc:  76.34; ppl:  2.76; xent: 1.01; lr: 0.00000601;   0/453 tok/s; 183057 sec
[2020-03-31 22:31:35,411 INFO] Step 110800/210000; acc:  60.40; ppl:  4.98; xent: 1.61; lr: 0.00000601;   0/741 tok/s; 183130 sec
[2020-03-31 22:32:38,879 INFO] Loading train dataset from ../bert_data/cnndm.train.9119.bert.pt, number of examples: 1985
[2020-03-31 22:32:48,701 INFO] Step 110850/210000; acc:  65.17; ppl:  4.28; xent: 1.46; lr: 0.00000601;   0/605 tok/s; 183204 sec
[2020-03-31 22:34:01,626 INFO] Step 110900/210000; acc:  68.81; ppl:  3.62; xent: 1.29; lr: 0.00000601;   0/464 tok/s; 183276 sec
[2020-03-31 22:34:55,742 INFO] Loading train dataset from ../bert_data/cnndm.train.9120.bert.pt, number of examples: 1981
[2020-03-31 22:35:14,896 INFO] Step 110950/210000; acc:  66.67; ppl:  3.87; xent: 1.35; lr: 0.00000600;   0/480 tok/s; 183350 sec
[2020-03-31 22:36:27,993 INFO] Step 111000/210000; acc:  66.63; ppl:  3.99; xent: 1.38; lr: 0.00000600;   0/659 tok/s; 183423 sec
[2020-03-31 22:36:27,997 INFO] Saving checkpoint ../models/model_step_111000.pt
[2020-03-31 22:37:17,143 INFO] Loading train dataset from ../bert_data/cnndm.train.9121.bert.pt, number of examples: 1985
[2020-03-31 22:37:43,166 INFO] Step 111050/210000; acc:  60.38; ppl:  5.61; xent: 1.72; lr: 0.00000600;   0/619 tok/s; 183498 sec
[2020-03-31 22:38:57,657 INFO] Step 111100/210000; acc:  64.97; ppl:  4.32; xent: 1.46; lr: 0.00000600;   0/511 tok/s; 183572 sec
[2020-03-31 22:39:36,004 INFO] Loading train dataset from ../bert_data/cnndm.train.9122.bert.pt, number of examples: 1986
[2020-03-31 22:40:11,129 INFO] Step 111150/210000; acc:  64.64; ppl:  4.53; xent: 1.51; lr: 0.00000600;   0/545 tok/s; 183646 sec
[2020-03-31 22:41:24,576 INFO] Step 111200/210000; acc:  63.43; ppl:  4.56; xent: 1.52; lr: 0.00000600;   0/542 tok/s; 183719 sec
[2020-03-31 22:41:55,396 INFO] Loading train dataset from ../bert_data/cnndm.train.9123.bert.pt, number of examples: 1983
[2020-03-31 22:42:38,098 INFO] Step 111250/210000; acc:  67.02; ppl:  4.17; xent: 1.43; lr: 0.00000600;   0/493 tok/s; 183793 sec
[2020-03-31 22:43:50,922 INFO] Step 111300/210000; acc:  70.40; ppl:  3.28; xent: 1.19; lr: 0.00000599;   0/413 tok/s; 183866 sec
[2020-03-31 22:44:13,120 INFO] Loading train dataset from ../bert_data/cnndm.train.9124.bert.pt, number of examples: 1983
[2020-03-31 22:45:04,021 INFO] Step 111350/210000; acc:  62.41; ppl:  5.19; xent: 1.65; lr: 0.00000599;   0/690 tok/s; 183939 sec
[2020-03-31 22:46:16,222 INFO] Step 111400/210000; acc:  65.21; ppl:  4.06; xent: 1.40; lr: 0.00000599;   0/558 tok/s; 184011 sec
[2020-03-31 22:46:30,724 INFO] Loading train dataset from ../bert_data/cnndm.train.9125.bert.pt, number of examples: 1991
[2020-03-31 22:47:28,396 INFO] Step 111450/210000; acc:  65.95; ppl:  3.84; xent: 1.34; lr: 0.00000599;   0/578 tok/s; 184083 sec
[2020-03-31 22:48:41,464 INFO] Step 111500/210000; acc:  75.38; ppl:  2.86; xent: 1.05; lr: 0.00000599;   0/476 tok/s; 184156 sec
[2020-03-31 22:48:41,467 INFO] Saving checkpoint ../models/model_step_111500.pt
[2020-03-31 22:48:51,218 INFO] Loading train dataset from ../bert_data/cnndm.train.9126.bert.pt, number of examples: 1985
[2020-03-31 22:49:56,325 INFO] Step 111550/210000; acc:  62.32; ppl:  5.00; xent: 1.61; lr: 0.00000599;   0/751 tok/s; 184231 sec
[2020-03-31 22:51:08,190 INFO] Loading train dataset from ../bert_data/cnndm.train.9127.bert.pt, number of examples: 1980
[2020-03-31 22:51:09,864 INFO] Step 111600/210000; acc:  65.67; ppl:  4.20; xent: 1.44; lr: 0.00000599;   0/484 tok/s; 184305 sec
[2020-03-31 22:52:22,251 INFO] Step 111650/210000; acc:  63.03; ppl:  4.86; xent: 1.58; lr: 0.00000599;   0/571 tok/s; 184377 sec
[2020-03-31 22:53:25,273 INFO] Loading train dataset from ../bert_data/cnndm.train.9128.bert.pt, number of examples: 1981
[2020-03-31 22:53:35,582 INFO] Step 111700/210000; acc:  62.47; ppl:  5.30; xent: 1.67; lr: 0.00000598;   0/603 tok/s; 184450 sec
[2020-03-31 22:54:48,117 INFO] Step 111750/210000; acc:  70.05; ppl:  3.56; xent: 1.27; lr: 0.00000598;   0/579 tok/s; 184523 sec
[2020-03-31 22:55:42,974 INFO] Loading train dataset from ../bert_data/cnndm.train.9129.bert.pt, number of examples: 1987
[2020-03-31 22:56:00,211 INFO] Step 111800/210000; acc:  73.67; ppl:  3.04; xent: 1.11; lr: 0.00000598;   0/435 tok/s; 184595 sec
[2020-03-31 22:57:13,152 INFO] Step 111850/210000; acc:  61.89; ppl:  5.40; xent: 1.69; lr: 0.00000598;   0/584 tok/s; 184668 sec
[2020-03-31 22:58:01,621 INFO] Loading train dataset from ../bert_data/cnndm.train.9130.bert.pt, number of examples: 1986
[2020-03-31 22:58:26,257 INFO] Step 111900/210000; acc:  62.10; ppl:  4.80; xent: 1.57; lr: 0.00000598;   0/606 tok/s; 184741 sec
[2020-03-31 22:59:39,497 INFO] Step 111950/210000; acc:  63.17; ppl:  4.15; xent: 1.42; lr: 0.00000598;   0/577 tok/s; 184814 sec
[2020-03-31 23:00:20,822 INFO] Loading train dataset from ../bert_data/cnndm.train.9131.bert.pt, number of examples: 1984
[2020-03-31 23:00:52,803 INFO] Step 112000/210000; acc:  66.53; ppl:  3.91; xent: 1.36; lr: 0.00000598;   0/503 tok/s; 184888 sec
[2020-03-31 23:00:52,816 INFO] Saving checkpoint ../models/model_step_112000.pt
[2020-03-31 23:02:09,178 INFO] Step 112050/210000; acc:  63.86; ppl:  4.49; xent: 1.50; lr: 0.00000597;   0/479 tok/s; 184964 sec
[2020-03-31 23:02:42,695 INFO] Loading train dataset from ../bert_data/cnndm.train.9132.bert.pt, number of examples: 1990
[2020-03-31 23:03:21,499 INFO] Step 112100/210000; acc:  60.44; ppl:  5.63; xent: 1.73; lr: 0.00000597;   0/601 tok/s; 185036 sec
[2020-03-31 23:04:34,596 INFO] Step 112150/210000; acc:  63.74; ppl:  4.33; xent: 1.47; lr: 0.00000597;   0/561 tok/s; 185109 sec
[2020-03-31 23:05:01,292 INFO] Loading train dataset from ../bert_data/cnndm.train.9133.bert.pt, number of examples: 1978
[2020-03-31 23:05:47,598 INFO] Step 112200/210000; acc:  72.53; ppl:  3.39; xent: 1.22; lr: 0.00000597;   0/563 tok/s; 185182 sec
[2020-03-31 23:07:00,339 INFO] Step 112250/210000; acc:  73.39; ppl:  3.20; xent: 1.16; lr: 0.00000597;   0/417 tok/s; 185255 sec
[2020-03-31 23:07:17,624 INFO] Loading train dataset from ../bert_data/cnndm.train.9134.bert.pt, number of examples: 156
[2020-03-31 23:07:28,307 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-03-31 23:08:20,328 INFO] Step 112300/210000; acc:  46.94; ppl: 16.68; xent: 2.81; lr: 0.00000597;   0/872 tok/s; 185335 sec
[2020-03-31 23:09:44,515 INFO] Step 112350/210000; acc:  45.33; ppl: 16.70; xent: 2.82; lr: 0.00000597;   0/881 tok/s; 185419 sec
[2020-03-31 23:10:15,700 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-03-31 23:11:09,647 INFO] Step 112400/210000; acc:  47.57; ppl: 13.56; xent: 2.61; lr: 0.00000597;   0/566 tok/s; 185504 sec
[2020-03-31 23:12:33,402 INFO] Step 112450/210000; acc:  48.00; ppl: 12.47; xent: 2.52; lr: 0.00000596;   0/527 tok/s; 185588 sec
[2020-03-31 23:13:04,042 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-03-31 23:13:57,932 INFO] Step 112500/210000; acc:  45.93; ppl: 15.11; xent: 2.72; lr: 0.00000596;   0/608 tok/s; 185673 sec
[2020-03-31 23:13:57,935 INFO] Saving checkpoint ../models/model_step_112500.pt
[2020-03-31 23:15:24,424 INFO] Step 112550/210000; acc:  51.86; ppl: 10.03; xent: 2.31; lr: 0.00000596;   0/549 tok/s; 185759 sec
[2020-03-31 23:15:53,494 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-03-31 23:16:49,007 INFO] Step 112600/210000; acc:  48.69; ppl: 13.12; xent: 2.57; lr: 0.00000596;   0/597 tok/s; 185844 sec
[2020-03-31 23:18:12,982 INFO] Step 112650/210000; acc:  54.22; ppl:  8.60; xent: 2.15; lr: 0.00000596;   0/600 tok/s; 185928 sec
[2020-03-31 23:18:40,480 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-03-31 23:19:37,502 INFO] Step 112700/210000; acc:  47.60; ppl: 13.80; xent: 2.62; lr: 0.00000596;   0/769 tok/s; 186012 sec
[2020-03-31 23:21:01,766 INFO] Step 112750/210000; acc:  47.04; ppl: 14.06; xent: 2.64; lr: 0.00000596;   0/651 tok/s; 186097 sec
[2020-03-31 23:21:29,499 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-03-31 23:22:27,012 INFO] Step 112800/210000; acc:  44.27; ppl: 17.14; xent: 2.84; lr: 0.00000595;   0/792 tok/s; 186182 sec
[2020-03-31 23:23:51,546 INFO] Step 112850/210000; acc:  48.81; ppl: 13.23; xent: 2.58; lr: 0.00000595;   0/808 tok/s; 186266 sec
[2020-03-31 23:24:17,760 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-03-31 23:25:16,573 INFO] Step 112900/210000; acc:  52.34; ppl:  9.94; xent: 2.30; lr: 0.00000595;   0/884 tok/s; 186351 sec
[2020-03-31 23:26:40,593 INFO] Step 112950/210000; acc:  47.03; ppl: 15.64; xent: 2.75; lr: 0.00000595;   0/686 tok/s; 186435 sec
[2020-03-31 23:27:05,017 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-03-31 23:28:05,971 INFO] Step 113000/210000; acc:  47.48; ppl: 13.89; xent: 2.63; lr: 0.00000595;   0/750 tok/s; 186521 sec
[2020-03-31 23:28:05,974 INFO] Saving checkpoint ../models/model_step_113000.pt
[2020-03-31 23:29:31,902 INFO] Step 113050/210000; acc:  51.99; ppl: 10.87; xent: 2.39; lr: 0.00000595;   0/905 tok/s; 186607 sec
[2020-03-31 23:29:56,179 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-03-31 23:30:56,817 INFO] Step 113100/210000; acc:  53.00; ppl:  9.76; xent: 2.28; lr: 0.00000595;   0/523 tok/s; 186692 sec
[2020-03-31 23:32:20,872 INFO] Step 113150/210000; acc:  57.85; ppl:  7.19; xent: 1.97; lr: 0.00000595;   0/676 tok/s; 186776 sec
[2020-03-31 23:32:43,165 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-03-31 23:33:45,891 INFO] Step 113200/210000; acc:  48.33; ppl: 11.09; xent: 2.41; lr: 0.00000594;   0/517 tok/s; 186861 sec
[2020-03-31 23:35:10,063 INFO] Step 113250/210000; acc:  49.26; ppl: 11.89; xent: 2.48; lr: 0.00000594;   0/731 tok/s; 186945 sec
[2020-03-31 23:35:30,742 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-03-31 23:36:34,007 INFO] Step 113300/210000; acc:  51.90; ppl: 10.51; xent: 2.35; lr: 0.00000594;   0/592 tok/s; 187029 sec
[2020-03-31 23:37:58,544 INFO] Step 113350/210000; acc:  50.96; ppl: 11.15; xent: 2.41; lr: 0.00000594;   0/569 tok/s; 187113 sec
[2020-03-31 23:38:19,363 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-03-31 23:39:23,840 INFO] Step 113400/210000; acc:  50.42; ppl: 11.58; xent: 2.45; lr: 0.00000594;   0/727 tok/s; 187199 sec
[2020-03-31 23:40:48,068 INFO] Step 113450/210000; acc:  50.71; ppl: 10.56; xent: 2.36; lr: 0.00000594;   0/792 tok/s; 187283 sec
[2020-03-31 23:41:07,498 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-03-31 23:42:13,089 INFO] Step 113500/210000; acc:  51.43; ppl:  9.83; xent: 2.29; lr: 0.00000594;   0/748 tok/s; 187368 sec
[2020-03-31 23:42:13,114 INFO] Saving checkpoint ../models/model_step_113500.pt
[2020-03-31 23:43:39,415 INFO] Step 113550/210000; acc:  48.67; ppl: 11.88; xent: 2.47; lr: 0.00000594;   0/659 tok/s; 187454 sec
[2020-03-31 23:43:57,041 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-03-31 23:45:04,508 INFO] Step 113600/210000; acc:  45.69; ppl: 14.03; xent: 2.64; lr: 0.00000593;   0/1033 tok/s; 187539 sec
[2020-03-31 23:46:28,832 INFO] Step 113650/210000; acc:  49.49; ppl: 11.90; xent: 2.48; lr: 0.00000593;   0/873 tok/s; 187624 sec
[2020-03-31 23:46:44,646 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-03-31 23:47:53,373 INFO] Step 113700/210000; acc:  50.12; ppl: 11.65; xent: 2.46; lr: 0.00000593;   0/954 tok/s; 187708 sec
[2020-03-31 23:49:18,116 INFO] Step 113750/210000; acc:  50.71; ppl: 11.71; xent: 2.46; lr: 0.00000593;   0/922 tok/s; 187793 sec
[2020-03-31 23:49:34,214 INFO] Loading train dataset from ../bert_data/cnndm.train.103.bert.pt, number of examples: 2000
[2020-03-31 23:50:42,911 INFO] Step 113800/210000; acc:  48.56; ppl: 12.47; xent: 2.52; lr: 0.00000593;   0/641 tok/s; 187878 sec
[2020-03-31 23:52:07,067 INFO] Step 113850/210000; acc:  51.59; ppl:  9.77; xent: 2.28; lr: 0.00000593;   0/916 tok/s; 187962 sec
[2020-03-31 23:52:21,297 INFO] Loading train dataset from ../bert_data/cnndm.train.104.bert.pt, number of examples: 2000
[2020-03-31 23:53:32,274 INFO] Step 113900/210000; acc:  52.53; ppl:  9.67; xent: 2.27; lr: 0.00000593;   0/567 tok/s; 188047 sec
[2020-03-31 23:54:56,486 INFO] Step 113950/210000; acc:  48.20; ppl: 12.81; xent: 2.55; lr: 0.00000592;   0/715 tok/s; 188131 sec
[2020-03-31 23:55:08,945 INFO] Loading train dataset from ../bert_data/cnndm.train.105.bert.pt, number of examples: 2001
[2020-03-31 23:56:21,345 INFO] Step 114000/210000; acc:  54.94; ppl:  9.44; xent: 2.24; lr: 0.00000592;   0/734 tok/s; 188216 sec
[2020-03-31 23:56:21,348 INFO] Saving checkpoint ../models/model_step_114000.pt
[2020-03-31 23:57:48,027 INFO] Step 114050/210000; acc:  51.59; ppl: 10.45; xent: 2.35; lr: 0.00000592;   0/713 tok/s; 188303 sec
[2020-03-31 23:57:59,017 INFO] Loading train dataset from ../bert_data/cnndm.train.106.bert.pt, number of examples: 1999
[2020-03-31 23:59:13,419 INFO] Step 114100/210000; acc:  48.86; ppl: 13.60; xent: 2.61; lr: 0.00000592;   0/815 tok/s; 188388 sec
[2020-04-01 00:00:37,989 INFO] Step 114150/210000; acc:  49.35; ppl: 12.13; xent: 2.50; lr: 0.00000592;   0/708 tok/s; 188473 sec
[2020-04-01 00:00:47,142 INFO] Loading train dataset from ../bert_data/cnndm.train.107.bert.pt, number of examples: 1999
[2020-04-01 00:02:03,187 INFO] Step 114200/210000; acc:  55.95; ppl:  7.77; xent: 2.05; lr: 0.00000592;   0/979 tok/s; 188558 sec
[2020-04-01 00:03:27,813 INFO] Step 114250/210000; acc:  48.74; ppl: 13.13; xent: 2.57; lr: 0.00000592;   0/834 tok/s; 188643 sec
[2020-04-01 00:03:35,344 INFO] Loading train dataset from ../bert_data/cnndm.train.108.bert.pt, number of examples: 2000
[2020-04-01 00:04:52,596 INFO] Step 114300/210000; acc:  53.98; ppl:  9.11; xent: 2.21; lr: 0.00000592;   0/772 tok/s; 188727 sec
[2020-04-01 00:06:15,954 INFO] Step 114350/210000; acc:  52.54; ppl: 10.70; xent: 2.37; lr: 0.00000591;   0/732 tok/s; 188811 sec
[2020-04-01 00:06:23,365 INFO] Loading train dataset from ../bert_data/cnndm.train.109.bert.pt, number of examples: 2000
[2020-04-01 00:07:40,603 INFO] Step 114400/210000; acc:  52.05; ppl: 10.23; xent: 2.33; lr: 0.00000591;   0/931 tok/s; 188895 sec
[2020-04-01 00:09:05,121 INFO] Step 114450/210000; acc:  50.68; ppl: 10.44; xent: 2.35; lr: 0.00000591;   0/943 tok/s; 188980 sec
[2020-04-01 00:09:11,008 INFO] Loading train dataset from ../bert_data/cnndm.train.11.bert.pt, number of examples: 1999
[2020-04-01 00:10:30,153 INFO] Step 114500/210000; acc:  53.78; ppl:  8.29; xent: 2.12; lr: 0.00000591;   0/638 tok/s; 189065 sec
[2020-04-01 00:10:30,157 INFO] Saving checkpoint ../models/model_step_114500.pt
[2020-04-01 00:11:56,822 INFO] Step 114550/210000; acc:  55.88; ppl:  8.92; xent: 2.19; lr: 0.00000591;   0/690 tok/s; 189152 sec
[2020-04-01 00:12:00,907 INFO] Loading train dataset from ../bert_data/cnndm.train.110.bert.pt, number of examples: 2000
[2020-04-01 00:13:21,947 INFO] Step 114600/210000; acc:  52.00; ppl:  8.74; xent: 2.17; lr: 0.00000591;   0/663 tok/s; 189237 sec
[2020-04-01 00:14:46,158 INFO] Step 114650/210000; acc:  55.84; ppl:  7.24; xent: 1.98; lr: 0.00000591;   0/766 tok/s; 189321 sec
[2020-04-01 00:14:48,845 INFO] Loading train dataset from ../bert_data/cnndm.train.111.bert.pt, number of examples: 2000
[2020-04-01 00:16:11,351 INFO] Step 114700/210000; acc:  55.17; ppl:  8.48; xent: 2.14; lr: 0.00000591;   0/720 tok/s; 189406 sec
[2020-04-01 00:17:36,685 INFO] Step 114750/210000; acc:  53.62; ppl:  8.89; xent: 2.19; lr: 0.00000590;   0/655 tok/s; 189492 sec
[2020-04-01 00:17:37,020 INFO] Loading train dataset from ../bert_data/cnndm.train.112.bert.pt, number of examples: 1999
[2020-04-01 00:19:01,122 INFO] Step 114800/210000; acc:  54.84; ppl:  8.74; xent: 2.17; lr: 0.00000590;   0/933 tok/s; 189576 sec
[2020-04-01 00:20:24,737 INFO] Loading train dataset from ../bert_data/cnndm.train.113.bert.pt, number of examples: 1999
[2020-04-01 00:20:26,811 INFO] Step 114850/210000; acc:  47.05; ppl: 13.34; xent: 2.59; lr: 0.00000590;   0/577 tok/s; 189662 sec
[2020-04-01 00:21:51,076 INFO] Step 114900/210000; acc:  55.07; ppl:  8.12; xent: 2.09; lr: 0.00000590;   0/912 tok/s; 189746 sec
[2020-04-01 00:23:12,177 INFO] Loading train dataset from ../bert_data/cnndm.train.114.bert.pt, number of examples: 1998
[2020-04-01 00:23:15,694 INFO] Step 114950/210000; acc:  54.61; ppl:  8.58; xent: 2.15; lr: 0.00000590;   0/508 tok/s; 189831 sec
[2020-04-01 00:24:39,409 INFO] Step 115000/210000; acc:  48.96; ppl: 11.16; xent: 2.41; lr: 0.00000590;   0/869 tok/s; 189914 sec
[2020-04-01 00:24:39,435 INFO] Saving checkpoint ../models/model_step_115000.pt
[2020-04-01 00:26:03,324 INFO] Loading train dataset from ../bert_data/cnndm.train.115.bert.pt, number of examples: 2000
[2020-04-01 00:26:06,770 INFO] Step 115050/210000; acc:  55.02; ppl:  8.06; xent: 2.09; lr: 0.00000590;   0/627 tok/s; 190002 sec
[2020-04-01 00:27:31,214 INFO] Step 115100/210000; acc:  58.34; ppl:  6.95; xent: 1.94; lr: 0.00000590;   0/558 tok/s; 190086 sec
[2020-04-01 00:28:50,768 INFO] Loading train dataset from ../bert_data/cnndm.train.116.bert.pt, number of examples: 2000
[2020-04-01 00:28:55,819 INFO] Step 115150/210000; acc:  52.78; ppl:  9.09; xent: 2.21; lr: 0.00000589;   0/763 tok/s; 190171 sec
[2020-04-01 00:30:20,056 INFO] Step 115200/210000; acc:  51.42; ppl: 11.11; xent: 2.41; lr: 0.00000589;   0/744 tok/s; 190255 sec
[2020-04-01 00:31:38,253 INFO] Loading train dataset from ../bert_data/cnndm.train.117.bert.pt, number of examples: 2000
[2020-04-01 00:31:44,951 INFO] Step 115250/210000; acc:  51.21; ppl: 10.63; xent: 2.36; lr: 0.00000589;   0/831 tok/s; 190340 sec
[2020-04-01 00:33:09,155 INFO] Step 115300/210000; acc:  54.38; ppl:  9.15; xent: 2.21; lr: 0.00000589;   0/897 tok/s; 190424 sec
[2020-04-01 00:34:26,011 INFO] Loading train dataset from ../bert_data/cnndm.train.118.bert.pt, number of examples: 2001
[2020-04-01 00:34:34,527 INFO] Step 115350/210000; acc:  48.83; ppl: 11.66; xent: 2.46; lr: 0.00000589;   0/980 tok/s; 190509 sec
[2020-04-01 00:35:58,570 INFO] Step 115400/210000; acc:  48.54; ppl: 11.03; xent: 2.40; lr: 0.00000589;   0/940 tok/s; 190593 sec
[2020-04-01 00:37:13,061 INFO] Loading train dataset from ../bert_data/cnndm.train.119.bert.pt, number of examples: 2000
[2020-04-01 00:37:23,478 INFO] Step 115450/210000; acc:  52.40; ppl:  8.67; xent: 2.16; lr: 0.00000589;   0/806 tok/s; 190678 sec
[2020-04-01 00:38:47,644 INFO] Step 115500/210000; acc:  48.68; ppl: 11.27; xent: 2.42; lr: 0.00000588;   0/940 tok/s; 190762 sec
[2020-04-01 00:38:47,648 INFO] Saving checkpoint ../models/model_step_115500.pt
[2020-04-01 00:40:04,694 INFO] Loading train dataset from ../bert_data/cnndm.train.12.bert.pt, number of examples: 2001
[2020-04-01 00:40:14,724 INFO] Step 115550/210000; acc:  53.24; ppl:  8.04; xent: 2.08; lr: 0.00000588;   0/504 tok/s; 190850 sec
[2020-04-01 00:41:38,854 INFO] Step 115600/210000; acc:  49.50; ppl: 11.32; xent: 2.43; lr: 0.00000588;   0/1067 tok/s; 190934 sec
[2020-04-01 00:42:51,913 INFO] Loading train dataset from ../bert_data/cnndm.train.120.bert.pt, number of examples: 2001
[2020-04-01 00:43:03,588 INFO] Step 115650/210000; acc:  45.55; ppl: 14.28; xent: 2.66; lr: 0.00000588;   0/561 tok/s; 191018 sec
[2020-04-01 00:44:28,405 INFO] Step 115700/210000; acc:  56.98; ppl:  7.29; xent: 1.99; lr: 0.00000588;   0/483 tok/s; 191103 sec
[2020-04-01 00:45:40,062 INFO] Loading train dataset from ../bert_data/cnndm.train.121.bert.pt, number of examples: 2001
[2020-04-01 00:45:53,768 INFO] Step 115750/210000; acc:  52.89; ppl: 10.39; xent: 2.34; lr: 0.00000588;   0/717 tok/s; 191189 sec
[2020-04-01 00:47:18,246 INFO] Step 115800/210000; acc:  52.29; ppl:  9.58; xent: 2.26; lr: 0.00000588;   0/642 tok/s; 191273 sec
[2020-04-01 00:48:27,947 INFO] Loading train dataset from ../bert_data/cnndm.train.122.bert.pt, number of examples: 2000
[2020-04-01 00:48:43,195 INFO] Step 115850/210000; acc:  53.15; ppl:  9.11; xent: 2.21; lr: 0.00000588;   0/786 tok/s; 191358 sec
[2020-04-01 00:50:07,641 INFO] Step 115900/210000; acc:  58.76; ppl:  6.77; xent: 1.91; lr: 0.00000587;   0/771 tok/s; 191442 sec
[2020-04-01 00:51:15,812 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001
[2020-04-01 00:51:32,524 INFO] Step 115950/210000; acc:  53.35; ppl:  8.39; xent: 2.13; lr: 0.00000587;   0/959 tok/s; 191527 sec
[2020-04-01 00:52:56,629 INFO] Step 116000/210000; acc:  48.21; ppl: 11.46; xent: 2.44; lr: 0.00000587;   0/963 tok/s; 191611 sec
[2020-04-01 00:52:56,654 INFO] Saving checkpoint ../models/model_step_116000.pt
[2020-04-01 00:54:06,674 INFO] Loading train dataset from ../bert_data/cnndm.train.124.bert.pt, number of examples: 2001
[2020-04-01 00:54:23,460 INFO] Step 116050/210000; acc:  56.36; ppl:  8.09; xent: 2.09; lr: 0.00000587;   0/961 tok/s; 191698 sec
[2020-04-01 00:55:47,732 INFO] Step 116100/210000; acc:  50.23; ppl: 10.54; xent: 2.35; lr: 0.00000587;   0/916 tok/s; 191783 sec
[2020-04-01 00:56:54,005 INFO] Loading train dataset from ../bert_data/cnndm.train.125.bert.pt, number of examples: 2000
[2020-04-01 00:57:12,705 INFO] Step 116150/210000; acc:  46.07; ppl: 11.67; xent: 2.46; lr: 0.00000587;   0/694 tok/s; 191868 sec
[2020-04-01 00:58:37,410 INFO] Step 116200/210000; acc:  52.15; ppl:  9.85; xent: 2.29; lr: 0.00000587;   0/601 tok/s; 191952 sec
[2020-04-01 00:59:42,106 INFO] Loading train dataset from ../bert_data/cnndm.train.126.bert.pt, number of examples: 1999
[2020-04-01 01:00:02,132 INFO] Step 116250/210000; acc:  51.61; ppl: 10.44; xent: 2.35; lr: 0.00000587;   0/582 tok/s; 192037 sec
[2020-04-01 01:01:26,401 INFO] Step 116300/210000; acc:  54.56; ppl:  8.62; xent: 2.15; lr: 0.00000586;   0/506 tok/s; 192121 sec
[2020-04-01 01:02:30,003 INFO] Loading train dataset from ../bert_data/cnndm.train.127.bert.pt, number of examples: 2000
[2020-04-01 01:02:51,821 INFO] Step 116350/210000; acc:  53.62; ppl:  9.43; xent: 2.24; lr: 0.00000586;   0/711 tok/s; 192207 sec
[2020-04-01 01:04:16,467 INFO] Step 116400/210000; acc:  54.85; ppl:  8.57; xent: 2.15; lr: 0.00000586;   0/674 tok/s; 192291 sec
[2020-04-01 01:05:17,737 INFO] Loading train dataset from ../bert_data/cnndm.train.128.bert.pt, number of examples: 2001
[2020-04-01 01:05:41,211 INFO] Step 116450/210000; acc:  52.93; ppl:  9.06; xent: 2.20; lr: 0.00000586;   0/682 tok/s; 192376 sec
[2020-04-01 01:07:05,781 INFO] Step 116500/210000; acc:  56.69; ppl:  8.18; xent: 2.10; lr: 0.00000586;   0/767 tok/s; 192461 sec
[2020-04-01 01:07:05,806 INFO] Saving checkpoint ../models/model_step_116500.pt
[2020-04-01 01:08:07,380 INFO] Loading train dataset from ../bert_data/cnndm.train.129.bert.pt, number of examples: 2000
[2020-04-01 01:08:32,807 INFO] Step 116550/210000; acc:  52.98; ppl:  8.80; xent: 2.18; lr: 0.00000586;   0/972 tok/s; 192548 sec
[2020-04-01 01:09:57,233 INFO] Step 116600/210000; acc:  55.02; ppl:  8.23; xent: 2.11; lr: 0.00000586;   0/811 tok/s; 192632 sec
[2020-04-01 01:10:56,812 INFO] Loading train dataset from ../bert_data/cnndm.train.13.bert.pt, number of examples: 2001
[2020-04-01 01:11:22,241 INFO] Step 116650/210000; acc:  54.31; ppl:  9.09; xent: 2.21; lr: 0.00000586;   0/928 tok/s; 192717 sec
[2020-04-01 01:12:46,827 INFO] Step 116700/210000; acc:  51.24; ppl:  9.27; xent: 2.23; lr: 0.00000585;   0/791 tok/s; 192802 sec
[2020-04-01 01:13:45,085 INFO] Loading train dataset from ../bert_data/cnndm.train.130.bert.pt, number of examples: 2000
[2020-04-01 01:14:12,009 INFO] Step 116750/210000; acc:  54.36; ppl:  7.75; xent: 2.05; lr: 0.00000585;   0/507 tok/s; 192887 sec
[2020-04-01 01:15:35,897 INFO] Step 116800/210000; acc:  52.74; ppl:  8.56; xent: 2.15; lr: 0.00000585;   0/588 tok/s; 192971 sec
[2020-04-01 01:16:31,579 INFO] Loading train dataset from ../bert_data/cnndm.train.131.bert.pt, number of examples: 2001
[2020-04-01 01:17:00,596 INFO] Step 116850/210000; acc:  50.24; ppl: 10.97; xent: 2.40; lr: 0.00000585;   0/520 tok/s; 193055 sec
[2020-04-01 01:18:24,681 INFO] Step 116900/210000; acc:  58.13; ppl:  7.39; xent: 2.00; lr: 0.00000585;   0/609 tok/s; 193140 sec
[2020-04-01 01:19:20,849 INFO] Loading train dataset from ../bert_data/cnndm.train.132.bert.pt, number of examples: 2001
[2020-04-01 01:19:49,372 INFO] Step 116950/210000; acc:  46.46; ppl: 13.23; xent: 2.58; lr: 0.00000585;   0/643 tok/s; 193224 sec
[2020-04-01 01:21:13,388 INFO] Step 117000/210000; acc:  47.94; ppl: 12.28; xent: 2.51; lr: 0.00000585;   0/721 tok/s; 193308 sec
[2020-04-01 01:21:13,409 INFO] Saving checkpoint ../models/model_step_117000.pt
[2020-04-01 01:22:10,280 INFO] Loading train dataset from ../bert_data/cnndm.train.133.bert.pt, number of examples: 1999
[2020-04-01 01:22:40,618 INFO] Step 117050/210000; acc:  56.69; ppl:  7.92; xent: 2.07; lr: 0.00000585;   0/705 tok/s; 193395 sec
[2020-04-01 01:24:04,849 INFO] Step 117100/210000; acc:  54.90; ppl:  9.08; xent: 2.21; lr: 0.00000584;   0/755 tok/s; 193480 sec
[2020-04-01 01:24:57,570 INFO] Loading train dataset from ../bert_data/cnndm.train.134.bert.pt, number of examples: 2001
[2020-04-01 01:25:29,768 INFO] Step 117150/210000; acc:  51.84; ppl: 10.99; xent: 2.40; lr: 0.00000584;   0/872 tok/s; 193565 sec
[2020-04-01 01:26:53,606 INFO] Step 117200/210000; acc:  53.53; ppl:  9.37; xent: 2.24; lr: 0.00000584;   0/773 tok/s; 193648 sec
[2020-04-01 01:27:45,567 INFO] Loading train dataset from ../bert_data/cnndm.train.135.bert.pt, number of examples: 1999
[2020-04-01 01:28:19,282 INFO] Step 117250/210000; acc:  50.57; ppl: 11.46; xent: 2.44; lr: 0.00000584;   0/927 tok/s; 193734 sec
[2020-04-01 01:29:43,705 INFO] Step 117300/210000; acc:  50.07; ppl: 10.31; xent: 2.33; lr: 0.00000584;   0/832 tok/s; 193819 sec
[2020-04-01 01:30:34,685 INFO] Loading train dataset from ../bert_data/cnndm.train.136.bert.pt, number of examples: 2001
[2020-04-01 01:31:08,286 INFO] Step 117350/210000; acc:  52.51; ppl:  9.87; xent: 2.29; lr: 0.00000584;   0/994 tok/s; 193903 sec
[2020-04-01 01:32:32,265 INFO] Step 117400/210000; acc:  51.00; ppl: 10.98; xent: 2.40; lr: 0.00000584;   0/908 tok/s; 193987 sec
[2020-04-01 01:33:21,811 INFO] Loading train dataset from ../bert_data/cnndm.train.137.bert.pt, number of examples: 2000
[2020-04-01 01:33:57,090 INFO] Step 117450/210000; acc:  56.34; ppl:  7.21; xent: 1.98; lr: 0.00000584;   0/710 tok/s; 194072 sec
[2020-04-01 01:35:20,882 INFO] Step 117500/210000; acc:  49.24; ppl: 10.65; xent: 2.37; lr: 0.00000583;   0/725 tok/s; 194156 sec
[2020-04-01 01:35:20,903 INFO] Saving checkpoint ../models/model_step_117500.pt
[2020-04-01 01:36:12,951 INFO] Loading train dataset from ../bert_data/cnndm.train.138.bert.pt, number of examples: 2000
[2020-04-01 01:36:48,316 INFO] Step 117550/210000; acc:  56.01; ppl:  7.40; xent: 2.00; lr: 0.00000583;   0/800 tok/s; 194243 sec
[2020-04-01 01:38:12,550 INFO] Step 117600/210000; acc:  48.36; ppl: 12.45; xent: 2.52; lr: 0.00000583;   0/784 tok/s; 194327 sec
[2020-04-01 01:39:00,278 INFO] Loading train dataset from ../bert_data/cnndm.train.139.bert.pt, number of examples: 2001
[2020-04-01 01:39:36,914 INFO] Step 117650/210000; acc:  55.34; ppl:  8.54; xent: 2.15; lr: 0.00000583;   0/618 tok/s; 194412 sec
[2020-04-01 01:41:01,429 INFO] Step 117700/210000; acc:  53.96; ppl:  8.90; xent: 2.19; lr: 0.00000583;   0/585 tok/s; 194496 sec
[2020-04-01 01:41:47,374 INFO] Loading train dataset from ../bert_data/cnndm.train.14.bert.pt, number of examples: 1998
[2020-04-01 01:42:26,275 INFO] Step 117750/210000; acc:  53.46; ppl:  9.79; xent: 2.28; lr: 0.00000583;   0/595 tok/s; 194581 sec
[2020-04-01 01:43:50,876 INFO] Step 117800/210000; acc:  55.05; ppl:  8.00; xent: 2.08; lr: 0.00000583;   0/483 tok/s; 194666 sec
[2020-04-01 01:44:35,495 INFO] Loading train dataset from ../bert_data/cnndm.train.140.bert.pt, number of examples: 2000
[2020-04-01 01:45:15,726 INFO] Step 117850/210000; acc:  53.70; ppl:  7.80; xent: 2.05; lr: 0.00000583;   0/731 tok/s; 194751 sec
[2020-04-01 01:46:39,974 INFO] Step 117900/210000; acc:  49.15; ppl: 10.10; xent: 2.31; lr: 0.00000582;   0/672 tok/s; 194835 sec
[2020-04-01 01:47:22,356 INFO] Loading train dataset from ../bert_data/cnndm.train.141.bert.pt, number of examples: 1999
[2020-04-01 01:48:04,338 INFO] Step 117950/210000; acc:  50.66; ppl: 10.02; xent: 2.30; lr: 0.00000582;   0/986 tok/s; 194919 sec
[2020-04-01 01:49:28,482 INFO] Step 118000/210000; acc:  49.53; ppl: 11.41; xent: 2.43; lr: 0.00000582;   0/703 tok/s; 195003 sec
[2020-04-01 01:49:28,485 INFO] Saving checkpoint ../models/model_step_118000.pt
[2020-04-01 01:50:13,595 INFO] Loading train dataset from ../bert_data/cnndm.train.142.bert.pt, number of examples: 2000
[2020-04-01 01:50:55,839 INFO] Step 118050/210000; acc:  51.12; ppl: 11.13; xent: 2.41; lr: 0.00000582;   0/883 tok/s; 195091 sec
[2020-04-01 01:52:20,042 INFO] Step 118100/210000; acc:  49.13; ppl: 10.08; xent: 2.31; lr: 0.00000582;   0/720 tok/s; 195175 sec
[2020-04-01 01:53:01,348 INFO] Loading train dataset from ../bert_data/cnndm.train.143.bert.pt, number of examples: 1084
[2020-04-01 01:53:45,271 INFO] Step 118150/210000; acc:  46.43; ppl: 12.45; xent: 2.52; lr: 0.00000582;   0/865 tok/s; 195260 sec
[2020-04-01 01:54:32,942 INFO] Loading train dataset from ../bert_data/cnndm.train.15.bert.pt, number of examples: 1999
[2020-04-01 01:55:09,912 INFO] Step 118200/210000; acc:  56.67; ppl:  6.89; xent: 1.93; lr: 0.00000582;   0/569 tok/s; 195345 sec
[2020-04-01 01:56:33,822 INFO] Step 118250/210000; acc:  54.07; ppl:  9.33; xent: 2.23; lr: 0.00000582;   0/747 tok/s; 195429 sec
[2020-04-01 01:57:21,482 INFO] Loading train dataset from ../bert_data/cnndm.train.150.bert.pt, number of examples: 1985
[2020-04-01 01:57:53,467 INFO] Step 118300/210000; acc:  69.15; ppl:  3.54; xent: 1.26; lr: 0.00000581;   0/487 tok/s; 195508 sec
[2020-04-01 01:59:06,747 INFO] Step 118350/210000; acc:  65.20; ppl:  4.12; xent: 1.42; lr: 0.00000581;   0/637 tok/s; 195582 sec
[2020-04-01 01:59:39,465 INFO] Loading train dataset from ../bert_data/cnndm.train.151.bert.pt, number of examples: 1990
[2020-04-01 02:00:20,207 INFO] Step 118400/210000; acc:  63.73; ppl:  4.41; xent: 1.48; lr: 0.00000581;   0/486 tok/s; 195655 sec
[2020-04-01 02:01:33,215 INFO] Step 118450/210000; acc:  66.34; ppl:  3.99; xent: 1.38; lr: 0.00000581;   0/643 tok/s; 195728 sec
[2020-04-01 02:01:59,062 INFO] Loading train dataset from ../bert_data/cnndm.train.152.bert.pt, number of examples: 993
[2020-04-01 02:02:47,576 INFO] Step 118500/210000; acc:  63.91; ppl:  4.47; xent: 1.50; lr: 0.00000581;   0/703 tok/s; 195802 sec
[2020-04-01 02:02:47,579 INFO] Saving checkpoint ../models/model_step_118500.pt
[2020-04-01 02:03:10,736 INFO] Loading train dataset from ../bert_data/cnndm.train.153.bert.pt, number of examples: 1982
[2020-04-01 02:04:10,384 INFO] Step 118550/210000; acc:  51.89; ppl:  7.55; xent: 2.02; lr: 0.00000581;   0/608 tok/s; 195885 sec
[2020-04-01 02:05:33,820 INFO] Step 118600/210000; acc:  46.54; ppl: 12.32; xent: 2.51; lr: 0.00000581;   0/1101 tok/s; 195969 sec
[2020-04-01 02:05:49,108 INFO] Loading train dataset from ../bert_data/cnndm.train.154.bert.pt, number of examples: 1978
[2020-04-01 02:06:57,193 INFO] Step 118650/210000; acc:  48.91; ppl:  9.11; xent: 2.21; lr: 0.00000581;   0/693 tok/s; 196052 sec
[2020-04-01 02:08:20,988 INFO] Step 118700/210000; acc:  46.05; ppl: 12.49; xent: 2.52; lr: 0.00000581;   0/951 tok/s; 196136 sec
[2020-04-01 02:08:26,517 INFO] Loading train dataset from ../bert_data/cnndm.train.155.bert.pt, number of examples: 1988
[2020-04-01 02:09:43,856 INFO] Step 118750/210000; acc:  44.62; ppl: 14.01; xent: 2.64; lr: 0.00000580;   0/1031 tok/s; 196219 sec
[2020-04-01 02:11:04,414 INFO] Loading train dataset from ../bert_data/cnndm.train.156.bert.pt, number of examples: 1993
[2020-04-01 02:11:07,800 INFO] Step 118800/210000; acc:  56.30; ppl:  6.01; xent: 1.79; lr: 0.00000580;   0/583 tok/s; 196303 sec
[2020-04-01 02:12:30,711 INFO] Step 118850/210000; acc:  48.79; ppl: 10.41; xent: 2.34; lr: 0.00000580;   0/1224 tok/s; 196386 sec
[2020-04-01 02:13:42,850 INFO] Loading train dataset from ../bert_data/cnndm.train.157.bert.pt, number of examples: 1981
[2020-04-01 02:13:54,505 INFO] Step 118900/210000; acc:  49.58; ppl:  9.17; xent: 2.22; lr: 0.00000580;   0/649 tok/s; 196469 sec
[2020-04-01 02:15:17,730 INFO] Step 118950/210000; acc:  45.40; ppl: 12.70; xent: 2.54; lr: 0.00000580;   0/1398 tok/s; 196553 sec
[2020-04-01 02:16:21,059 INFO] Loading train dataset from ../bert_data/cnndm.train.158.bert.pt, number of examples: 1987
[2020-04-01 02:16:42,313 INFO] Step 119000/210000; acc:  51.27; ppl:  8.05; xent: 2.09; lr: 0.00000580;   0/785 tok/s; 196637 sec
[2020-04-01 02:16:42,317 INFO] Saving checkpoint ../models/model_step_119000.pt
[2020-04-01 02:18:07,166 INFO] Step 119050/210000; acc:  50.74; ppl:  8.86; xent: 2.18; lr: 0.00000580;   0/618 tok/s; 196722 sec
[2020-04-01 02:19:00,567 INFO] Loading train dataset from ../bert_data/cnndm.train.159.bert.pt, number of examples: 1986
[2020-04-01 02:19:30,660 INFO] Step 119100/210000; acc:  48.99; ppl:  8.79; xent: 2.17; lr: 0.00000580;   0/1002 tok/s; 196805 sec
[2020-04-01 02:20:54,392 INFO] Step 119150/210000; acc:  59.43; ppl:  5.35; xent: 1.68; lr: 0.00000579;   0/354 tok/s; 196889 sec
[2020-04-01 02:21:38,217 INFO] Loading train dataset from ../bert_data/cnndm.train.16.bert.pt, number of examples: 2001
[2020-04-01 02:22:18,480 INFO] Step 119200/210000; acc:  51.25; ppl:  9.66; xent: 2.27; lr: 0.00000579;   0/782 tok/s; 196973 sec
[2020-04-01 02:23:42,575 INFO] Step 119250/210000; acc:  52.13; ppl:  8.91; xent: 2.19; lr: 0.00000579;   0/651 tok/s; 197057 sec
[2020-04-01 02:24:27,263 INFO] Loading train dataset from ../bert_data/cnndm.train.160.bert.pt, number of examples: 1983
[2020-04-01 02:25:07,385 INFO] Step 119300/210000; acc:  48.52; ppl:  9.91; xent: 2.29; lr: 0.00000579;   0/1281 tok/s; 197142 sec
[2020-04-01 02:26:30,391 INFO] Step 119350/210000; acc:  55.75; ppl:  6.75; xent: 1.91; lr: 0.00000579;   0/630 tok/s; 197225 sec
[2020-04-01 02:27:06,300 INFO] Loading train dataset from ../bert_data/cnndm.train.161.bert.pt, number of examples: 1990
[2020-04-01 02:27:54,260 INFO] Step 119400/210000; acc:  50.28; ppl:  9.34; xent: 2.23; lr: 0.00000579;   0/1044 tok/s; 197309 sec
[2020-04-01 02:29:16,875 INFO] Step 119450/210000; acc:  57.14; ppl:  6.23; xent: 1.83; lr: 0.00000579;   0/559 tok/s; 197392 sec
[2020-04-01 02:29:43,546 INFO] Loading train dataset from ../bert_data/cnndm.train.162.bert.pt, number of examples: 1986
[2020-04-01 02:30:40,711 INFO] Step 119500/210000; acc:  51.54; ppl:  9.07; xent: 2.21; lr: 0.00000579;   0/1011 tok/s; 197476 sec
[2020-04-01 02:30:40,714 INFO] Saving checkpoint ../models/model_step_119500.pt
[2020-04-01 02:32:05,441 INFO] Step 119550/210000; acc:  53.12; ppl:  7.26; xent: 1.98; lr: 0.00000578;   0/810 tok/s; 197560 sec
[2020-04-01 02:32:24,333 INFO] Loading train dataset from ../bert_data/cnndm.train.163.bert.pt, number of examples: 1982
[2020-04-01 02:33:29,370 INFO] Step 119600/210000; acc:  62.91; ppl:  4.28; xent: 1.46; lr: 0.00000578;   0/373 tok/s; 197644 sec
[2020-04-01 02:34:51,899 INFO] Step 119650/210000; acc:  50.13; ppl:  8.36; xent: 2.12; lr: 0.00000578;   0/908 tok/s; 197727 sec
[2020-04-01 02:35:00,791 INFO] Loading train dataset from ../bert_data/cnndm.train.164.bert.pt, number of examples: 1984
[2020-04-01 02:36:15,190 INFO] Step 119700/210000; acc:  60.68; ppl:  4.97; xent: 1.60; lr: 0.00000578;   0/477 tok/s; 197810 sec
[2020-04-01 02:37:38,078 INFO] Step 119750/210000; acc:  45.99; ppl: 11.83; xent: 2.47; lr: 0.00000578;   0/1130 tok/s; 197893 sec
[2020-04-01 02:37:38,374 INFO] Loading train dataset from ../bert_data/cnndm.train.165.bert.pt, number of examples: 1984
[2020-04-01 02:39:00,997 INFO] Step 119800/210000; acc:  60.17; ppl:  5.48; xent: 1.70; lr: 0.00000578;   0/606 tok/s; 197976 sec
[2020-04-01 02:40:16,088 INFO] Loading train dataset from ../bert_data/cnndm.train.166.bert.pt, number of examples: 1370
[2020-04-01 02:40:24,501 INFO] Step 119850/210000; acc:  51.13; ppl:  9.02; xent: 2.20; lr: 0.00000578;   0/910 tok/s; 198059 sec
[2020-04-01 02:41:47,431 INFO] Step 119900/210000; acc:  55.12; ppl:  6.53; xent: 1.88; lr: 0.00000578;   0/677 tok/s; 198142 sec
[2020-04-01 02:42:05,003 INFO] Loading train dataset from ../bert_data/cnndm.train.167.bert.pt, number of examples: 1089
[2020-04-01 02:43:09,393 INFO] Step 119950/210000; acc:  59.07; ppl:  5.10; xent: 1.63; lr: 0.00000577;   0/852 tok/s; 198224 sec
[2020-04-01 02:43:30,286 INFO] Loading train dataset from ../bert_data/cnndm.train.168.bert.pt, number of examples: 1991
[2020-04-01 02:44:29,750 INFO] Step 120000/210000; acc:  53.44; ppl:  6.67; xent: 1.90; lr: 0.00000577;   0/938 tok/s; 198305 sec
[2020-04-01 02:44:29,754 INFO] Saving checkpoint ../models/model_step_120000.pt
[2020-04-01 02:45:54,059 INFO] Step 120050/210000; acc:  51.87; ppl:  8.41; xent: 2.13; lr: 0.00000577;   0/827 tok/s; 198389 sec
[2020-04-01 02:46:07,193 INFO] Loading train dataset from ../bert_data/cnndm.train.169.bert.pt, number of examples: 1995
[2020-04-01 02:47:15,167 INFO] Step 120100/210000; acc:  55.63; ppl:  6.29; xent: 1.84; lr: 0.00000577;   0/831 tok/s; 198470 sec
[2020-04-01 02:48:37,074 INFO] Step 120150/210000; acc:  46.73; ppl: 11.12; xent: 2.41; lr: 0.00000577;   0/768 tok/s; 198552 sec
[2020-04-01 02:48:44,358 INFO] Loading train dataset from ../bert_data/cnndm.train.17.bert.pt, number of examples: 2000
[2020-04-01 02:50:01,740 INFO] Step 120200/210000; acc:  54.51; ppl:  8.66; xent: 2.16; lr: 0.00000577;   0/522 tok/s; 198637 sec
[2020-04-01 02:51:26,208 INFO] Step 120250/210000; acc:  47.40; ppl: 12.91; xent: 2.56; lr: 0.00000577;   0/608 tok/s; 198721 sec
[2020-04-01 02:51:31,810 INFO] Loading train dataset from ../bert_data/cnndm.train.170.bert.pt, number of examples: 1981
[2020-04-01 02:52:48,592 INFO] Step 120300/210000; acc:  49.04; ppl:  9.70; xent: 2.27; lr: 0.00000577;   0/1171 tok/s; 198803 sec
[2020-04-01 02:54:06,214 INFO] Loading train dataset from ../bert_data/cnndm.train.171.bert.pt, number of examples: 1990
[2020-04-01 02:54:10,984 INFO] Step 120350/210000; acc:  55.97; ppl:  6.13; xent: 1.81; lr: 0.00000577;   0/791 tok/s; 198886 sec
[2020-04-01 02:55:32,210 INFO] Step 120400/210000; acc:  48.05; ppl:  9.40; xent: 2.24; lr: 0.00000576;   0/803 tok/s; 198967 sec
[2020-04-01 02:56:40,765 INFO] Loading train dataset from ../bert_data/cnndm.train.172.bert.pt, number of examples: 1983
[2020-04-01 02:56:53,722 INFO] Step 120450/210000; acc:  56.00; ppl:  6.33; xent: 1.85; lr: 0.00000576;   0/700 tok/s; 199049 sec
[2020-04-01 02:58:14,774 INFO] Step 120500/210000; acc:  50.70; ppl:  7.93; xent: 2.07; lr: 0.00000576;   0/651 tok/s; 199130 sec
[2020-04-01 02:58:14,778 INFO] Saving checkpoint ../models/model_step_120500.pt
[2020-04-01 02:59:15,603 INFO] Loading train dataset from ../bert_data/cnndm.train.18.bert.pt, number of examples: 1998
[2020-04-01 02:59:39,364 INFO] Step 120550/210000; acc:  51.14; ppl: 10.50; xent: 2.35; lr: 0.00000576;   0/789 tok/s; 199214 sec
[2020-04-01 03:01:03,250 INFO] Step 120600/210000; acc:  55.84; ppl:  7.73; xent: 2.05; lr: 0.00000576;   0/727 tok/s; 199298 sec
[2020-04-01 03:02:02,840 INFO] Loading train dataset from ../bert_data/cnndm.train.19.bert.pt, number of examples: 2000
[2020-04-01 03:02:28,140 INFO] Step 120650/210000; acc:  55.80; ppl:  8.05; xent: 2.09; lr: 0.00000576;   0/887 tok/s; 199383 sec
[2020-04-01 03:03:52,092 INFO] Step 120700/210000; acc:  51.79; ppl: 10.73; xent: 2.37; lr: 0.00000576;   0/677 tok/s; 199467 sec
[2020-04-01 03:04:52,077 INFO] Loading train dataset from ../bert_data/cnndm.train.2.bert.pt, number of examples: 2001
[2020-04-01 03:05:17,126 INFO] Step 120750/210000; acc:  55.11; ppl:  7.75; xent: 2.05; lr: 0.00000576;   0/891 tok/s; 199552 sec
[2020-04-01 03:06:41,431 INFO] Step 120800/210000; acc:  53.79; ppl:  7.97; xent: 2.08; lr: 0.00000575;   0/754 tok/s; 199636 sec
[2020-04-01 03:07:39,483 INFO] Loading train dataset from ../bert_data/cnndm.train.20.bert.pt, number of examples: 2000
[2020-04-01 03:08:06,454 INFO] Step 120850/210000; acc:  46.09; ppl: 13.18; xent: 2.58; lr: 0.00000575;   0/589 tok/s; 199721 sec
[2020-04-01 03:09:31,168 INFO] Step 120900/210000; acc:  51.62; ppl:  9.60; xent: 2.26; lr: 0.00000575;   0/759 tok/s; 199806 sec
[2020-04-01 03:10:27,607 INFO] Loading train dataset from ../bert_data/cnndm.train.21.bert.pt, number of examples: 2001
[2020-04-01 03:10:56,271 INFO] Step 120950/210000; acc:  51.27; ppl:  9.70; xent: 2.27; lr: 0.00000575;   0/509 tok/s; 199891 sec
[2020-04-01 03:12:20,556 INFO] Step 121000/210000; acc:  52.13; ppl:  9.63; xent: 2.27; lr: 0.00000575;   0/1006 tok/s; 199975 sec
[2020-04-01 03:12:20,583 INFO] Saving checkpoint ../models/model_step_121000.pt
[2020-04-01 03:13:17,232 INFO] Loading train dataset from ../bert_data/cnndm.train.22.bert.pt, number of examples: 1999
[2020-04-01 03:13:47,555 INFO] Step 121050/210000; acc:  51.92; ppl:  9.93; xent: 2.30; lr: 0.00000575;   0/643 tok/s; 200062 sec
[2020-04-01 03:15:11,543 INFO] Step 121100/210000; acc:  50.73; ppl:  9.86; xent: 2.29; lr: 0.00000575;   0/815 tok/s; 200146 sec
[2020-04-01 03:16:06,106 INFO] Loading train dataset from ../bert_data/cnndm.train.23.bert.pt, number of examples: 2001
[2020-04-01 03:16:36,689 INFO] Step 121150/210000; acc:  52.79; ppl:  9.92; xent: 2.29; lr: 0.00000575;   0/687 tok/s; 200232 sec
[2020-04-01 03:18:00,884 INFO] Step 121200/210000; acc:  53.71; ppl:  9.23; xent: 2.22; lr: 0.00000574;   0/551 tok/s; 200316 sec
[2020-04-01 03:18:53,776 INFO] Loading train dataset from ../bert_data/cnndm.train.24.bert.pt, number of examples: 2001
[2020-04-01 03:19:25,528 INFO] Step 121250/210000; acc:  59.14; ppl:  6.95; xent: 1.94; lr: 0.00000574;   0/834 tok/s; 200400 sec
[2020-04-01 03:20:49,851 INFO] Step 121300/210000; acc:  56.82; ppl:  7.71; xent: 2.04; lr: 0.00000574;   0/652 tok/s; 200485 sec
[2020-04-01 03:21:42,840 INFO] Loading train dataset from ../bert_data/cnndm.train.25.bert.pt, number of examples: 2001
[2020-04-01 03:22:14,707 INFO] Step 121350/210000; acc:  46.12; ppl: 13.09; xent: 2.57; lr: 0.00000574;   0/829 tok/s; 200570 sec
[2020-04-01 03:23:38,999 INFO] Step 121400/210000; acc:  51.79; ppl:  9.84; xent: 2.29; lr: 0.00000574;   0/800 tok/s; 200654 sec
[2020-04-01 03:24:30,315 INFO] Loading train dataset from ../bert_data/cnndm.train.26.bert.pt, number of examples: 2000
[2020-04-01 03:25:04,108 INFO] Step 121450/210000; acc:  52.72; ppl:  9.55; xent: 2.26; lr: 0.00000574;   0/832 tok/s; 200739 sec
[2020-04-01 03:26:27,756 INFO] Step 121500/210000; acc:  51.37; ppl:  9.00; xent: 2.20; lr: 0.00000574;   0/809 tok/s; 200823 sec
[2020-04-01 03:26:27,780 INFO] Saving checkpoint ../models/model_step_121500.pt
[2020-04-01 03:27:19,389 INFO] Loading train dataset from ../bert_data/cnndm.train.27.bert.pt, number of examples: 2001
[2020-04-01 03:27:55,050 INFO] Step 121550/210000; acc:  52.26; ppl:  9.79; xent: 2.28; lr: 0.00000574;   0/794 tok/s; 200910 sec
[2020-04-01 03:29:19,298 INFO] Step 121600/210000; acc:  54.54; ppl:  7.93; xent: 2.07; lr: 0.00000574;   0/975 tok/s; 200994 sec
[2020-04-01 03:30:08,886 INFO] Loading train dataset from ../bert_data/cnndm.train.28.bert.pt, number of examples: 2000
[2020-04-01 03:30:44,499 INFO] Step 121650/210000; acc:  56.77; ppl:  7.82; xent: 2.06; lr: 0.00000573;   0/445 tok/s; 201079 sec
[2020-04-01 03:32:08,621 INFO] Step 121700/210000; acc:  52.32; ppl:  9.35; xent: 2.24; lr: 0.00000573;   0/785 tok/s; 201163 sec
[2020-04-01 03:32:56,659 INFO] Loading train dataset from ../bert_data/cnndm.train.29.bert.pt, number of examples: 1999
[2020-04-01 03:33:33,632 INFO] Step 121750/210000; acc:  53.70; ppl:  8.78; xent: 2.17; lr: 0.00000573;   0/603 tok/s; 201248 sec
[2020-04-01 03:34:57,940 INFO] Step 121800/210000; acc:  55.31; ppl:  7.49; xent: 2.01; lr: 0.00000573;   0/815 tok/s; 201333 sec
[2020-04-01 03:35:44,160 INFO] Loading train dataset from ../bert_data/cnndm.train.3.bert.pt, number of examples: 2001
[2020-04-01 03:36:22,457 INFO] Step 121850/210000; acc:  54.99; ppl:  8.48; xent: 2.14; lr: 0.00000573;   0/709 tok/s; 201417 sec
[2020-04-01 03:37:46,413 INFO] Step 121900/210000; acc:  55.05; ppl:  7.79; xent: 2.05; lr: 0.00000573;   0/587 tok/s; 201501 sec
[2020-04-01 03:38:30,751 INFO] Loading train dataset from ../bert_data/cnndm.train.30.bert.pt, number of examples: 1996
[2020-04-01 03:39:10,999 INFO] Step 121950/210000; acc:  53.28; ppl:  9.01; xent: 2.20; lr: 0.00000573;   0/781 tok/s; 201586 sec
[2020-04-01 03:40:35,210 INFO] Step 122000/210000; acc:  47.77; ppl: 12.65; xent: 2.54; lr: 0.00000573;   0/719 tok/s; 201670 sec
[2020-04-01 03:40:35,234 INFO] Saving checkpoint ../models/model_step_122000.pt
[2020-04-01 03:41:22,115 INFO] Loading train dataset from ../bert_data/cnndm.train.31.bert.pt, number of examples: 2000
[2020-04-01 03:42:02,718 INFO] Step 122050/210000; acc:  53.71; ppl:  8.61; xent: 2.15; lr: 0.00000572;   0/980 tok/s; 201758 sec
[2020-04-01 03:43:26,927 INFO] Step 122100/210000; acc:  56.44; ppl:  8.44; xent: 2.13; lr: 0.00000572;   0/794 tok/s; 201842 sec
[2020-04-01 03:44:09,761 INFO] Loading train dataset from ../bert_data/cnndm.train.32.bert.pt, number of examples: 1998
[2020-04-01 03:44:51,986 INFO] Step 122150/210000; acc:  51.86; ppl: 10.28; xent: 2.33; lr: 0.00000572;   0/942 tok/s; 201927 sec
[2020-04-01 03:46:16,051 INFO] Step 122200/210000; acc:  52.98; ppl:  9.56; xent: 2.26; lr: 0.00000572;   0/907 tok/s; 202011 sec
[2020-04-01 03:46:57,313 INFO] Loading train dataset from ../bert_data/cnndm.train.33.bert.pt, number of examples: 1999
[2020-04-01 03:47:41,225 INFO] Step 122250/210000; acc:  58.51; ppl:  6.85; xent: 1.92; lr: 0.00000572;   0/933 tok/s; 202096 sec
[2020-04-01 03:49:05,136 INFO] Step 122300/210000; acc:  50.89; ppl: 10.94; xent: 2.39; lr: 0.00000572;   0/938 tok/s; 202180 sec
[2020-04-01 03:49:44,473 INFO] Loading train dataset from ../bert_data/cnndm.train.34.bert.pt, number of examples: 2000
[2020-04-01 03:50:29,898 INFO] Step 122350/210000; acc:  46.72; ppl: 13.48; xent: 2.60; lr: 0.00000572;   0/907 tok/s; 202265 sec
[2020-04-01 03:51:54,023 INFO] Step 122400/210000; acc:  49.74; ppl: 11.05; xent: 2.40; lr: 0.00000572;   0/979 tok/s; 202349 sec
[2020-04-01 03:52:33,481 INFO] Loading train dataset from ../bert_data/cnndm.train.35.bert.pt, number of examples: 1998
[2020-04-01 03:53:18,959 INFO] Step 122450/210000; acc:  56.85; ppl:  6.73; xent: 1.91; lr: 0.00000572;   0/697 tok/s; 202434 sec
[2020-04-01 03:54:42,845 INFO] Step 122500/210000; acc:  60.85; ppl:  5.50; xent: 1.70; lr: 0.00000571;   0/625 tok/s; 202518 sec
[2020-04-01 03:54:42,869 INFO] Saving checkpoint ../models/model_step_122500.pt
[2020-04-01 03:55:23,007 INFO] Loading train dataset from ../bert_data/cnndm.train.36.bert.pt, number of examples: 2000
[2020-04-01 03:56:10,125 INFO] Step 122550/210000; acc:  52.50; ppl:  9.29; xent: 2.23; lr: 0.00000571;   0/725 tok/s; 202605 sec
[2020-04-01 03:57:34,755 INFO] Step 122600/210000; acc:  51.94; ppl: 10.06; xent: 2.31; lr: 0.00000571;   0/777 tok/s; 202690 sec
[2020-04-01 03:58:11,098 INFO] Loading train dataset from ../bert_data/cnndm.train.37.bert.pt, number of examples: 1999
[2020-04-01 03:58:59,887 INFO] Step 122650/210000; acc:  54.70; ppl:  7.93; xent: 2.07; lr: 0.00000571;   0/880 tok/s; 202775 sec
[2020-04-01 04:00:23,903 INFO] Step 122700/210000; acc:  51.98; ppl:  9.32; xent: 2.23; lr: 0.00000571;   0/743 tok/s; 202859 sec
[2020-04-01 04:00:58,154 INFO] Loading train dataset from ../bert_data/cnndm.train.38.bert.pt, number of examples: 2001
[2020-04-01 04:01:48,567 INFO] Step 122750/210000; acc:  53.66; ppl:  8.72; xent: 2.17; lr: 0.00000571;   0/853 tok/s; 202943 sec
[2020-04-01 04:03:13,307 INFO] Step 122800/210000; acc:  50.94; ppl:  9.81; xent: 2.28; lr: 0.00000571;   0/975 tok/s; 203028 sec
[2020-04-01 04:03:47,677 INFO] Loading train dataset from ../bert_data/cnndm.train.39.bert.pt, number of examples: 2000
[2020-04-01 04:04:38,206 INFO] Step 122850/210000; acc:  48.63; ppl: 11.43; xent: 2.44; lr: 0.00000571;   0/837 tok/s; 203113 sec
[2020-04-01 04:06:02,583 INFO] Step 122900/210000; acc:  52.02; ppl: 10.04; xent: 2.31; lr: 0.00000570;   0/968 tok/s; 203197 sec
[2020-04-01 04:06:35,355 INFO] Loading train dataset from ../bert_data/cnndm.train.4.bert.pt, number of examples: 2001
[2020-04-01 04:07:27,867 INFO] Step 122950/210000; acc:  53.89; ppl:  8.27; xent: 2.11; lr: 0.00000570;   0/663 tok/s; 203283 sec
[2020-04-01 04:08:51,750 INFO] Step 123000/210000; acc:  49.79; ppl: 11.33; xent: 2.43; lr: 0.00000570;   0/834 tok/s; 203367 sec
[2020-04-01 04:08:51,772 INFO] Saving checkpoint ../models/model_step_123000.pt
[2020-04-01 04:09:25,212 INFO] Loading train dataset from ../bert_data/cnndm.train.40.bert.pt, number of examples: 1999
[2020-04-01 04:10:19,335 INFO] Step 123050/210000; acc:  52.95; ppl:  8.58; xent: 2.15; lr: 0.00000570;   0/563 tok/s; 203454 sec
[2020-04-01 04:11:43,229 INFO] Step 123100/210000; acc:  52.99; ppl:  7.89; xent: 2.07; lr: 0.00000570;   0/587 tok/s; 203538 sec
[2020-04-01 04:12:12,162 INFO] Loading train dataset from ../bert_data/cnndm.train.41.bert.pt, number of examples: 2000
[2020-04-01 04:13:07,796 INFO] Step 123150/210000; acc:  56.67; ppl:  7.35; xent: 1.99; lr: 0.00000570;   0/505 tok/s; 203623 sec
[2020-04-01 04:14:31,712 INFO] Step 123200/210000; acc:  56.70; ppl:  6.82; xent: 1.92; lr: 0.00000570;   0/479 tok/s; 203707 sec
[2020-04-01 04:15:01,176 INFO] Loading train dataset from ../bert_data/cnndm.train.42.bert.pt, number of examples: 2000
[2020-04-01 04:15:56,990 INFO] Step 123250/210000; acc:  54.59; ppl:  9.04; xent: 2.20; lr: 0.00000570;   0/729 tok/s; 203792 sec
[2020-04-01 04:17:21,324 INFO] Step 123300/210000; acc:  55.46; ppl:  8.57; xent: 2.15; lr: 0.00000570;   0/807 tok/s; 203876 sec
[2020-04-01 04:17:49,118 INFO] Loading train dataset from ../bert_data/cnndm.train.43.bert.pt, number of examples: 2001
[2020-04-01 04:18:46,244 INFO] Step 123350/210000; acc:  54.00; ppl:  8.57; xent: 2.15; lr: 0.00000569;   0/556 tok/s; 203961 sec
[2020-04-01 04:20:10,570 INFO] Step 123400/210000; acc:  55.02; ppl:  7.76; xent: 2.05; lr: 0.00000569;   0/629 tok/s; 204045 sec
[2020-04-01 04:20:36,587 INFO] Loading train dataset from ../bert_data/cnndm.train.44.bert.pt, number of examples: 1998
[2020-04-01 04:21:35,791 INFO] Step 123450/210000; acc:  58.07; ppl:  7.19; xent: 1.97; lr: 0.00000569;   0/895 tok/s; 204131 sec
[2020-04-01 04:23:00,531 INFO] Step 123500/210000; acc:  59.52; ppl:  7.36; xent: 2.00; lr: 0.00000569;   0/937 tok/s; 204215 sec
[2020-04-01 04:23:00,557 INFO] Saving checkpoint ../models/model_step_123500.pt
[2020-04-01 04:23:27,290 INFO] Loading train dataset from ../bert_data/cnndm.train.45.bert.pt, number of examples: 2001
[2020-04-01 04:24:28,097 INFO] Step 123550/210000; acc:  53.55; ppl:  9.27; xent: 2.23; lr: 0.00000569;   0/1015 tok/s; 204303 sec
[2020-04-01 04:25:52,193 INFO] Step 123600/210000; acc:  48.83; ppl: 10.99; xent: 2.40; lr: 0.00000569;   0/979 tok/s; 204387 sec
[2020-04-01 04:26:16,633 INFO] Loading train dataset from ../bert_data/cnndm.train.46.bert.pt, number of examples: 2001
[2020-04-01 04:27:17,095 INFO] Step 123650/210000; acc:  54.10; ppl:  8.69; xent: 2.16; lr: 0.00000569;   0/747 tok/s; 204472 sec
[2020-04-01 04:28:40,915 INFO] Step 123700/210000; acc:  55.75; ppl:  8.07; xent: 2.09; lr: 0.00000569;   0/869 tok/s; 204556 sec
[2020-04-01 04:29:03,609 INFO] Loading train dataset from ../bert_data/cnndm.train.47.bert.pt, number of examples: 2000
[2020-04-01 04:30:05,938 INFO] Step 123750/210000; acc:  52.92; ppl:  9.00; xent: 2.20; lr: 0.00000569;   0/549 tok/s; 204641 sec
[2020-04-01 04:31:30,156 INFO] Step 123800/210000; acc:  52.30; ppl: 10.35; xent: 2.34; lr: 0.00000568;   0/836 tok/s; 204725 sec
[2020-04-01 04:31:50,719 INFO] Loading train dataset from ../bert_data/cnndm.train.48.bert.pt, number of examples: 2000
[2020-04-01 04:32:54,637 INFO] Step 123850/210000; acc:  54.30; ppl:  7.79; xent: 2.05; lr: 0.00000568;   0/584 tok/s; 204809 sec
[2020-04-01 04:34:19,059 INFO] Step 123900/210000; acc:  50.65; ppl:  9.35; xent: 2.24; lr: 0.00000568;   0/651 tok/s; 204894 sec
[2020-04-01 04:34:40,331 INFO] Loading train dataset from ../bert_data/cnndm.train.49.bert.pt, number of examples: 2001
[2020-04-01 04:35:44,472 INFO] Step 123950/210000; acc:  49.75; ppl: 10.50; xent: 2.35; lr: 0.00000568;   0/727 tok/s; 204979 sec
[2020-04-01 04:37:08,600 INFO] Step 124000/210000; acc:  50.75; ppl: 10.21; xent: 2.32; lr: 0.00000568;   0/595 tok/s; 205063 sec
[2020-04-01 04:37:08,603 INFO] Saving checkpoint ../models/model_step_124000.pt
[2020-04-01 04:37:30,057 INFO] Loading train dataset from ../bert_data/cnndm.train.5.bert.pt, number of examples: 2001
[2020-04-01 04:38:35,429 INFO] Step 124050/210000; acc:  56.35; ppl:  7.86; xent: 2.06; lr: 0.00000568;   0/821 tok/s; 205150 sec
[2020-04-01 04:39:59,579 INFO] Step 124100/210000; acc:  56.92; ppl:  7.73; xent: 2.04; lr: 0.00000568;   0/795 tok/s; 205234 sec
[2020-04-01 04:40:18,744 INFO] Loading train dataset from ../bert_data/cnndm.train.50.bert.pt, number of examples: 2001
[2020-04-01 04:41:24,049 INFO] Step 124150/210000; acc:  54.27; ppl:  8.27; xent: 2.11; lr: 0.00000568;   0/812 tok/s; 205319 sec
[2020-04-01 04:42:47,937 INFO] Step 124200/210000; acc:  52.50; ppl:  8.80; xent: 2.18; lr: 0.00000568;   0/630 tok/s; 205403 sec
[2020-04-01 04:43:05,478 INFO] Loading train dataset from ../bert_data/cnndm.train.51.bert.pt, number of examples: 2000
[2020-04-01 04:44:12,461 INFO] Step 124250/210000; acc:  50.11; ppl: 10.16; xent: 2.32; lr: 0.00000567;   0/823 tok/s; 205487 sec
[2020-04-01 04:45:36,582 INFO] Step 124300/210000; acc:  57.50; ppl:  7.16; xent: 1.97; lr: 0.00000567;   0/627 tok/s; 205571 sec
[2020-04-01 04:45:54,067 INFO] Loading train dataset from ../bert_data/cnndm.train.52.bert.pt, number of examples: 2001
[2020-04-01 04:47:01,650 INFO] Step 124350/210000; acc:  56.59; ppl:  7.79; xent: 2.05; lr: 0.00000567;   0/848 tok/s; 205656 sec
[2020-04-01 04:48:25,608 INFO] Step 124400/210000; acc:  54.83; ppl:  8.71; xent: 2.17; lr: 0.00000567;   0/842 tok/s; 205740 sec
[2020-04-01 04:48:41,346 INFO] Loading train dataset from ../bert_data/cnndm.train.53.bert.pt, number of examples: 1999
[2020-04-01 04:49:50,157 INFO] Step 124450/210000; acc:  54.28; ppl:  9.53; xent: 2.25; lr: 0.00000567;   0/745 tok/s; 205825 sec
[2020-04-01 04:51:14,426 INFO] Step 124500/210000; acc:  56.40; ppl:  7.16; xent: 1.97; lr: 0.00000567;   0/915 tok/s; 205909 sec
[2020-04-01 04:51:14,450 INFO] Saving checkpoint ../models/model_step_124500.pt
[2020-04-01 04:51:30,935 INFO] Loading train dataset from ../bert_data/cnndm.train.54.bert.pt, number of examples: 2001
[2020-04-01 04:52:41,506 INFO] Step 124550/210000; acc:  51.59; ppl: 10.28; xent: 2.33; lr: 0.00000567;   0/516 tok/s; 205996 sec
[2020-04-01 04:54:05,888 INFO] Step 124600/210000; acc:  50.10; ppl: 10.28; xent: 2.33; lr: 0.00000567;   0/859 tok/s; 206081 sec
[2020-04-01 04:54:20,225 INFO] Loading train dataset from ../bert_data/cnndm.train.55.bert.pt, number of examples: 1999
[2020-04-01 04:55:30,721 INFO] Step 124650/210000; acc:  55.37; ppl:  8.19; xent: 2.10; lr: 0.00000566;   0/635 tok/s; 206166 sec
[2020-04-01 04:56:55,115 INFO] Step 124700/210000; acc:  60.87; ppl:  5.91; xent: 1.78; lr: 0.00000566;   0/793 tok/s; 206250 sec
[2020-04-01 04:57:07,543 INFO] Loading train dataset from ../bert_data/cnndm.train.56.bert.pt, number of examples: 2001
[2020-04-01 04:58:20,095 INFO] Step 124750/210000; acc:  54.91; ppl:  8.71; xent: 2.16; lr: 0.00000566;   0/505 tok/s; 206335 sec
[2020-04-01 04:59:44,476 INFO] Step 124800/210000; acc:  54.53; ppl:  7.93; xent: 2.07; lr: 0.00000566;   0/553 tok/s; 206419 sec
[2020-04-01 04:59:55,201 INFO] Loading train dataset from ../bert_data/cnndm.train.57.bert.pt, number of examples: 1999
[2020-04-01 05:01:09,006 INFO] Step 124850/210000; acc:  59.66; ppl:  6.64; xent: 1.89; lr: 0.00000566;   0/818 tok/s; 206504 sec
[2020-04-01 05:02:33,297 INFO] Step 124900/210000; acc:  53.36; ppl:  8.76; xent: 2.17; lr: 0.00000566;   0/862 tok/s; 206588 sec
[2020-04-01 05:02:42,481 INFO] Loading train dataset from ../bert_data/cnndm.train.58.bert.pt, number of examples: 2000
[2020-04-01 05:03:58,385 INFO] Step 124950/210000; acc:  54.54; ppl:  8.27; xent: 2.11; lr: 0.00000566;   0/943 tok/s; 206673 sec
[2020-04-01 05:05:22,868 INFO] Step 125000/210000; acc:  54.13; ppl:  8.76; xent: 2.17; lr: 0.00000566;   0/796 tok/s; 206758 sec
[2020-04-01 05:05:22,895 INFO] Saving checkpoint ../models/model_step_125000.pt
[2020-04-01 05:05:32,633 INFO] Loading train dataset from ../bert_data/cnndm.train.59.bert.pt, number of examples: 2000
[2020-04-01 05:06:50,826 INFO] Step 125050/210000; acc:  54.87; ppl:  7.72; xent: 2.04; lr: 0.00000566;   0/788 tok/s; 206846 sec
[2020-04-01 05:08:14,933 INFO] Step 125100/210000; acc:  58.54; ppl:  6.96; xent: 1.94; lr: 0.00000565;   0/814 tok/s; 206930 sec
[2020-04-01 05:08:20,722 INFO] Loading train dataset from ../bert_data/cnndm.train.6.bert.pt, number of examples: 2001
[2020-04-01 05:09:40,078 INFO] Step 125150/210000; acc:  56.23; ppl:  7.06; xent: 1.95; lr: 0.00000565;   0/636 tok/s; 207015 sec
[2020-04-01 05:11:04,583 INFO] Step 125200/210000; acc:  54.39; ppl:  7.80; xent: 2.05; lr: 0.00000565;   0/625 tok/s; 207099 sec
[2020-04-01 05:11:10,453 INFO] Loading train dataset from ../bert_data/cnndm.train.60.bert.pt, number of examples: 2000
[2020-04-01 05:12:29,541 INFO] Step 125250/210000; acc:  52.23; ppl:  8.55; xent: 2.15; lr: 0.00000565;   0/518 tok/s; 207184 sec
[2020-04-01 05:13:53,843 INFO] Step 125300/210000; acc:  52.03; ppl:  9.32; xent: 2.23; lr: 0.00000565;   0/620 tok/s; 207269 sec
[2020-04-01 05:13:57,972 INFO] Loading train dataset from ../bert_data/cnndm.train.61.bert.pt, number of examples: 2001
[2020-04-01 05:15:18,694 INFO] Step 125350/210000; acc:  55.69; ppl:  7.86; xent: 2.06; lr: 0.00000565;   0/809 tok/s; 207354 sec
[2020-04-01 05:16:42,742 INFO] Step 125400/210000; acc:  50.08; ppl: 10.30; xent: 2.33; lr: 0.00000565;   0/778 tok/s; 207438 sec
[2020-04-01 05:16:45,192 INFO] Loading train dataset from ../bert_data/cnndm.train.62.bert.pt, number of examples: 2001
[2020-04-01 05:18:07,905 INFO] Step 125450/210000; acc:  54.92; ppl:  8.57; xent: 2.15; lr: 0.00000565;   0/801 tok/s; 207523 sec
[2020-04-01 05:19:32,230 INFO] Step 125500/210000; acc:  52.91; ppl:  9.02; xent: 2.20; lr: 0.00000565;   0/824 tok/s; 207607 sec
[2020-04-01 05:19:32,233 INFO] Saving checkpoint ../models/model_step_125500.pt
[2020-04-01 05:19:36,473 INFO] Loading train dataset from ../bert_data/cnndm.train.63.bert.pt, number of examples: 2001
[2020-04-01 05:20:58,765 INFO] Step 125550/210000; acc:  54.13; ppl:  8.20; xent: 2.10; lr: 0.00000564;   0/695 tok/s; 207694 sec
[2020-04-01 05:22:23,396 INFO] Step 125600/210000; acc:  51.49; ppl:  9.08; xent: 2.21; lr: 0.00000564;   0/526 tok/s; 207778 sec
[2020-04-01 05:22:23,749 INFO] Loading train dataset from ../bert_data/cnndm.train.64.bert.pt, number of examples: 2001
[2020-04-01 05:23:47,954 INFO] Step 125650/210000; acc:  55.12; ppl:  8.04; xent: 2.08; lr: 0.00000564;   0/770 tok/s; 207863 sec
[2020-04-01 05:25:11,436 INFO] Loading train dataset from ../bert_data/cnndm.train.65.bert.pt, number of examples: 2000
[2020-04-01 05:25:13,258 INFO] Step 125700/210000; acc:  50.80; ppl:  9.69; xent: 2.27; lr: 0.00000564;   0/457 tok/s; 207948 sec
[2020-04-01 05:26:37,309 INFO] Step 125750/210000; acc:  48.74; ppl: 11.33; xent: 2.43; lr: 0.00000564;   0/987 tok/s; 208032 sec
[2020-04-01 05:27:59,018 INFO] Loading train dataset from ../bert_data/cnndm.train.66.bert.pt, number of examples: 2001
[2020-04-01 05:28:02,380 INFO] Step 125800/210000; acc:  57.22; ppl:  6.39; xent: 1.85; lr: 0.00000564;   0/562 tok/s; 208117 sec
[2020-04-01 05:29:26,397 INFO] Step 125850/210000; acc:  56.94; ppl:  6.59; xent: 1.89; lr: 0.00000564;   0/750 tok/s; 208201 sec
[2020-04-01 05:30:47,965 INFO] Loading train dataset from ../bert_data/cnndm.train.67.bert.pt, number of examples: 1999
[2020-04-01 05:30:51,466 INFO] Step 125900/210000; acc:  51.13; ppl:  9.25; xent: 2.23; lr: 0.00000564;   0/613 tok/s; 208286 sec
[2020-04-01 05:32:15,728 INFO] Step 125950/210000; acc:  53.39; ppl:  9.06; xent: 2.20; lr: 0.00000564;   0/500 tok/s; 208371 sec
[2020-04-01 05:33:35,410 INFO] Loading train dataset from ../bert_data/cnndm.train.68.bert.pt, number of examples: 1999
[2020-04-01 05:33:40,382 INFO] Step 126000/210000; acc:  53.89; ppl:  8.06; xent: 2.09; lr: 0.00000563;   0/705 tok/s; 208455 sec
[2020-04-01 05:33:40,385 INFO] Saving checkpoint ../models/model_step_126000.pt
[2020-04-01 05:35:07,041 INFO] Step 126050/210000; acc:  55.42; ppl:  8.30; xent: 2.12; lr: 0.00000563;   0/626 tok/s; 208542 sec
[2020-04-01 05:36:25,715 INFO] Loading train dataset from ../bert_data/cnndm.train.69.bert.pt, number of examples: 2000
[2020-04-01 05:36:32,425 INFO] Step 126100/210000; acc:  50.33; ppl: 10.62; xent: 2.36; lr: 0.00000563;   0/817 tok/s; 208627 sec
[2020-04-01 05:37:56,587 INFO] Step 126150/210000; acc:  52.10; ppl:  9.21; xent: 2.22; lr: 0.00000563;   0/893 tok/s; 208711 sec
[2020-04-01 05:39:13,459 INFO] Loading train dataset from ../bert_data/cnndm.train.7.bert.pt, number of examples: 2001
[2020-04-01 05:39:21,905 INFO] Step 126200/210000; acc:  51.50; ppl:  9.38; xent: 2.24; lr: 0.00000563;   0/890 tok/s; 208797 sec
[2020-04-01 05:40:46,066 INFO] Step 126250/210000; acc:  52.65; ppl:  9.67; xent: 2.27; lr: 0.00000563;   0/931 tok/s; 208881 sec
[2020-04-01 05:42:03,145 INFO] Loading train dataset from ../bert_data/cnndm.train.70.bert.pt, number of examples: 2000
[2020-04-01 05:42:11,639 INFO] Step 126300/210000; acc:  49.07; ppl: 10.82; xent: 2.38; lr: 0.00000563;   0/846 tok/s; 208966 sec
[2020-04-01 05:43:35,742 INFO] Step 126350/210000; acc:  54.29; ppl:  8.33; xent: 2.12; lr: 0.00000563;   0/874 tok/s; 209051 sec
[2020-04-01 05:44:50,039 INFO] Loading train dataset from ../bert_data/cnndm.train.71.bert.pt, number of examples: 1999
[2020-04-01 05:45:00,270 INFO] Step 126400/210000; acc:  50.47; ppl:  9.39; xent: 2.24; lr: 0.00000563;   0/696 tok/s; 209135 sec
[2020-04-01 05:46:24,890 INFO] Step 126450/210000; acc:  52.62; ppl:  9.16; xent: 2.21; lr: 0.00000562;   0/747 tok/s; 209220 sec
[2020-04-01 05:47:38,012 INFO] Loading train dataset from ../bert_data/cnndm.train.72.bert.pt, number of examples: 2000
[2020-04-01 05:47:49,922 INFO] Step 126500/210000; acc:  57.80; ppl:  6.84; xent: 1.92; lr: 0.00000562;   0/534 tok/s; 209305 sec
[2020-04-01 05:47:49,925 INFO] Saving checkpoint ../models/model_step_126500.pt
[2020-04-01 05:49:17,024 INFO] Step 126550/210000; acc:  50.47; ppl:  9.56; xent: 2.26; lr: 0.00000562;   0/1067 tok/s; 209392 sec
[2020-04-01 05:50:28,386 INFO] Loading train dataset from ../bert_data/cnndm.train.73.bert.pt, number of examples: 2000
[2020-04-01 05:50:41,741 INFO] Step 126600/210000; acc:  56.78; ppl:  7.74; xent: 2.05; lr: 0.00000562;   0/679 tok/s; 209477 sec
[2020-04-01 05:52:06,002 INFO] Step 126650/210000; acc:  59.76; ppl:  6.92; xent: 1.93; lr: 0.00000562;   0/698 tok/s; 209561 sec
[2020-04-01 05:53:17,580 INFO] Loading train dataset from ../bert_data/cnndm.train.74.bert.pt, number of examples: 2000
[2020-04-01 05:53:31,087 INFO] Step 126700/210000; acc:  57.90; ppl:  7.45; xent: 2.01; lr: 0.00000562;   0/779 tok/s; 209646 sec
[2020-04-01 05:54:55,385 INFO] Step 126750/210000; acc:  53.42; ppl:  8.87; xent: 2.18; lr: 0.00000562;   0/485 tok/s; 209730 sec
[2020-04-01 05:56:05,090 INFO] Loading train dataset from ../bert_data/cnndm.train.75.bert.pt, number of examples: 1999
[2020-04-01 05:56:20,255 INFO] Step 126800/210000; acc:  49.55; ppl: 11.44; xent: 2.44; lr: 0.00000562;   0/866 tok/s; 209815 sec
[2020-04-01 05:57:44,849 INFO] Step 126850/210000; acc:  53.12; ppl:  9.63; xent: 2.26; lr: 0.00000562;   0/637 tok/s; 209900 sec
[2020-04-01 05:58:52,795 INFO] Loading train dataset from ../bert_data/cnndm.train.76.bert.pt, number of examples: 1999
[2020-04-01 05:59:09,727 INFO] Step 126900/210000; acc:  57.29; ppl:  6.72; xent: 1.91; lr: 0.00000561;   0/820 tok/s; 209985 sec
[2020-04-01 06:00:33,795 INFO] Step 126950/210000; acc:  52.76; ppl:  8.57; xent: 2.15; lr: 0.00000561;   0/859 tok/s; 210069 sec
[2020-04-01 06:01:39,788 INFO] Loading train dataset from ../bert_data/cnndm.train.77.bert.pt, number of examples: 2001
[2020-04-01 06:01:58,613 INFO] Step 127000/210000; acc:  49.97; ppl: 10.98; xent: 2.40; lr: 0.00000561;   0/816 tok/s; 210153 sec
[2020-04-01 06:01:58,617 INFO] Saving checkpoint ../models/model_step_127000.pt
[2020-04-01 06:03:25,222 INFO] Step 127050/210000; acc:  54.63; ppl:  7.79; xent: 2.05; lr: 0.00000561;   0/909 tok/s; 210240 sec
[2020-04-01 06:04:31,601 INFO] Loading train dataset from ../bert_data/cnndm.train.78.bert.pt, number of examples: 2001
[2020-04-01 06:04:50,040 INFO] Step 127100/210000; acc:  59.02; ppl:  6.55; xent: 1.88; lr: 0.00000561;   0/496 tok/s; 210325 sec
[2020-04-01 06:06:13,954 INFO] Step 127150/210000; acc:  61.39; ppl:  5.71; xent: 1.74; lr: 0.00000561;   0/525 tok/s; 210409 sec
[2020-04-01 06:07:18,505 INFO] Loading train dataset from ../bert_data/cnndm.train.79.bert.pt, number of examples: 1999
[2020-04-01 06:07:38,535 INFO] Step 127200/210000; acc:  50.41; ppl: 10.55; xent: 2.36; lr: 0.00000561;   0/516 tok/s; 210493 sec
[2020-04-01 06:09:02,969 INFO] Step 127250/210000; acc:  59.36; ppl:  6.52; xent: 1.87; lr: 0.00000561;   0/568 tok/s; 210578 sec
[2020-04-01 06:10:05,944 INFO] Loading train dataset from ../bert_data/cnndm.train.8.bert.pt, number of examples: 2000
[2020-04-01 06:10:28,012 INFO] Step 127300/210000; acc:  52.10; ppl:  9.75; xent: 2.28; lr: 0.00000561;   0/654 tok/s; 210663 sec
[2020-04-01 06:11:52,642 INFO] Step 127350/210000; acc:  50.71; ppl:  9.96; xent: 2.30; lr: 0.00000560;   0/679 tok/s; 210747 sec
[2020-04-01 06:12:53,977 INFO] Loading train dataset from ../bert_data/cnndm.train.80.bert.pt, number of examples: 1999
[2020-04-01 06:13:17,969 INFO] Step 127400/210000; acc:  51.10; ppl:  9.27; xent: 2.23; lr: 0.00000560;   0/767 tok/s; 210833 sec
[2020-04-01 06:14:42,283 INFO] Step 127450/210000; acc:  55.16; ppl:  9.07; xent: 2.20; lr: 0.00000560;   0/751 tok/s; 210917 sec
[2020-04-01 06:15:41,857 INFO] Loading train dataset from ../bert_data/cnndm.train.81.bert.pt, number of examples: 2000
[2020-04-01 06:16:07,165 INFO] Step 127500/210000; acc:  55.65; ppl:  8.64; xent: 2.16; lr: 0.00000560;   0/945 tok/s; 211002 sec
[2020-04-01 06:16:07,169 INFO] Saving checkpoint ../models/model_step_127500.pt
[2020-04-01 06:17:33,831 INFO] Step 127550/210000; acc:  48.06; ppl: 13.64; xent: 2.61; lr: 0.00000560;   0/817 tok/s; 211089 sec
[2020-04-01 06:18:33,266 INFO] Loading train dataset from ../bert_data/cnndm.train.82.bert.pt, number of examples: 2001
[2020-04-01 06:18:58,620 INFO] Step 127600/210000; acc:  50.03; ppl: 11.74; xent: 2.46; lr: 0.00000560;   0/865 tok/s; 211173 sec
[2020-04-01 06:20:22,748 INFO] Step 127650/210000; acc:  48.32; ppl: 12.17; xent: 2.50; lr: 0.00000560;   0/981 tok/s; 211258 sec
[2020-04-01 06:21:20,895 INFO] Loading train dataset from ../bert_data/cnndm.train.83.bert.pt, number of examples: 1999
[2020-04-01 06:21:48,038 INFO] Step 127700/210000; acc:  57.19; ppl:  7.91; xent: 2.07; lr: 0.00000560;   0/789 tok/s; 211343 sec
[2020-04-01 06:23:12,488 INFO] Step 127750/210000; acc:  56.93; ppl:  7.69; xent: 2.04; lr: 0.00000560;   0/667 tok/s; 211427 sec
[2020-04-01 06:24:08,950 INFO] Loading train dataset from ../bert_data/cnndm.train.84.bert.pt, number of examples: 2000
[2020-04-01 06:24:37,604 INFO] Step 127800/210000; acc:  56.33; ppl:  7.18; xent: 1.97; lr: 0.00000559;   0/593 tok/s; 211512 sec
[2020-04-01 06:26:01,947 INFO] Step 127850/210000; acc:  54.66; ppl:  9.16; xent: 2.22; lr: 0.00000559;   0/610 tok/s; 211597 sec
[2020-04-01 06:26:56,667 INFO] Loading train dataset from ../bert_data/cnndm.train.85.bert.pt, number of examples: 2001
[2020-04-01 06:27:27,063 INFO] Step 127900/210000; acc:  50.81; ppl:  9.88; xent: 2.29; lr: 0.00000559;   0/743 tok/s; 211682 sec
[2020-04-01 06:28:51,542 INFO] Step 127950/210000; acc:  55.92; ppl:  7.69; xent: 2.04; lr: 0.00000559;   0/691 tok/s; 211766 sec
[2020-04-01 06:29:44,484 INFO] Loading train dataset from ../bert_data/cnndm.train.86.bert.pt, number of examples: 1999
[2020-04-01 06:30:16,471 INFO] Step 128000/210000; acc:  55.25; ppl:  7.52; xent: 2.02; lr: 0.00000559;   0/779 tok/s; 211851 sec
[2020-04-01 06:30:16,474 INFO] Saving checkpoint ../models/model_step_128000.pt
[2020-04-01 06:31:43,280 INFO] Step 128050/210000; acc:  52.81; ppl:  8.34; xent: 2.12; lr: 0.00000559;   0/715 tok/s; 211938 sec
[2020-04-01 06:32:34,587 INFO] Loading train dataset from ../bert_data/cnndm.train.87.bert.pt, number of examples: 1999
[2020-04-01 06:33:08,556 INFO] Step 128100/210000; acc:  53.90; ppl:  8.93; xent: 2.19; lr: 0.00000559;   0/941 tok/s; 212023 sec
[2020-04-01 06:34:33,380 INFO] Step 128150/210000; acc:  51.70; ppl: 10.07; xent: 2.31; lr: 0.00000559;   0/844 tok/s; 212108 sec
[2020-04-01 06:35:22,932 INFO] Loading train dataset from ../bert_data/cnndm.train.88.bert.pt, number of examples: 1999
[2020-04-01 06:35:58,358 INFO] Step 128200/210000; acc:  52.38; ppl:  9.29; xent: 2.23; lr: 0.00000559;   0/702 tok/s; 212193 sec
[2020-04-01 06:37:22,708 INFO] Step 128250/210000; acc:  53.50; ppl:  9.61; xent: 2.26; lr: 0.00000558;   0/813 tok/s; 212278 sec
[2020-04-01 06:38:12,188 INFO] Loading train dataset from ../bert_data/cnndm.train.89.bert.pt, number of examples: 2001
[2020-04-01 06:38:47,334 INFO] Step 128300/210000; acc:  52.20; ppl:  9.05; xent: 2.20; lr: 0.00000558;   0/736 tok/s; 212362 sec
[2020-04-01 06:40:11,818 INFO] Step 128350/210000; acc:  48.32; ppl: 10.53; xent: 2.35; lr: 0.00000558;   0/808 tok/s; 212447 sec
[2020-04-01 06:40:59,392 INFO] Loading train dataset from ../bert_data/cnndm.train.9.bert.pt, number of examples: 1999
[2020-04-01 06:41:36,440 INFO] Step 128400/210000; acc:  56.49; ppl:  8.28; xent: 2.11; lr: 0.00000558;   0/632 tok/s; 212531 sec
[2020-04-01 06:43:00,511 INFO] Step 128450/210000; acc:  58.47; ppl:  6.53; xent: 1.88; lr: 0.00000558;   0/582 tok/s; 212615 sec
[2020-04-01 06:43:46,887 INFO] Loading train dataset from ../bert_data/cnndm.train.90.bert.pt, number of examples: 2000
[2020-04-01 06:44:25,959 INFO] Step 128500/210000; acc:  52.86; ppl:  9.85; xent: 2.29; lr: 0.00000558;   0/710 tok/s; 212701 sec
[2020-04-01 06:44:25,961 INFO] Saving checkpoint ../models/model_step_128500.pt
[2020-04-01 06:45:53,294 INFO] Step 128550/210000; acc:  54.42; ppl:  6.86; xent: 1.93; lr: 0.00000558;   0/460 tok/s; 212788 sec
[2020-04-01 06:46:39,559 INFO] Loading train dataset from ../bert_data/cnndm.train.9000.bert.pt, number of examples: 1984
[2020-04-01 06:47:17,773 INFO] Step 128600/210000; acc:  43.07; ppl: 14.39; xent: 2.67; lr: 0.00000558;   0/1372 tok/s; 212873 sec
[2020-04-01 06:48:39,934 INFO] Step 128650/210000; acc:  51.83; ppl:  8.14; xent: 2.10; lr: 0.00000558;   0/785 tok/s; 212955 sec
[2020-04-01 06:49:15,572 INFO] Loading train dataset from ../bert_data/cnndm.train.9001.bert.pt, number of examples: 1983
[2020-04-01 06:50:03,048 INFO] Step 128700/210000; acc:  47.54; ppl: 11.40; xent: 2.43; lr: 0.00000557;   0/965 tok/s; 213038 sec
[2020-04-01 06:51:25,545 INFO] Step 128750/210000; acc:  50.32; ppl:  7.89; xent: 2.07; lr: 0.00000557;   0/581 tok/s; 213120 sec
[2020-04-01 06:51:52,430 INFO] Loading train dataset from ../bert_data/cnndm.train.9002.bert.pt, number of examples: 1990
[2020-04-01 06:52:48,713 INFO] Step 128800/210000; acc:  44.07; ppl: 12.90; xent: 2.56; lr: 0.00000557;   0/687 tok/s; 213204 sec
[2020-04-01 06:54:10,915 INFO] Step 128850/210000; acc:  51.55; ppl:  8.86; xent: 2.18; lr: 0.00000557;   0/808 tok/s; 213286 sec
[2020-04-01 06:54:29,681 INFO] Loading train dataset from ../bert_data/cnndm.train.9003.bert.pt, number of examples: 1985
[2020-04-01 06:55:34,853 INFO] Step 128900/210000; acc:  52.14; ppl:  7.08; xent: 1.96; lr: 0.00000557;   0/649 tok/s; 213370 sec
[2020-04-01 06:56:56,545 INFO] Step 128950/210000; acc:  47.68; ppl: 10.02; xent: 2.30; lr: 0.00000557;   0/874 tok/s; 213451 sec
[2020-04-01 06:57:07,224 INFO] Loading train dataset from ../bert_data/cnndm.train.9004.bert.pt, number of examples: 1981
[2020-04-01 06:58:19,667 INFO] Step 129000/210000; acc:  49.57; ppl:  8.98; xent: 2.19; lr: 0.00000557;   0/630 tok/s; 213534 sec
[2020-04-01 06:58:19,670 INFO] Saving checkpoint ../models/model_step_129000.pt
[2020-04-01 06:59:44,619 INFO] Step 129050/210000; acc:  48.22; ppl: 10.63; xent: 2.36; lr: 0.00000557;   0/1082 tok/s; 213619 sec
[2020-04-01 06:59:47,000 INFO] Loading train dataset from ../bert_data/cnndm.train.9005.bert.pt, number of examples: 1986
[2020-04-01 07:01:07,584 INFO] Step 129100/210000; acc:  52.85; ppl:  6.55; xent: 1.88; lr: 0.00000557;   0/478 tok/s; 213702 sec
[2020-04-01 07:02:25,037 INFO] Loading train dataset from ../bert_data/cnndm.train.9006.bert.pt, number of examples: 1993
[2020-04-01 07:02:31,450 INFO] Step 129150/210000; acc:  43.62; ppl: 13.45; xent: 2.60; lr: 0.00000557;   0/1220 tok/s; 213786 sec
[2020-04-01 07:03:53,834 INFO] Step 129200/210000; acc:  56.31; ppl:  5.71; xent: 1.74; lr: 0.00000556;   0/512 tok/s; 213869 sec
[2020-04-01 07:05:03,076 INFO] Loading train dataset from ../bert_data/cnndm.train.9007.bert.pt, number of examples: 1983
[2020-04-01 07:05:17,711 INFO] Step 129250/210000; acc:  43.70; ppl: 13.34; xent: 2.59; lr: 0.00000556;   0/1298 tok/s; 213953 sec
[2020-04-01 07:06:40,357 INFO] Step 129300/210000; acc:  51.35; ppl:  7.98; xent: 2.08; lr: 0.00000556;   0/589 tok/s; 214035 sec
[2020-04-01 07:07:39,380 INFO] Loading train dataset from ../bert_data/cnndm.train.9008.bert.pt, number of examples: 1990
[2020-04-01 07:08:03,908 INFO] Step 129350/210000; acc:  46.68; ppl: 10.74; xent: 2.37; lr: 0.00000556;   0/1035 tok/s; 214119 sec
[2020-04-01 07:09:26,818 INFO] Step 129400/210000; acc:  52.85; ppl:  6.57; xent: 1.88; lr: 0.00000556;   0/825 tok/s; 214202 sec
[2020-04-01 07:10:17,301 INFO] Loading train dataset from ../bert_data/cnndm.train.9009.bert.pt, number of examples: 1988
[2020-04-01 07:10:51,083 INFO] Step 129450/210000; acc:  50.40; ppl:  9.87; xent: 2.29; lr: 0.00000556;   0/744 tok/s; 214286 sec
[2020-04-01 07:12:14,082 INFO] Step 129500/210000; acc:  49.89; ppl:  9.45; xent: 2.25; lr: 0.00000556;   0/830 tok/s; 214369 sec
[2020-04-01 07:12:14,086 INFO] Saving checkpoint ../models/model_step_129500.pt
[2020-04-01 07:12:58,850 INFO] Loading train dataset from ../bert_data/cnndm.train.9010.bert.pt, number of examples: 1984
[2020-04-01 07:13:40,460 INFO] Step 129550/210000; acc:  58.71; ppl:  5.81; xent: 1.76; lr: 0.00000556;   0/376 tok/s; 214455 sec
[2020-04-01 07:15:02,748 INFO] Step 129600/210000; acc:  46.01; ppl: 10.83; xent: 2.38; lr: 0.00000556;   0/1192 tok/s; 214538 sec
[2020-04-01 07:15:34,563 INFO] Loading train dataset from ../bert_data/cnndm.train.9011.bert.pt, number of examples: 1983
[2020-04-01 07:16:26,582 INFO] Step 129650/210000; acc:  58.93; ppl:  5.33; xent: 1.67; lr: 0.00000555;   0/490 tok/s; 214621 sec
[2020-04-01 07:17:48,257 INFO] Step 129700/210000; acc:  49.91; ppl:  9.03; xent: 2.20; lr: 0.00000555;   0/1117 tok/s; 214703 sec
[2020-04-01 07:18:13,783 INFO] Loading train dataset from ../bert_data/cnndm.train.9012.bert.pt, number of examples: 1985
[2020-04-01 07:19:11,548 INFO] Step 129750/210000; acc:  61.13; ppl:  5.23; xent: 1.65; lr: 0.00000555;   0/491 tok/s; 214786 sec
[2020-04-01 07:20:34,775 INFO] Step 129800/210000; acc:  49.20; ppl: 10.56; xent: 2.36; lr: 0.00000555;   0/1065 tok/s; 214870 sec
[2020-04-01 07:20:52,390 INFO] Loading train dataset from ../bert_data/cnndm.train.9013.bert.pt, number of examples: 1987
[2020-04-01 07:21:59,165 INFO] Step 129850/210000; acc:  53.17; ppl:  7.53; xent: 2.02; lr: 0.00000555;   0/622 tok/s; 214954 sec
[2020-04-01 07:23:21,265 INFO] Step 129900/210000; acc:  45.89; ppl: 11.53; xent: 2.44; lr: 0.00000555;   0/1322 tok/s; 215036 sec
[2020-04-01 07:23:28,532 INFO] Loading train dataset from ../bert_data/cnndm.train.9014.bert.pt, number of examples: 1976
[2020-04-01 07:24:44,480 INFO] Step 129950/210000; acc:  48.07; ppl:  8.89; xent: 2.18; lr: 0.00000555;   0/704 tok/s; 215119 sec
[2020-04-01 07:26:05,557 INFO] Loading train dataset from ../bert_data/cnndm.train.9015.bert.pt, number of examples: 1982
[2020-04-01 07:26:08,991 INFO] Step 130000/210000; acc:  54.82; ppl:  7.49; xent: 2.01; lr: 0.00000555;   0/584 tok/s; 215204 sec
[2020-04-01 07:26:08,994 INFO] Saving checkpoint ../models/model_step_130000.pt
[2020-04-01 07:27:34,354 INFO] Step 130050/210000; acc:  47.04; ppl: 11.31; xent: 2.43; lr: 0.00000555;   0/1049 tok/s; 215289 sec
[2020-04-01 07:28:45,581 INFO] Loading train dataset from ../bert_data/cnndm.train.9016.bert.pt, number of examples: 1984
[2020-04-01 07:28:56,822 INFO] Step 130100/210000; acc:  52.15; ppl:  7.59; xent: 2.03; lr: 0.00000554;   0/639 tok/s; 215372 sec
[2020-04-01 07:30:19,429 INFO] Step 130150/210000; acc:  47.15; ppl: 10.94; xent: 2.39; lr: 0.00000554;   0/1010 tok/s; 215454 sec
[2020-04-01 07:31:22,845 INFO] Loading train dataset from ../bert_data/cnndm.train.9017.bert.pt, number of examples: 1984
[2020-04-01 07:31:43,215 INFO] Step 130200/210000; acc:  46.14; ppl: 10.75; xent: 2.38; lr: 0.00000554;   0/819 tok/s; 215538 sec
[2020-04-01 07:33:05,323 INFO] Step 130250/210000; acc:  41.67; ppl: 14.36; xent: 2.66; lr: 0.00000554;   0/1229 tok/s; 215620 sec
[2020-04-01 07:34:00,341 INFO] Loading train dataset from ../bert_data/cnndm.train.9018.bert.pt, number of examples: 1989
[2020-04-01 07:34:28,268 INFO] Step 130300/210000; acc:  49.86; ppl:  8.20; xent: 2.10; lr: 0.00000554;   0/705 tok/s; 215703 sec
[2020-04-01 07:35:50,544 INFO] Step 130350/210000; acc:  43.17; ppl: 15.54; xent: 2.74; lr: 0.00000554;   0/1269 tok/s; 215785 sec
[2020-04-01 07:36:38,091 INFO] Loading train dataset from ../bert_data/cnndm.train.9019.bert.pt, number of examples: 1986
[2020-04-01 07:37:14,381 INFO] Step 130400/210000; acc:  46.83; ppl:  9.93; xent: 2.30; lr: 0.00000554;   0/860 tok/s; 215869 sec
[2020-04-01 07:38:36,584 INFO] Step 130450/210000; acc:  45.94; ppl: 12.58; xent: 2.53; lr: 0.00000554;   0/1013 tok/s; 215951 sec
[2020-04-01 07:39:14,710 INFO] Loading train dataset from ../bert_data/cnndm.train.9020.bert.pt, number of examples: 1992
[2020-04-01 07:39:58,913 INFO] Step 130500/210000; acc:  54.27; ppl:  7.70; xent: 2.04; lr: 0.00000554;   0/887 tok/s; 216034 sec
[2020-04-01 07:39:58,917 INFO] Saving checkpoint ../models/model_step_130500.pt
[2020-04-01 07:41:23,808 INFO] Step 130550/210000; acc:  53.10; ppl:  8.15; xent: 2.10; lr: 0.00000554;   0/805 tok/s; 216119 sec
[2020-04-01 07:41:53,658 INFO] Loading train dataset from ../bert_data/cnndm.train.9021.bert.pt, number of examples: 1988
[2020-04-01 07:42:46,451 INFO] Step 130600/210000; acc:  53.00; ppl:  7.71; xent: 2.04; lr: 0.00000553;   0/742 tok/s; 216201 sec
[2020-04-01 07:44:08,972 INFO] Step 130650/210000; acc:  44.07; ppl: 12.91; xent: 2.56; lr: 0.00000553;   0/1002 tok/s; 216284 sec
[2020-04-01 07:44:30,671 INFO] Loading train dataset from ../bert_data/cnndm.train.9022.bert.pt, number of examples: 1991
[2020-04-01 07:45:31,893 INFO] Step 130700/210000; acc:  51.90; ppl:  9.19; xent: 2.22; lr: 0.00000553;   0/849 tok/s; 216367 sec
[2020-04-01 07:46:53,699 INFO] Step 130750/210000; acc:  43.49; ppl: 13.39; xent: 2.59; lr: 0.00000553;   0/1307 tok/s; 216449 sec
[2020-04-01 07:47:08,798 INFO] Loading train dataset from ../bert_data/cnndm.train.9023.bert.pt, number of examples: 1986
[2020-04-01 07:48:15,876 INFO] Step 130800/210000; acc:  56.43; ppl:  7.05; xent: 1.95; lr: 0.00000553;   0/737 tok/s; 216531 sec
[2020-04-01 07:49:38,896 INFO] Step 130850/210000; acc:  46.97; ppl: 11.07; xent: 2.40; lr: 0.00000553;   0/1276 tok/s; 216614 sec
[2020-04-01 07:49:46,181 INFO] Loading train dataset from ../bert_data/cnndm.train.9024.bert.pt, number of examples: 1989
[2020-04-01 07:51:01,644 INFO] Step 130900/210000; acc:  51.92; ppl:  8.48; xent: 2.14; lr: 0.00000553;   0/844 tok/s; 216696 sec
[2020-04-01 07:52:22,616 INFO] Loading train dataset from ../bert_data/cnndm.train.9025.bert.pt, number of examples: 1984
[2020-04-01 07:52:24,543 INFO] Step 130950/210000; acc:  50.51; ppl:  8.37; xent: 2.12; lr: 0.00000553;   0/529 tok/s; 216779 sec
[2020-04-01 07:53:46,808 INFO] Step 131000/210000; acc:  50.76; ppl:  8.50; xent: 2.14; lr: 0.00000553;   0/814 tok/s; 216862 sec
[2020-04-01 07:53:46,812 INFO] Saving checkpoint ../models/model_step_131000.pt
[2020-04-01 07:55:00,544 INFO] Loading train dataset from ../bert_data/cnndm.train.9026.bert.pt, number of examples: 1986
[2020-04-01 07:55:12,636 INFO] Step 131050/210000; acc:  57.45; ppl:  5.60; xent: 1.72; lr: 0.00000552;   0/501 tok/s; 216947 sec
[2020-04-01 07:56:35,450 INFO] Step 131100/210000; acc:  49.92; ppl:  9.08; xent: 2.21; lr: 0.00000552;   0/1107 tok/s; 217030 sec
[2020-04-01 07:57:39,128 INFO] Loading train dataset from ../bert_data/cnndm.train.9027.bert.pt, number of examples: 1989
[2020-04-01 07:57:58,765 INFO] Step 131150/210000; acc:  56.89; ppl:  6.25; xent: 1.83; lr: 0.00000552;   0/625 tok/s; 217114 sec
[2020-04-01 07:59:20,904 INFO] Step 131200/210000; acc:  44.55; ppl: 12.14; xent: 2.50; lr: 0.00000552;   0/1216 tok/s; 217196 sec
[2020-04-01 08:00:18,213 INFO] Loading train dataset from ../bert_data/cnndm.train.9028.bert.pt, number of examples: 1986
[2020-04-01 08:00:44,558 INFO] Step 131250/210000; acc:  56.97; ppl:  5.71; xent: 1.74; lr: 0.00000552;   0/526 tok/s; 217279 sec
[2020-04-01 08:02:07,415 INFO] Step 131300/210000; acc:  45.19; ppl: 12.81; xent: 2.55; lr: 0.00000552;   0/1311 tok/s; 217362 sec
[2020-04-01 08:02:55,516 INFO] Loading train dataset from ../bert_data/cnndm.train.9029.bert.pt, number of examples: 1981
[2020-04-01 08:03:30,402 INFO] Step 131350/210000; acc:  56.21; ppl:  6.20; xent: 1.82; lr: 0.00000552;   0/502 tok/s; 217445 sec
[2020-04-01 08:04:53,229 INFO] Step 131400/210000; acc:  44.04; ppl: 14.56; xent: 2.68; lr: 0.00000552;   0/1456 tok/s; 217528 sec
[2020-04-01 08:05:32,396 INFO] Loading train dataset from ../bert_data/cnndm.train.9030.bert.pt, number of examples: 1984
[2020-04-01 08:06:16,554 INFO] Step 131450/210000; acc:  49.83; ppl:  8.72; xent: 2.17; lr: 0.00000552;   0/778 tok/s; 217611 sec
[2020-04-01 08:07:39,389 INFO] Step 131500/210000; acc:  48.65; ppl: 10.55; xent: 2.36; lr: 0.00000552;   0/1292 tok/s; 217694 sec
[2020-04-01 08:07:39,392 INFO] Saving checkpoint ../models/model_step_131500.pt
[2020-04-01 08:08:12,607 INFO] Loading train dataset from ../bert_data/cnndm.train.9031.bert.pt, number of examples: 1975
[2020-04-01 08:09:05,260 INFO] Step 131550/210000; acc:  47.53; ppl: 11.14; xent: 2.41; lr: 0.00000551;   0/926 tok/s; 217780 sec
[2020-04-01 08:10:29,676 INFO] Step 131600/210000; acc:  58.51; ppl:  5.16; xent: 1.64; lr: 0.00000551;   0/471 tok/s; 217864 sec
[2020-04-01 08:10:50,014 INFO] Loading train dataset from ../bert_data/cnndm.train.9032.bert.pt, number of examples: 1986
[2020-04-01 08:11:52,550 INFO] Step 131650/210000; acc:  42.95; ppl: 11.68; xent: 2.46; lr: 0.00000551;   0/1219 tok/s; 217947 sec
[2020-04-01 08:13:16,105 INFO] Step 131700/210000; acc:  56.05; ppl:  7.17; xent: 1.97; lr: 0.00000551;   0/649 tok/s; 218031 sec
[2020-04-01 08:13:26,506 INFO] Loading train dataset from ../bert_data/cnndm.train.9033.bert.pt, number of examples: 1986
[2020-04-01 08:14:38,984 INFO] Step 131750/210000; acc:  45.82; ppl: 12.16; xent: 2.50; lr: 0.00000551;   0/1191 tok/s; 218114 sec
[2020-04-01 08:16:01,252 INFO] Step 131800/210000; acc:  47.77; ppl:  9.95; xent: 2.30; lr: 0.00000551;   0/848 tok/s; 218196 sec
[2020-04-01 08:16:03,469 INFO] Loading train dataset from ../bert_data/cnndm.train.9034.bert.pt, number of examples: 1989
[2020-04-01 08:17:24,414 INFO] Step 131850/210000; acc:  46.37; ppl: 12.07; xent: 2.49; lr: 0.00000551;   0/845 tok/s; 218279 sec
[2020-04-01 08:18:40,695 INFO] Loading train dataset from ../bert_data/cnndm.train.9035.bert.pt, number of examples: 1988
[2020-04-01 08:18:47,246 INFO] Step 131900/210000; acc:  49.97; ppl:  8.72; xent: 2.17; lr: 0.00000551;   0/1003 tok/s; 218362 sec
[2020-04-01 08:20:08,404 INFO] Step 131950/210000; acc:  50.13; ppl:  8.50; xent: 2.14; lr: 0.00000551;   0/670 tok/s; 218443 sec
[2020-04-01 08:21:16,163 INFO] Loading train dataset from ../bert_data/cnndm.train.9036.bert.pt, number of examples: 1984
[2020-04-01 08:21:31,427 INFO] Step 132000/210000; acc:  49.23; ppl:  9.37; xent: 2.24; lr: 0.00000550;   0/1128 tok/s; 218526 sec
[2020-04-01 08:21:31,431 INFO] Saving checkpoint ../models/model_step_132000.pt
[2020-04-01 08:22:57,168 INFO] Step 132050/210000; acc:  56.11; ppl:  6.81; xent: 1.92; lr: 0.00000550;   0/502 tok/s; 218612 sec
[2020-04-01 08:23:56,005 INFO] Loading train dataset from ../bert_data/cnndm.train.9037.bert.pt, number of examples: 1983
[2020-04-01 08:24:20,925 INFO] Step 132100/210000; acc:  42.31; ppl: 16.25; xent: 2.79; lr: 0.00000550;   0/764 tok/s; 218696 sec
[2020-04-01 08:25:43,634 INFO] Step 132150/210000; acc:  51.33; ppl:  8.23; xent: 2.11; lr: 0.00000550;   0/820 tok/s; 218778 sec
[2020-04-01 08:26:31,509 INFO] Loading train dataset from ../bert_data/cnndm.train.9038.bert.pt, number of examples: 1982
[2020-04-01 08:27:06,438 INFO] Step 132200/210000; acc:  63.94; ppl:  4.61; xent: 1.53; lr: 0.00000550;   0/379 tok/s; 218861 sec
[2020-04-01 08:28:29,154 INFO] Step 132250/210000; acc:  49.59; ppl:  9.35; xent: 2.24; lr: 0.00000550;   0/966 tok/s; 218944 sec
[2020-04-01 08:29:09,718 INFO] Loading train dataset from ../bert_data/cnndm.train.9039.bert.pt, number of examples: 1983
[2020-04-01 08:29:52,593 INFO] Step 132300/210000; acc:  58.30; ppl:  5.15; xent: 1.64; lr: 0.00000550;   0/479 tok/s; 219027 sec
[2020-04-01 08:31:14,078 INFO] Step 132350/210000; acc:  43.72; ppl: 12.44; xent: 2.52; lr: 0.00000550;   0/1180 tok/s; 219109 sec
[2020-04-01 08:31:46,237 INFO] Loading train dataset from ../bert_data/cnndm.train.9040.bert.pt, number of examples: 1982
[2020-04-01 08:32:38,188 INFO] Step 132400/210000; acc:  52.52; ppl:  7.57; xent: 2.02; lr: 0.00000550;   0/695 tok/s; 219193 sec
[2020-04-01 08:34:00,874 INFO] Step 132450/210000; acc:  44.79; ppl: 12.95; xent: 2.56; lr: 0.00000550;   0/1083 tok/s; 219276 sec
[2020-04-01 08:34:23,041 INFO] Loading train dataset from ../bert_data/cnndm.train.9041.bert.pt, number of examples: 1986
[2020-04-01 08:35:24,437 INFO] Step 132500/210000; acc:  54.54; ppl:  6.87; xent: 1.93; lr: 0.00000549;   0/846 tok/s; 219359 sec
[2020-04-01 08:35:24,440 INFO] Saving checkpoint ../models/model_step_132500.pt
[2020-04-01 08:36:48,796 INFO] Step 132550/210000; acc:  41.38; ppl: 14.86; xent: 2.70; lr: 0.00000549;   0/1038 tok/s; 219444 sec
[2020-04-01 08:37:02,445 INFO] Loading train dataset from ../bert_data/cnndm.train.9042.bert.pt, number of examples: 1984
[2020-04-01 08:38:11,266 INFO] Step 132600/210000; acc:  51.11; ppl:  8.57; xent: 2.15; lr: 0.00000549;   0/778 tok/s; 219526 sec
[2020-04-01 08:39:33,454 INFO] Step 132650/210000; acc:  45.86; ppl: 11.11; xent: 2.41; lr: 0.00000549;   0/1169 tok/s; 219608 sec
[2020-04-01 08:39:40,713 INFO] Loading train dataset from ../bert_data/cnndm.train.9043.bert.pt, number of examples: 1984
[2020-04-01 08:40:56,957 INFO] Step 132700/210000; acc:  48.31; ppl: 10.12; xent: 2.31; lr: 0.00000549;   0/1062 tok/s; 219692 sec
[2020-04-01 08:42:17,836 INFO] Loading train dataset from ../bert_data/cnndm.train.9044.bert.pt, number of examples: 1988
[2020-04-01 08:42:21,253 INFO] Step 132750/210000; acc:  62.30; ppl:  5.06; xent: 1.62; lr: 0.00000549;   0/486 tok/s; 219776 sec
[2020-04-01 08:43:43,931 INFO] Step 132800/210000; acc:  46.00; ppl: 11.17; xent: 2.41; lr: 0.00000549;   0/1406 tok/s; 219859 sec
[2020-04-01 08:44:55,727 INFO] Loading train dataset from ../bert_data/cnndm.train.9045.bert.pt, number of examples: 1983
[2020-04-01 08:45:07,079 INFO] Step 132850/210000; acc:  58.04; ppl:  5.61; xent: 1.72; lr: 0.00000549;   0/588 tok/s; 219942 sec
[2020-04-01 08:46:29,998 INFO] Step 132900/210000; acc:  48.56; ppl: 10.75; xent: 2.38; lr: 0.00000549;   0/1257 tok/s; 220025 sec
[2020-04-01 08:47:33,617 INFO] Loading train dataset from ../bert_data/cnndm.train.9046.bert.pt, number of examples: 1983
[2020-04-01 08:47:53,540 INFO] Step 132950/210000; acc:  53.35; ppl:  7.53; xent: 2.02; lr: 0.00000549;   0/661 tok/s; 220108 sec
[2020-04-01 08:49:17,199 INFO] Step 133000/210000; acc:  46.15; ppl: 10.78; xent: 2.38; lr: 0.00000548;   0/975 tok/s; 220192 sec
[2020-04-01 08:49:17,202 INFO] Saving checkpoint ../models/model_step_133000.pt
[2020-04-01 08:50:13,346 INFO] Loading train dataset from ../bert_data/cnndm.train.9047.bert.pt, number of examples: 1983
[2020-04-01 08:50:43,050 INFO] Step 133050/210000; acc:  50.43; ppl:  9.31; xent: 2.23; lr: 0.00000548;   0/971 tok/s; 220278 sec
[2020-04-01 08:52:06,522 INFO] Step 133100/210000; acc:  46.36; ppl:  9.94; xent: 2.30; lr: 0.00000548;   0/750 tok/s; 220361 sec
[2020-04-01 08:52:51,476 INFO] Loading train dataset from ../bert_data/cnndm.train.9048.bert.pt, number of examples: 1984
[2020-04-01 08:53:29,368 INFO] Step 133150/210000; acc:  47.61; ppl:  9.70; xent: 2.27; lr: 0.00000548;   0/1013 tok/s; 220444 sec
[2020-04-01 08:54:52,004 INFO] Step 133200/210000; acc:  57.80; ppl:  5.87; xent: 1.77; lr: 0.00000548;   0/448 tok/s; 220527 sec
[2020-04-01 08:55:28,864 INFO] Loading train dataset from ../bert_data/cnndm.train.9049.bert.pt, number of examples: 1978
[2020-04-01 08:56:15,991 INFO] Step 133250/210000; acc:  45.47; ppl: 11.66; xent: 2.46; lr: 0.00000548;   0/1301 tok/s; 220611 sec
[2020-04-01 08:57:38,325 INFO] Step 133300/210000; acc:  61.45; ppl:  4.94; xent: 1.60; lr: 0.00000548;   0/508 tok/s; 220693 sec
[2020-04-01 08:58:05,145 INFO] Loading train dataset from ../bert_data/cnndm.train.9050.bert.pt, number of examples: 1978
[2020-04-01 08:59:01,069 INFO] Step 133350/210000; acc:  43.52; ppl: 12.62; xent: 2.54; lr: 0.00000548;   0/1239 tok/s; 220776 sec
[2020-04-01 09:00:23,283 INFO] Step 133400/210000; acc:  53.53; ppl:  6.54; xent: 1.88; lr: 0.00000548;   0/518 tok/s; 220858 sec
[2020-04-01 09:00:41,995 INFO] Loading train dataset from ../bert_data/cnndm.train.9051.bert.pt, number of examples: 1981
[2020-04-01 09:01:46,581 INFO] Step 133450/210000; acc:  52.71; ppl:  7.73; xent: 2.04; lr: 0.00000547;   0/685 tok/s; 220941 sec
[2020-04-01 09:03:08,450 INFO] Step 133500/210000; acc:  50.15; ppl:  9.24; xent: 2.22; lr: 0.00000547;   0/810 tok/s; 221023 sec
[2020-04-01 09:03:08,453 INFO] Saving checkpoint ../models/model_step_133500.pt
[2020-04-01 09:03:19,236 INFO] Loading train dataset from ../bert_data/cnndm.train.9052.bert.pt, number of examples: 1981
[2020-04-01 09:04:33,659 INFO] Step 133550/210000; acc:  53.57; ppl:  7.28; xent: 1.98; lr: 0.00000547;   0/656 tok/s; 221108 sec
[2020-04-01 09:05:54,816 INFO] Loading train dataset from ../bert_data/cnndm.train.9053.bert.pt, number of examples: 1981
[2020-04-01 09:05:56,537 INFO] Step 133600/210000; acc:  48.91; ppl:  9.56; xent: 2.26; lr: 0.00000547;   0/751 tok/s; 221191 sec
[2020-04-01 09:07:18,028 INFO] Step 133650/210000; acc:  53.72; ppl:  6.74; xent: 1.91; lr: 0.00000547;   0/661 tok/s; 221273 sec
[2020-04-01 09:08:31,596 INFO] Loading train dataset from ../bert_data/cnndm.train.9054.bert.pt, number of examples: 1987
[2020-04-01 09:08:41,485 INFO] Step 133700/210000; acc:  57.90; ppl:  6.04; xent: 1.80; lr: 0.00000547;   0/667 tok/s; 221356 sec
[2020-04-01 09:10:04,458 INFO] Step 133750/210000; acc:  55.54; ppl:  6.13; xent: 1.81; lr: 0.00000547;   0/763 tok/s; 221439 sec
[2020-04-01 09:11:09,962 INFO] Loading train dataset from ../bert_data/cnndm.train.9055.bert.pt, number of examples: 1984
[2020-04-01 09:11:28,241 INFO] Step 133800/210000; acc:  55.14; ppl:  6.58; xent: 1.88; lr: 0.00000547;   0/621 tok/s; 221523 sec
[2020-04-01 09:12:51,251 INFO] Step 133850/210000; acc:  49.90; ppl:  8.53; xent: 2.14; lr: 0.00000547;   0/913 tok/s; 221606 sec
[2020-04-01 09:13:48,191 INFO] Loading train dataset from ../bert_data/cnndm.train.9056.bert.pt, number of examples: 1985
[2020-04-01 09:14:14,204 INFO] Step 133900/210000; acc:  61.68; ppl:  4.94; xent: 1.60; lr: 0.00000547;   0/534 tok/s; 221689 sec
[2020-04-01 09:15:36,724 INFO] Step 133950/210000; acc:  45.78; ppl: 12.05; xent: 2.49; lr: 0.00000546;   0/1295 tok/s; 221772 sec
[2020-04-01 09:16:23,419 INFO] Loading train dataset from ../bert_data/cnndm.train.9057.bert.pt, number of examples: 1986
[2020-04-01 09:17:00,044 INFO] Step 134000/210000; acc:  53.45; ppl:  7.38; xent: 2.00; lr: 0.00000546;   0/597 tok/s; 221855 sec
[2020-04-01 09:17:00,048 INFO] Saving checkpoint ../models/model_step_134000.pt
[2020-04-01 09:18:25,460 INFO] Step 134050/210000; acc:  44.78; ppl: 13.81; xent: 2.63; lr: 0.00000546;   0/1296 tok/s; 221940 sec
[2020-04-01 09:19:03,892 INFO] Loading train dataset from ../bert_data/cnndm.train.9058.bert.pt, number of examples: 1979
[2020-04-01 09:19:48,812 INFO] Step 134100/210000; acc:  56.26; ppl:  6.44; xent: 1.86; lr: 0.00000546;   0/792 tok/s; 222024 sec
[2020-04-01 09:21:11,978 INFO] Step 134150/210000; acc:  47.71; ppl: 11.32; xent: 2.43; lr: 0.00000546;   0/838 tok/s; 222107 sec
[2020-04-01 09:21:39,619 INFO] Loading train dataset from ../bert_data/cnndm.train.9059.bert.pt, number of examples: 1987
[2020-04-01 09:22:34,412 INFO] Step 134200/210000; acc:  49.58; ppl:  9.46; xent: 2.25; lr: 0.00000546;   0/1212 tok/s; 222189 sec
[2020-04-01 09:23:57,267 INFO] Step 134250/210000; acc:  62.72; ppl:  5.46; xent: 1.70; lr: 0.00000546;   0/452 tok/s; 222272 sec
[2020-04-01 09:24:17,760 INFO] Loading train dataset from ../bert_data/cnndm.train.9060.bert.pt, number of examples: 1992
[2020-04-01 09:25:19,998 INFO] Step 134300/210000; acc:  43.96; ppl: 12.78; xent: 2.55; lr: 0.00000546;   0/1389 tok/s; 222355 sec
[2020-04-01 09:26:43,645 INFO] Step 134350/210000; acc:  57.22; ppl:  5.79; xent: 1.76; lr: 0.00000546;   0/506 tok/s; 222438 sec
[2020-04-01 09:26:55,856 INFO] Loading train dataset from ../bert_data/cnndm.train.9061.bert.pt, number of examples: 1985
[2020-04-01 09:28:07,098 INFO] Step 134400/210000; acc:  43.34; ppl: 13.42; xent: 2.60; lr: 0.00000546;   0/1063 tok/s; 222522 sec
[2020-04-01 09:29:29,627 INFO] Step 134450/210000; acc:  58.51; ppl:  6.26; xent: 1.83; lr: 0.00000545;   0/460 tok/s; 222604 sec
[2020-04-01 09:29:35,178 INFO] Loading train dataset from ../bert_data/cnndm.train.9062.bert.pt, number of examples: 1982
[2020-04-01 09:30:53,019 INFO] Step 134500/210000; acc:  43.05; ppl: 13.49; xent: 2.60; lr: 0.00000545;   0/1147 tok/s; 222688 sec
[2020-04-01 09:30:53,023 INFO] Saving checkpoint ../models/model_step_134500.pt
[2020-04-01 09:32:14,447 INFO] Loading train dataset from ../bert_data/cnndm.train.9063.bert.pt, number of examples: 1988
[2020-04-01 09:32:17,674 INFO] Step 134550/210000; acc:  58.79; ppl:  5.33; xent: 1.67; lr: 0.00000545;   0/563 tok/s; 222772 sec
[2020-04-01 09:33:40,346 INFO] Step 134600/210000; acc:  41.50; ppl: 15.99; xent: 2.77; lr: 0.00000545;   0/1255 tok/s; 222855 sec
[2020-04-01 09:34:50,688 INFO] Loading train dataset from ../bert_data/cnndm.train.9064.bert.pt, number of examples: 1984
[2020-04-01 09:35:03,797 INFO] Step 134650/210000; acc:  48.78; ppl:  8.79; xent: 2.17; lr: 0.00000545;   0/807 tok/s; 222939 sec
[2020-04-01 09:36:25,964 INFO] Step 134700/210000; acc:  41.51; ppl: 14.61; xent: 2.68; lr: 0.00000545;   0/1340 tok/s; 223021 sec
[2020-04-01 09:37:26,770 INFO] Loading train dataset from ../bert_data/cnndm.train.9065.bert.pt, number of examples: 1986
[2020-04-01 09:37:48,529 INFO] Step 134750/210000; acc:  49.04; ppl:  9.31; xent: 2.23; lr: 0.00000545;   0/725 tok/s; 223103 sec
[2020-04-01 09:39:11,258 INFO] Step 134800/210000; acc:  52.48; ppl:  7.16; xent: 1.97; lr: 0.00000545;   0/998 tok/s; 223186 sec
[2020-04-01 09:40:03,916 INFO] Loading train dataset from ../bert_data/cnndm.train.9066.bert.pt, number of examples: 1988
[2020-04-01 09:40:33,802 INFO] Step 134850/210000; acc:  48.13; ppl:  9.23; xent: 2.22; lr: 0.00000545;   0/843 tok/s; 223269 sec
[2020-04-01 09:41:56,344 INFO] Step 134900/210000; acc:  49.92; ppl:  9.26; xent: 2.23; lr: 0.00000545;   0/1077 tok/s; 223351 sec
[2020-04-01 09:42:41,159 INFO] Loading train dataset from ../bert_data/cnndm.train.9067.bert.pt, number of examples: 1986
[2020-04-01 09:43:18,631 INFO] Step 134950/210000; acc:  53.58; ppl:  7.49; xent: 2.01; lr: 0.00000544;   0/920 tok/s; 223433 sec
[2020-04-01 09:44:41,209 INFO] Step 135000/210000; acc:  43.05; ppl: 12.01; xent: 2.49; lr: 0.00000544;   0/938 tok/s; 223516 sec
[2020-04-01 09:44:41,213 INFO] Saving checkpoint ../models/model_step_135000.pt
[2020-04-01 09:45:19,928 INFO] Loading train dataset from ../bert_data/cnndm.train.9068.bert.pt, number of examples: 1988
[2020-04-01 09:46:05,740 INFO] Step 135050/210000; acc:  45.69; ppl: 11.13; xent: 2.41; lr: 0.00000544;   0/1094 tok/s; 223601 sec
[2020-04-01 09:47:28,795 INFO] Step 135100/210000; acc:  58.60; ppl:  5.02; xent: 1.61; lr: 0.00000544;   0/402 tok/s; 223684 sec
[2020-04-01 09:47:57,271 INFO] Loading train dataset from ../bert_data/cnndm.train.9069.bert.pt, number of examples: 1982
[2020-04-01 09:48:51,931 INFO] Step 135150/210000; acc:  48.11; ppl: 10.62; xent: 2.36; lr: 0.00000544;   0/1288 tok/s; 223767 sec
[2020-04-01 09:50:15,176 INFO] Step 135200/210000; acc:  56.41; ppl:  5.41; xent: 1.69; lr: 0.00000544;   0/490 tok/s; 223850 sec
[2020-04-01 09:50:33,728 INFO] Loading train dataset from ../bert_data/cnndm.train.9070.bert.pt, number of examples: 1988
[2020-04-01 09:51:38,566 INFO] Step 135250/210000; acc:  45.71; ppl: 10.79; xent: 2.38; lr: 0.00000544;   0/1026 tok/s; 223933 sec
[2020-04-01 09:53:01,226 INFO] Step 135300/210000; acc:  53.95; ppl:  7.23; xent: 1.98; lr: 0.00000544;   0/684 tok/s; 224016 sec
[2020-04-01 09:53:11,591 INFO] Loading train dataset from ../bert_data/cnndm.train.9071.bert.pt, number of examples: 1988
[2020-04-01 09:54:24,386 INFO] Step 135350/210000; acc:  48.27; ppl:  9.21; xent: 2.22; lr: 0.00000544;   0/1133 tok/s; 224099 sec
[2020-04-01 09:55:46,707 INFO] Step 135400/210000; acc:  51.67; ppl:  8.06; xent: 2.09; lr: 0.00000544;   0/619 tok/s; 224182 sec
[2020-04-01 09:55:50,755 INFO] Loading train dataset from ../bert_data/cnndm.train.9072.bert.pt, number of examples: 1992
[2020-04-01 09:57:10,274 INFO] Step 135450/210000; acc:  44.97; ppl: 13.44; xent: 2.60; lr: 0.00000543;   0/855 tok/s; 224265 sec
[2020-04-01 09:58:27,218 INFO] Loading train dataset from ../bert_data/cnndm.train.9073.bert.pt, number of examples: 1987
[2020-04-01 09:58:33,654 INFO] Step 135500/210000; acc:  53.75; ppl:  6.91; xent: 1.93; lr: 0.00000543;   0/917 tok/s; 224348 sec
[2020-04-01 09:58:33,656 INFO] Saving checkpoint ../models/model_step_135500.pt
[2020-04-01 09:59:59,733 INFO] Step 135550/210000; acc:  51.55; ppl:  8.61; xent: 2.15; lr: 0.00000543;   0/677 tok/s; 224435 sec
[2020-04-01 10:01:09,636 INFO] Loading train dataset from ../bert_data/cnndm.train.9074.bert.pt, number of examples: 1991
[2020-04-01 10:01:22,808 INFO] Step 135600/210000; acc:  49.93; ppl:  9.33; xent: 2.23; lr: 0.00000543;   0/889 tok/s; 224518 sec
[2020-04-01 10:02:45,933 INFO] Step 135650/210000; acc:  62.36; ppl:  4.38; xent: 1.48; lr: 0.00000543;   0/391 tok/s; 224601 sec
[2020-04-01 10:03:45,857 INFO] Loading train dataset from ../bert_data/cnndm.train.9075.bert.pt, number of examples: 1983
[2020-04-01 10:04:09,117 INFO] Step 135700/210000; acc:  46.47; ppl: 11.74; xent: 2.46; lr: 0.00000543;   0/1118 tok/s; 224684 sec
[2020-04-01 10:05:32,305 INFO] Step 135750/210000; acc:  62.50; ppl:  4.73; xent: 1.55; lr: 0.00000543;   0/494 tok/s; 224767 sec
[2020-04-01 10:06:23,105 INFO] Loading train dataset from ../bert_data/cnndm.train.9076.bert.pt, number of examples: 1986
[2020-04-01 10:06:54,478 INFO] Step 135800/210000; acc:  47.30; ppl: 10.48; xent: 2.35; lr: 0.00000543;   0/1156 tok/s; 224849 sec
[2020-04-01 10:08:16,250 INFO] Step 135850/210000; acc:  53.94; ppl:  7.22; xent: 1.98; lr: 0.00000543;   0/566 tok/s; 224931 sec
[2020-04-01 10:08:59,940 INFO] Loading train dataset from ../bert_data/cnndm.train.9077.bert.pt, number of examples: 1990
[2020-04-01 10:09:39,656 INFO] Step 135900/210000; acc:  43.34; ppl: 13.87; xent: 2.63; lr: 0.00000543;   0/1298 tok/s; 225014 sec
[2020-04-01 10:11:03,030 INFO] Step 135950/210000; acc:  56.04; ppl:  6.85; xent: 1.92; lr: 0.00000542;   0/642 tok/s; 225098 sec
[2020-04-01 10:11:38,140 INFO] Loading train dataset from ../bert_data/cnndm.train.9078.bert.pt, number of examples: 1992
[2020-04-01 10:12:25,868 INFO] Step 136000/210000; acc:  48.90; ppl: 10.27; xent: 2.33; lr: 0.00000542;   0/1185 tok/s; 225181 sec
[2020-04-01 10:12:25,871 INFO] Saving checkpoint ../models/model_step_136000.pt
[2020-04-01 10:13:50,949 INFO] Step 136050/210000; acc:  61.87; ppl:  5.23; xent: 1.65; lr: 0.00000542;   0/497 tok/s; 225266 sec
[2020-04-01 10:14:19,837 INFO] Loading train dataset from ../bert_data/cnndm.train.9079.bert.pt, number of examples: 1984
[2020-04-01 10:15:14,116 INFO] Step 136100/210000; acc:  44.68; ppl: 12.21; xent: 2.50; lr: 0.00000542;   0/1396 tok/s; 225349 sec
[2020-04-01 10:16:36,548 INFO] Step 136150/210000; acc:  56.44; ppl:  5.97; xent: 1.79; lr: 0.00000542;   0/644 tok/s; 225431 sec
[2020-04-01 10:16:55,601 INFO] Loading train dataset from ../bert_data/cnndm.train.9080.bert.pt, number of examples: 1988
[2020-04-01 10:18:00,309 INFO] Step 136200/210000; acc:  48.64; ppl: 10.14; xent: 2.32; lr: 0.00000542;   0/990 tok/s; 225515 sec
[2020-04-01 10:19:22,536 INFO] Step 136250/210000; acc:  50.16; ppl:  9.23; xent: 2.22; lr: 0.00000542;   0/730 tok/s; 225597 sec
[2020-04-01 10:19:33,310 INFO] Loading train dataset from ../bert_data/cnndm.train.9081.bert.pt, number of examples: 1991
[2020-04-01 10:20:46,004 INFO] Step 136300/210000; acc:  51.80; ppl:  7.56; xent: 2.02; lr: 0.00000542;   0/550 tok/s; 225681 sec
[2020-04-01 10:22:09,077 INFO] Step 136350/210000; acc:  45.91; ppl: 12.55; xent: 2.53; lr: 0.00000542;   0/1146 tok/s; 225764 sec
[2020-04-01 10:22:11,418 INFO] Loading train dataset from ../bert_data/cnndm.train.9082.bert.pt, number of examples: 1985
[2020-04-01 10:23:31,744 INFO] Step 136400/210000; acc:  55.47; ppl:  7.06; xent: 1.95; lr: 0.00000542;   0/565 tok/s; 225847 sec
[2020-04-01 10:24:47,713 INFO] Loading train dataset from ../bert_data/cnndm.train.9083.bert.pt, number of examples: 1986
[2020-04-01 10:24:54,102 INFO] Step 136450/210000; acc:  49.24; ppl:  9.01; xent: 2.20; lr: 0.00000541;   0/1106 tok/s; 225929 sec
[2020-04-01 10:26:16,974 INFO] Step 136500/210000; acc:  61.13; ppl:  4.98; xent: 1.61; lr: 0.00000541;   0/400 tok/s; 226012 sec
[2020-04-01 10:26:16,978 INFO] Saving checkpoint ../models/model_step_136500.pt
[2020-04-01 10:27:27,664 INFO] Loading train dataset from ../bert_data/cnndm.train.9084.bert.pt, number of examples: 1982
[2020-04-01 10:27:42,470 INFO] Step 136550/210000; acc:  47.89; ppl:  9.81; xent: 2.28; lr: 0.00000541;   0/1218 tok/s; 226097 sec
[2020-04-01 10:29:05,329 INFO] Step 136600/210000; acc:  59.24; ppl:  5.34; xent: 1.68; lr: 0.00000541;   0/488 tok/s; 226180 sec
[2020-04-01 10:30:05,484 INFO] Loading train dataset from ../bert_data/cnndm.train.9085.bert.pt, number of examples: 1985
[2020-04-01 10:30:28,358 INFO] Step 136650/210000; acc:  52.14; ppl:  8.42; xent: 2.13; lr: 0.00000541;   0/1202 tok/s; 226263 sec
[2020-04-01 10:31:50,847 INFO] Step 136700/210000; acc:  57.41; ppl:  5.84; xent: 1.76; lr: 0.00000541;   0/512 tok/s; 226346 sec
[2020-04-01 10:32:42,957 INFO] Loading train dataset from ../bert_data/cnndm.train.9086.bert.pt, number of examples: 1983
[2020-04-01 10:33:14,167 INFO] Step 136750/210000; acc:  48.84; ppl:  9.55; xent: 2.26; lr: 0.00000541;   0/1292 tok/s; 226429 sec
[2020-04-01 10:34:37,292 INFO] Step 136800/210000; acc:  56.21; ppl:  6.04; xent: 1.80; lr: 0.00000541;   0/646 tok/s; 226512 sec
[2020-04-01 10:35:19,039 INFO] Loading train dataset from ../bert_data/cnndm.train.9087.bert.pt, number of examples: 1986
[2020-04-01 10:36:00,626 INFO] Step 136850/210000; acc:  49.67; ppl:  8.75; xent: 2.17; lr: 0.00000541;   0/813 tok/s; 226595 sec
[2020-04-01 10:37:22,996 INFO] Step 136900/210000; acc:  51.91; ppl:  7.71; xent: 2.04; lr: 0.00000541;   0/696 tok/s; 226678 sec
[2020-04-01 10:37:58,033 INFO] Loading train dataset from ../bert_data/cnndm.train.9088.bert.pt, number of examples: 1986
[2020-04-01 10:38:45,939 INFO] Step 136950/210000; acc:  51.85; ppl:  8.29; xent: 2.12; lr: 0.00000540;   0/749 tok/s; 226761 sec
[2020-04-01 10:40:08,121 INFO] Step 137000/210000; acc:  48.38; ppl:  9.09; xent: 2.21; lr: 0.00000540;   0/866 tok/s; 226843 sec
[2020-04-01 10:40:08,124 INFO] Saving checkpoint ../models/model_step_137000.pt
[2020-04-01 10:40:35,646 INFO] Loading train dataset from ../bert_data/cnndm.train.9089.bert.pt, number of examples: 1986
[2020-04-01 10:41:33,475 INFO] Step 137050/210000; acc:  45.71; ppl: 11.35; xent: 2.43; lr: 0.00000540;   0/813 tok/s; 226928 sec
[2020-04-01 10:42:55,582 INFO] Step 137100/210000; acc:  48.66; ppl:  9.45; xent: 2.25; lr: 0.00000540;   0/890 tok/s; 227010 sec
[2020-04-01 10:43:12,895 INFO] Loading train dataset from ../bert_data/cnndm.train.9090.bert.pt, number of examples: 20
[2020-04-01 10:43:14,997 INFO] Loading train dataset from ../bert_data/cnndm.train.9100.bert.pt, number of examples: 1988
[2020-04-01 10:44:11,355 INFO] Step 137150/210000; acc:  65.45; ppl:  4.14; xent: 1.42; lr: 0.00000540;   0/551 tok/s; 227086 sec
[2020-04-01 10:45:23,719 INFO] Step 137200/210000; acc:  54.10; ppl:  6.53; xent: 1.88; lr: 0.00000540;   0/643 tok/s; 227159 sec
[2020-04-01 10:45:32,843 INFO] Loading train dataset from ../bert_data/cnndm.train.9101.bert.pt, number of examples: 1983
[2020-04-01 10:46:36,740 INFO] Step 137250/210000; acc:  59.90; ppl:  5.50; xent: 1.70; lr: 0.00000540;   0/693 tok/s; 227232 sec
[2020-04-01 10:47:49,804 INFO] Step 137300/210000; acc:  59.75; ppl:  5.46; xent: 1.70; lr: 0.00000540;   0/700 tok/s; 227305 sec
[2020-04-01 10:47:51,740 INFO] Loading train dataset from ../bert_data/cnndm.train.9102.bert.pt, number of examples: 1992
[2020-04-01 10:49:03,795 INFO] Step 137350/210000; acc:  64.48; ppl:  4.44; xent: 1.49; lr: 0.00000540;   0/420 tok/s; 227379 sec
[2020-04-01 10:50:09,842 INFO] Loading train dataset from ../bert_data/cnndm.train.9103.bert.pt, number of examples: 1978
[2020-04-01 10:50:16,812 INFO] Step 137400/210000; acc:  64.92; ppl:  4.60; xent: 1.53; lr: 0.00000540;   0/514 tok/s; 227452 sec
[2020-04-01 10:51:30,447 INFO] Step 137450/210000; acc:  62.71; ppl:  4.85; xent: 1.58; lr: 0.00000539;   0/562 tok/s; 227525 sec
[2020-04-01 10:52:28,529 INFO] Loading train dataset from ../bert_data/cnndm.train.9104.bert.pt, number of examples: 1986
[2020-04-01 10:52:45,134 INFO] Step 137500/210000; acc:  72.54; ppl:  3.15; xent: 1.15; lr: 0.00000539;   0/423 tok/s; 227600 sec
[2020-04-01 10:52:45,137 INFO] Saving checkpoint ../models/model_step_137500.pt
[2020-04-01 10:54:00,519 INFO] Step 137550/210000; acc:  57.62; ppl:  5.87; xent: 1.77; lr: 0.00000539;   0/602 tok/s; 227675 sec
[2020-04-01 10:54:50,348 INFO] Loading train dataset from ../bert_data/cnndm.train.9105.bert.pt, number of examples: 1988
[2020-04-01 10:55:13,786 INFO] Step 137600/210000; acc:  64.41; ppl:  4.16; xent: 1.43; lr: 0.00000539;   0/608 tok/s; 227749 sec
[2020-04-01 10:56:25,817 INFO] Step 137650/210000; acc:  57.73; ppl:  6.41; xent: 1.86; lr: 0.00000539;   0/651 tok/s; 227821 sec
[2020-04-01 10:57:08,521 INFO] Loading train dataset from ../bert_data/cnndm.train.9106.bert.pt, number of examples: 1978
[2020-04-01 10:57:39,628 INFO] Step 137700/210000; acc:  68.72; ppl:  3.78; xent: 1.33; lr: 0.00000539;   0/438 tok/s; 227894 sec
[2020-04-01 10:58:53,759 INFO] Step 137750/210000; acc:  59.71; ppl:  5.17; xent: 1.64; lr: 0.00000539;   0/500 tok/s; 227969 sec
[2020-04-01 10:59:27,961 INFO] Loading train dataset from ../bert_data/cnndm.train.9107.bert.pt, number of examples: 1985
[2020-04-01 11:00:08,016 INFO] Step 137800/210000; acc:  61.31; ppl:  4.56; xent: 1.52; lr: 0.00000539;   0/498 tok/s; 228043 sec
[2020-04-01 11:01:19,550 INFO] Step 137850/210000; acc:  70.00; ppl:  3.59; xent: 1.28; lr: 0.00000539;   0/541 tok/s; 228114 sec
[2020-04-01 11:01:46,133 INFO] Loading train dataset from ../bert_data/cnndm.train.9108.bert.pt, number of examples: 1989
[2020-04-01 11:02:32,710 INFO] Step 137900/210000; acc:  75.53; ppl:  3.06; xent: 1.12; lr: 0.00000539;   0/486 tok/s; 228188 sec
[2020-04-01 11:03:45,114 INFO] Step 137950/210000; acc:  59.62; ppl:  5.05; xent: 1.62; lr: 0.00000538;   0/665 tok/s; 228260 sec
[2020-04-01 11:04:04,507 INFO] Loading train dataset from ../bert_data/cnndm.train.9109.bert.pt, number of examples: 1987
[2020-04-01 11:04:58,420 INFO] Step 138000/210000; acc:  61.17; ppl:  4.88; xent: 1.58; lr: 0.00000538;   0/585 tok/s; 228333 sec
[2020-04-01 11:04:58,424 INFO] Saving checkpoint ../models/model_step_138000.pt
[2020-04-01 11:06:14,050 INFO] Step 138050/210000; acc:  65.17; ppl:  4.53; xent: 1.51; lr: 0.00000538;   0/640 tok/s; 228409 sec
[2020-04-01 11:06:26,240 INFO] Loading train dataset from ../bert_data/cnndm.train.9110.bert.pt, number of examples: 1985
[2020-04-01 11:07:27,158 INFO] Step 138100/210000; acc:  56.79; ppl:  6.45; xent: 1.86; lr: 0.00000538;   0/685 tok/s; 228482 sec
[2020-04-01 11:08:40,859 INFO] Step 138150/210000; acc:  60.41; ppl:  5.16; xent: 1.64; lr: 0.00000538;   0/549 tok/s; 228556 sec
[2020-04-01 11:08:44,124 INFO] Loading train dataset from ../bert_data/cnndm.train.9111.bert.pt, number of examples: 1985
[2020-04-01 11:09:54,078 INFO] Step 138200/210000; acc:  70.24; ppl:  3.62; xent: 1.29; lr: 0.00000538;   0/501 tok/s; 228629 sec
[2020-04-01 11:11:03,589 INFO] Loading train dataset from ../bert_data/cnndm.train.9112.bert.pt, number of examples: 1983
[2020-04-01 11:11:08,237 INFO] Step 138250/210000; acc:  67.68; ppl:  3.85; xent: 1.35; lr: 0.00000538;   0/438 tok/s; 228703 sec
[2020-04-01 11:12:21,132 INFO] Step 138300/210000; acc:  58.21; ppl:  5.56; xent: 1.72; lr: 0.00000538;   0/700 tok/s; 228776 sec
[2020-04-01 11:13:22,831 INFO] Loading train dataset from ../bert_data/cnndm.train.9113.bert.pt, number of examples: 1979
[2020-04-01 11:13:34,356 INFO] Step 138350/210000; acc:  63.25; ppl:  5.03; xent: 1.61; lr: 0.00000538;   0/683 tok/s; 228849 sec
[2020-04-01 11:14:48,187 INFO] Step 138400/210000; acc:  61.68; ppl:  4.66; xent: 1.54; lr: 0.00000538;   0/506 tok/s; 228923 sec
[2020-04-01 11:15:41,841 INFO] Loading train dataset from ../bert_data/cnndm.train.9114.bert.pt, number of examples: 1989
[2020-04-01 11:16:02,496 INFO] Step 138450/210000; acc:  71.16; ppl:  3.18; xent: 1.16; lr: 0.00000538;   0/530 tok/s; 228997 sec
[2020-04-01 11:17:16,297 INFO] Step 138500/210000; acc:  75.52; ppl:  2.78; xent: 1.02; lr: 0.00000537;   0/450 tok/s; 229071 sec
[2020-04-01 11:17:16,300 INFO] Saving checkpoint ../models/model_step_138500.pt
[2020-04-01 11:18:03,620 INFO] Loading train dataset from ../bert_data/cnndm.train.9115.bert.pt, number of examples: 1989
[2020-04-01 11:18:31,663 INFO] Step 138550/210000; acc:  59.72; ppl:  5.05; xent: 1.62; lr: 0.00000537;   0/566 tok/s; 229146 sec
[2020-04-01 11:19:44,693 INFO] Step 138600/210000; acc:  68.08; ppl:  4.05; xent: 1.40; lr: 0.00000537;   0/591 tok/s; 229220 sec
[2020-04-01 11:20:22,649 INFO] Loading train dataset from ../bert_data/cnndm.train.9116.bert.pt, number of examples: 1985
[2020-04-01 11:20:58,207 INFO] Step 138650/210000; acc:  62.54; ppl:  5.05; xent: 1.62; lr: 0.00000537;   0/596 tok/s; 229293 sec
[2020-04-01 11:22:11,932 INFO] Step 138700/210000; acc:  62.07; ppl:  5.03; xent: 1.62; lr: 0.00000537;   0/539 tok/s; 229367 sec
[2020-04-01 11:22:41,653 INFO] Loading train dataset from ../bert_data/cnndm.train.9117.bert.pt, number of examples: 1979
[2020-04-01 11:23:25,585 INFO] Step 138750/210000; acc:  65.97; ppl:  4.15; xent: 1.42; lr: 0.00000537;   0/536 tok/s; 229440 sec
[2020-04-01 11:24:37,736 INFO] Step 138800/210000; acc:  67.98; ppl:  3.39; xent: 1.22; lr: 0.00000537;   0/413 tok/s; 229513 sec
[2020-04-01 11:24:59,721 INFO] Loading train dataset from ../bert_data/cnndm.train.9118.bert.pt, number of examples: 1980
[2020-04-01 11:25:51,212 INFO] Step 138850/210000; acc:  61.47; ppl:  5.23; xent: 1.66; lr: 0.00000537;   0/604 tok/s; 229586 sec
[2020-04-01 11:27:04,839 INFO] Step 138900/210000; acc:  61.73; ppl:  5.06; xent: 1.62; lr: 0.00000537;   0/602 tok/s; 229660 sec
[2020-04-01 11:27:19,798 INFO] Loading train dataset from ../bert_data/cnndm.train.9119.bert.pt, number of examples: 1985
[2020-04-01 11:28:18,527 INFO] Step 138950/210000; acc:  64.33; ppl:  4.30; xent: 1.46; lr: 0.00000537;   0/498 tok/s; 229733 sec
[2020-04-01 11:29:31,860 INFO] Step 139000/210000; acc:  69.08; ppl:  3.70; xent: 1.31; lr: 0.00000536;   0/415 tok/s; 229807 sec
[2020-04-01 11:29:31,862 INFO] Saving checkpoint ../models/model_step_139000.pt
[2020-04-01 11:29:41,818 INFO] Loading train dataset from ../bert_data/cnndm.train.9120.bert.pt, number of examples: 1981
[2020-04-01 11:30:48,246 INFO] Step 139050/210000; acc:  62.69; ppl:  4.86; xent: 1.58; lr: 0.00000536;   0/711 tok/s; 229883 sec
[2020-04-01 11:31:59,479 INFO] Loading train dataset from ../bert_data/cnndm.train.9121.bert.pt, number of examples: 1985
[2020-04-01 11:32:01,220 INFO] Step 139100/210000; acc:  64.40; ppl:  4.57; xent: 1.52; lr: 0.00000536;   0/397 tok/s; 229956 sec
[2020-04-01 11:33:14,856 INFO] Step 139150/210000; acc:  65.93; ppl:  4.21; xent: 1.44; lr: 0.00000536;   0/576 tok/s; 230030 sec
[2020-04-01 11:34:20,003 INFO] Loading train dataset from ../bert_data/cnndm.train.9122.bert.pt, number of examples: 1986
[2020-04-01 11:34:28,756 INFO] Step 139200/210000; acc:  66.92; ppl:  4.26; xent: 1.45; lr: 0.00000536;   0/574 tok/s; 230104 sec
[2020-04-01 11:35:42,609 INFO] Step 139250/210000; acc:  70.59; ppl:  3.16; xent: 1.15; lr: 0.00000536;   0/384 tok/s; 230177 sec
[2020-04-01 11:36:37,972 INFO] Loading train dataset from ../bert_data/cnndm.train.9123.bert.pt, number of examples: 1983
[2020-04-01 11:36:55,661 INFO] Step 139300/210000; acc:  69.87; ppl:  3.42; xent: 1.23; lr: 0.00000536;   0/446 tok/s; 230250 sec
[2020-04-01 11:38:08,402 INFO] Step 139350/210000; acc:  63.64; ppl:  4.51; xent: 1.51; lr: 0.00000536;   0/619 tok/s; 230323 sec
[2020-04-01 11:38:57,455 INFO] Loading train dataset from ../bert_data/cnndm.train.9124.bert.pt, number of examples: 1983
[2020-04-01 11:39:22,506 INFO] Step 139400/210000; acc:  69.42; ppl:  3.46; xent: 1.24; lr: 0.00000536;   0/406 tok/s; 230397 sec
[2020-04-01 11:40:35,289 INFO] Step 139450/210000; acc:  70.96; ppl:  3.19; xent: 1.16; lr: 0.00000536;   0/569 tok/s; 230470 sec
[2020-04-01 11:41:16,062 INFO] Loading train dataset from ../bert_data/cnndm.train.9125.bert.pt, number of examples: 1991
[2020-04-01 11:41:48,049 INFO] Step 139500/210000; acc:  69.88; ppl:  3.62; xent: 1.29; lr: 0.00000535;   0/520 tok/s; 230543 sec
[2020-04-01 11:41:48,059 INFO] Saving checkpoint ../models/model_step_139500.pt
[2020-04-01 11:43:03,403 INFO] Step 139550/210000; acc:  70.69; ppl:  3.34; xent: 1.21; lr: 0.00000535;   0/491 tok/s; 230618 sec
[2020-04-01 11:43:35,549 INFO] Loading train dataset from ../bert_data/cnndm.train.9126.bert.pt, number of examples: 1985
[2020-04-01 11:44:15,616 INFO] Step 139600/210000; acc:  65.12; ppl:  4.60; xent: 1.53; lr: 0.00000535;   0/679 tok/s; 230690 sec
[2020-04-01 11:45:28,705 INFO] Step 139650/210000; acc:  71.67; ppl:  3.21; xent: 1.17; lr: 0.00000535;   0/520 tok/s; 230764 sec
[2020-04-01 11:45:54,290 INFO] Loading train dataset from ../bert_data/cnndm.train.9127.bert.pt, number of examples: 1980
[2020-04-01 11:46:41,825 INFO] Step 139700/210000; acc:  65.06; ppl:  4.97; xent: 1.60; lr: 0.00000535;   0/659 tok/s; 230837 sec
[2020-04-01 11:47:54,682 INFO] Step 139750/210000; acc:  65.20; ppl:  4.42; xent: 1.49; lr: 0.00000535;   0/608 tok/s; 230910 sec
[2020-04-01 11:48:11,455 INFO] Loading train dataset from ../bert_data/cnndm.train.9128.bert.pt, number of examples: 1981
[2020-04-01 11:49:08,141 INFO] Step 139800/210000; acc:  63.66; ppl:  4.26; xent: 1.45; lr: 0.00000535;   0/577 tok/s; 230983 sec
[2020-04-01 11:50:20,945 INFO] Step 139850/210000; acc:  72.51; ppl:  3.21; xent: 1.17; lr: 0.00000535;   0/429 tok/s; 231056 sec
[2020-04-01 11:50:29,874 INFO] Loading train dataset from ../bert_data/cnndm.train.9129.bert.pt, number of examples: 1987
[2020-04-01 11:51:33,927 INFO] Step 139900/210000; acc:  65.79; ppl:  4.19; xent: 1.43; lr: 0.00000535;   0/638 tok/s; 231129 sec
[2020-04-01 11:52:46,939 INFO] Step 139950/210000; acc:  62.87; ppl:  4.81; xent: 1.57; lr: 0.00000535;   0/585 tok/s; 231202 sec
[2020-04-01 11:52:49,054 INFO] Loading train dataset from ../bert_data/cnndm.train.9130.bert.pt, number of examples: 1986
[2020-04-01 11:54:00,815 INFO] Step 140000/210000; acc:  66.51; ppl:  3.94; xent: 1.37; lr: 0.00000535;   0/542 tok/s; 231276 sec
[2020-04-01 11:54:00,818 INFO] Saving checkpoint ../models/model_step_140000.pt
[2020-04-01 11:55:08,915 INFO] Loading train dataset from ../bert_data/cnndm.train.9131.bert.pt, number of examples: 1984
[2020-04-01 11:55:16,014 INFO] Step 140050/210000; acc:  71.14; ppl:  3.59; xent: 1.28; lr: 0.00000534;   0/494 tok/s; 231351 sec
[2020-04-01 11:56:30,255 INFO] Step 140100/210000; acc:  72.55; ppl:  3.07; xent: 1.12; lr: 0.00000534;   0/403 tok/s; 231425 sec
[2020-04-01 11:57:28,658 INFO] Loading train dataset from ../bert_data/cnndm.train.9132.bert.pt, number of examples: 1990
[2020-04-01 11:57:42,913 INFO] Step 140150/210000; acc:  60.34; ppl:  5.22; xent: 1.65; lr: 0.00000534;   0/674 tok/s; 231498 sec
[2020-04-01 11:58:55,086 INFO] Step 140200/210000; acc:  64.90; ppl:  3.86; xent: 1.35; lr: 0.00000534;   0/581 tok/s; 231570 sec
[2020-04-01 11:59:46,999 INFO] Loading train dataset from ../bert_data/cnndm.train.9133.bert.pt, number of examples: 1978
[2020-04-01 12:00:08,494 INFO] Step 140250/210000; acc:  64.85; ppl:  4.17; xent: 1.43; lr: 0.00000534;   0/598 tok/s; 231643 sec
[2020-04-01 12:01:21,345 INFO] Step 140300/210000; acc:  67.46; ppl:  3.63; xent: 1.29; lr: 0.00000534;   0/472 tok/s; 231716 sec
[2020-04-01 12:02:03,727 INFO] Loading train dataset from ../bert_data/cnndm.train.9134.bert.pt, number of examples: 156
[2020-04-01 12:02:15,919 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-04-01 12:02:37,770 INFO] Step 140350/210000; acc:  50.32; ppl: 12.93; xent: 2.56; lr: 0.00000534;   0/751 tok/s; 231793 sec
[2020-04-01 12:04:01,886 INFO] Step 140400/210000; acc:  44.37; ppl: 15.10; xent: 2.71; lr: 0.00000534;   0/613 tok/s; 231877 sec
[2020-04-01 12:05:03,470 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-04-01 12:05:26,898 INFO] Step 140450/210000; acc:  48.56; ppl: 12.42; xent: 2.52; lr: 0.00000534;   0/860 tok/s; 231962 sec
[2020-04-01 12:06:50,990 INFO] Step 140500/210000; acc:  45.01; ppl: 15.71; xent: 2.75; lr: 0.00000534;   0/905 tok/s; 232046 sec
[2020-04-01 12:06:51,014 INFO] Saving checkpoint ../models/model_step_140500.pt
[2020-04-01 12:07:52,573 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-04-01 12:08:17,975 INFO] Step 140550/210000; acc:  49.42; ppl: 12.25; xent: 2.51; lr: 0.00000533;   0/949 tok/s; 232133 sec
[2020-04-01 12:09:42,146 INFO] Step 140600/210000; acc:  49.62; ppl: 12.08; xent: 2.49; lr: 0.00000533;   0/935 tok/s; 232217 sec
[2020-04-01 12:10:41,661 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-04-01 12:11:06,845 INFO] Step 140650/210000; acc:  58.42; ppl:  7.75; xent: 2.05; lr: 0.00000533;   0/814 tok/s; 232302 sec
[2020-04-01 12:12:31,374 INFO] Step 140700/210000; acc:  51.01; ppl: 11.54; xent: 2.45; lr: 0.00000533;   0/828 tok/s; 232386 sec
[2020-04-01 12:13:29,340 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-04-01 12:13:56,501 INFO] Step 140750/210000; acc:  49.37; ppl: 11.18; xent: 2.41; lr: 0.00000533;   0/483 tok/s; 232471 sec
[2020-04-01 12:15:20,941 INFO] Step 140800/210000; acc:  48.24; ppl: 12.59; xent: 2.53; lr: 0.00000533;   0/737 tok/s; 232556 sec
[2020-04-01 12:16:17,300 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-04-01 12:16:46,019 INFO] Step 140850/210000; acc:  49.84; ppl: 10.30; xent: 2.33; lr: 0.00000533;   0/529 tok/s; 232641 sec
[2020-04-01 12:18:10,802 INFO] Step 140900/210000; acc:  48.53; ppl: 12.33; xent: 2.51; lr: 0.00000533;   0/630 tok/s; 232726 sec
[2020-04-01 12:19:05,419 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-04-01 12:19:35,929 INFO] Step 140950/210000; acc:  56.14; ppl:  7.35; xent: 2.00; lr: 0.00000533;   0/628 tok/s; 232811 sec
[2020-04-01 12:21:00,244 INFO] Step 141000/210000; acc:  52.21; ppl: 10.60; xent: 2.36; lr: 0.00000533;   0/680 tok/s; 232895 sec
[2020-04-01 12:21:00,248 INFO] Saving checkpoint ../models/model_step_141000.pt
[2020-04-01 12:21:56,936 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-04-01 12:22:27,258 INFO] Step 141050/210000; acc:  53.16; ppl:  9.58; xent: 2.26; lr: 0.00000533;   0/754 tok/s; 232982 sec
[2020-04-01 12:23:51,339 INFO] Step 141100/210000; acc:  50.17; ppl:  9.70; xent: 2.27; lr: 0.00000532;   0/544 tok/s; 233066 sec
[2020-04-01 12:24:44,136 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-04-01 12:25:16,225 INFO] Step 141150/210000; acc:  50.54; ppl: 11.84; xent: 2.47; lr: 0.00000532;   0/932 tok/s; 233151 sec
[2020-04-01 12:26:40,343 INFO] Step 141200/210000; acc:  53.23; ppl:  9.68; xent: 2.27; lr: 0.00000532;   0/715 tok/s; 233235 sec
[2020-04-01 12:27:33,870 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-04-01 12:28:05,969 INFO] Step 141250/210000; acc:  46.98; ppl: 12.77; xent: 2.55; lr: 0.00000532;   0/887 tok/s; 233321 sec
[2020-04-01 12:29:30,198 INFO] Step 141300/210000; acc:  50.78; ppl:  9.85; xent: 2.29; lr: 0.00000532;   0/772 tok/s; 233405 sec
[2020-04-01 12:30:21,634 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-04-01 12:30:55,228 INFO] Step 141350/210000; acc:  50.51; ppl: 11.14; xent: 2.41; lr: 0.00000532;   0/926 tok/s; 233490 sec
[2020-04-01 12:32:19,449 INFO] Step 141400/210000; acc:  56.99; ppl:  8.14; xent: 2.10; lr: 0.00000532;   0/936 tok/s; 233574 sec
[2020-04-01 12:33:08,744 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-04-01 12:33:44,195 INFO] Step 141450/210000; acc:  52.71; ppl:  9.78; xent: 2.28; lr: 0.00000532;   0/657 tok/s; 233659 sec
[2020-04-01 12:35:08,346 INFO] Step 141500/210000; acc:  43.26; ppl: 16.46; xent: 2.80; lr: 0.00000532;   0/650 tok/s; 233743 sec
[2020-04-01 12:35:08,369 INFO] Saving checkpoint ../models/model_step_141500.pt
[2020-04-01 12:35:58,531 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-04-01 12:36:35,529 INFO] Step 141550/210000; acc:  53.62; ppl:  7.64; xent: 2.03; lr: 0.00000532;   0/545 tok/s; 233830 sec
[2020-04-01 12:37:59,715 INFO] Step 141600/210000; acc:  45.89; ppl: 13.17; xent: 2.58; lr: 0.00000531;   0/804 tok/s; 233915 sec
[2020-04-01 12:38:45,965 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-04-01 12:39:24,731 INFO] Step 141650/210000; acc:  56.91; ppl:  8.35; xent: 2.12; lr: 0.00000531;   0/667 tok/s; 234000 sec
[2020-04-01 12:40:49,426 INFO] Step 141700/210000; acc:  48.70; ppl: 12.69; xent: 2.54; lr: 0.00000531;   0/657 tok/s; 234084 sec
[2020-04-01 12:41:35,820 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-04-01 12:42:14,589 INFO] Step 141750/210000; acc:  54.61; ppl:  8.50; xent: 2.14; lr: 0.00000531;   0/657 tok/s; 234169 sec
[2020-04-01 12:43:38,887 INFO] Step 141800/210000; acc:  48.08; ppl: 12.78; xent: 2.55; lr: 0.00000531;   0/608 tok/s; 234254 sec
[2020-04-01 12:44:23,424 INFO] Loading train dataset from ../bert_data/cnndm.train.103.bert.pt, number of examples: 2000
[2020-04-01 12:45:03,777 INFO] Step 141850/210000; acc:  56.84; ppl:  7.92; xent: 2.07; lr: 0.00000531;   0/816 tok/s; 234339 sec
[2020-04-01 12:46:28,283 INFO] Step 141900/210000; acc:  55.18; ppl:  8.44; xent: 2.13; lr: 0.00000531;   0/793 tok/s; 234423 sec
[2020-04-01 12:47:11,076 INFO] Loading train dataset from ../bert_data/cnndm.train.104.bert.pt, number of examples: 2000
[2020-04-01 12:47:53,282 INFO] Step 141950/210000; acc:  49.35; ppl: 13.19; xent: 2.58; lr: 0.00000531;   0/912 tok/s; 234508 sec
[2020-04-01 12:49:18,221 INFO] Step 142000/210000; acc:  52.99; ppl:  9.05; xent: 2.20; lr: 0.00000531;   0/934 tok/s; 234593 sec
[2020-04-01 12:49:18,245 INFO] Saving checkpoint ../models/model_step_142000.pt
[2020-04-01 12:50:01,632 INFO] Loading train dataset from ../bert_data/cnndm.train.105.bert.pt, number of examples: 2001
[2020-04-01 12:50:45,465 INFO] Step 142050/210000; acc:  55.35; ppl:  8.50; xent: 2.14; lr: 0.00000531;   0/800 tok/s; 234680 sec
[2020-04-01 12:52:09,568 INFO] Step 142100/210000; acc:  53.02; ppl:  9.40; xent: 2.24; lr: 0.00000531;   0/843 tok/s; 234764 sec
[2020-04-01 12:52:50,929 INFO] Loading train dataset from ../bert_data/cnndm.train.106.bert.pt, number of examples: 1999
[2020-04-01 12:53:34,792 INFO] Step 142150/210000; acc:  50.56; ppl:  9.56; xent: 2.26; lr: 0.00000530;   0/670 tok/s; 234850 sec
[2020-04-01 12:54:59,764 INFO] Step 142200/210000; acc:  44.56; ppl: 14.02; xent: 2.64; lr: 0.00000530;   0/880 tok/s; 234935 sec
[2020-04-01 12:55:39,054 INFO] Loading train dataset from ../bert_data/cnndm.train.107.bert.pt, number of examples: 1999
[2020-04-01 12:56:24,903 INFO] Step 142250/210000; acc:  56.74; ppl:  7.99; xent: 2.08; lr: 0.00000530;   0/663 tok/s; 235020 sec
[2020-04-01 12:57:49,386 INFO] Step 142300/210000; acc:  57.01; ppl:  7.48; xent: 2.01; lr: 0.00000530;   0/593 tok/s; 235104 sec
[2020-04-01 12:58:27,474 INFO] Loading train dataset from ../bert_data/cnndm.train.108.bert.pt, number of examples: 2000
[2020-04-01 12:59:14,497 INFO] Step 142350/210000; acc:  52.29; ppl:  9.74; xent: 2.28; lr: 0.00000530;   0/492 tok/s; 235189 sec
[2020-04-01 13:00:38,009 INFO] Step 142400/210000; acc:  52.31; ppl: 11.23; xent: 2.42; lr: 0.00000530;   0/945 tok/s; 235273 sec
[2020-04-01 13:01:13,871 INFO] Loading train dataset from ../bert_data/cnndm.train.109.bert.pt, number of examples: 2000
[2020-04-01 13:02:02,429 INFO] Step 142450/210000; acc:  58.77; ppl:  6.73; xent: 1.91; lr: 0.00000530;   0/824 tok/s; 235357 sec
[2020-04-01 13:03:26,340 INFO] Step 142500/210000; acc:  56.58; ppl:  8.13; xent: 2.10; lr: 0.00000530;   0/734 tok/s; 235441 sec
[2020-04-01 13:03:26,365 INFO] Saving checkpoint ../models/model_step_142500.pt
[2020-04-01 13:04:03,412 INFO] Loading train dataset from ../bert_data/cnndm.train.11.bert.pt, number of examples: 1999
[2020-04-01 13:04:54,222 INFO] Step 142550/210000; acc:  54.93; ppl:  8.40; xent: 2.13; lr: 0.00000530;   0/867 tok/s; 235529 sec
[2020-04-01 13:06:18,531 INFO] Step 142600/210000; acc:  51.72; ppl: 10.41; xent: 2.34; lr: 0.00000530;   0/878 tok/s; 235613 sec
[2020-04-01 13:06:51,269 INFO] Loading train dataset from ../bert_data/cnndm.train.110.bert.pt, number of examples: 2000
[2020-04-01 13:07:43,701 INFO] Step 142650/210000; acc:  53.18; ppl:  8.75; xent: 2.17; lr: 0.00000530;   0/941 tok/s; 235699 sec
[2020-04-01 13:09:07,947 INFO] Step 142700/210000; acc:  50.35; ppl: 10.23; xent: 2.33; lr: 0.00000529;   0/937 tok/s; 235783 sec
[2020-04-01 13:09:39,321 INFO] Loading train dataset from ../bert_data/cnndm.train.111.bert.pt, number of examples: 2000
[2020-04-01 13:10:32,973 INFO] Step 142750/210000; acc:  50.10; ppl: 10.06; xent: 2.31; lr: 0.00000529;   0/591 tok/s; 235868 sec
[2020-04-01 13:11:57,406 INFO] Step 142800/210000; acc:  54.15; ppl:  8.20; xent: 2.10; lr: 0.00000529;   0/616 tok/s; 235952 sec
[2020-04-01 13:12:28,539 INFO] Loading train dataset from ../bert_data/cnndm.train.112.bert.pt, number of examples: 1999
[2020-04-01 13:13:22,034 INFO] Step 142850/210000; acc:  50.04; ppl: 10.39; xent: 2.34; lr: 0.00000529;   0/684 tok/s; 236037 sec
[2020-04-01 13:14:46,629 INFO] Step 142900/210000; acc:  57.33; ppl:  7.12; xent: 1.96; lr: 0.00000529;   0/653 tok/s; 236121 sec
[2020-04-01 13:15:16,206 INFO] Loading train dataset from ../bert_data/cnndm.train.113.bert.pt, number of examples: 1999
[2020-04-01 13:16:12,452 INFO] Step 142950/210000; acc:  56.76; ppl:  7.47; xent: 2.01; lr: 0.00000529;   0/637 tok/s; 236207 sec
[2020-04-01 13:17:36,754 INFO] Step 143000/210000; acc:  49.62; ppl: 11.15; xent: 2.41; lr: 0.00000529;   0/632 tok/s; 236292 sec
[2020-04-01 13:17:36,777 INFO] Saving checkpoint ../models/model_step_143000.pt
[2020-04-01 13:18:06,548 INFO] Loading train dataset from ../bert_data/cnndm.train.114.bert.pt, number of examples: 1998
[2020-04-01 13:19:03,795 INFO] Step 143050/210000; acc:  53.80; ppl:  7.62; xent: 2.03; lr: 0.00000529;   0/852 tok/s; 236379 sec
[2020-04-01 13:20:27,848 INFO] Step 143100/210000; acc:  52.00; ppl:  9.78; xent: 2.28; lr: 0.00000529;   0/597 tok/s; 236463 sec
[2020-04-01 13:20:53,849 INFO] Loading train dataset from ../bert_data/cnndm.train.115.bert.pt, number of examples: 2000
[2020-04-01 13:21:53,007 INFO] Step 143150/210000; acc:  55.25; ppl:  8.30; xent: 2.12; lr: 0.00000529;   0/934 tok/s; 236548 sec
[2020-04-01 13:23:17,377 INFO] Step 143200/210000; acc:  56.66; ppl:  8.40; xent: 2.13; lr: 0.00000529;   0/892 tok/s; 236632 sec
[2020-04-01 13:23:41,746 INFO] Loading train dataset from ../bert_data/cnndm.train.116.bert.pt, number of examples: 2000
[2020-04-01 13:24:42,584 INFO] Step 143250/210000; acc:  51.36; ppl: 10.27; xent: 2.33; lr: 0.00000528;   0/788 tok/s; 236717 sec
[2020-04-01 13:26:07,158 INFO] Step 143300/210000; acc:  54.56; ppl:  8.50; xent: 2.14; lr: 0.00000528;   0/780 tok/s; 236802 sec
[2020-04-01 13:26:29,570 INFO] Loading train dataset from ../bert_data/cnndm.train.117.bert.pt, number of examples: 2000
[2020-04-01 13:27:32,041 INFO] Step 143350/210000; acc:  56.78; ppl:  7.29; xent: 1.99; lr: 0.00000528;   0/612 tok/s; 236887 sec
[2020-04-01 13:28:56,437 INFO] Step 143400/210000; acc:  52.91; ppl:  8.98; xent: 2.20; lr: 0.00000528;   0/635 tok/s; 236971 sec
[2020-04-01 13:29:17,648 INFO] Loading train dataset from ../bert_data/cnndm.train.118.bert.pt, number of examples: 2001
[2020-04-01 13:30:21,601 INFO] Step 143450/210000; acc:  52.35; ppl: 10.19; xent: 2.32; lr: 0.00000528;   0/670 tok/s; 237056 sec
[2020-04-01 13:31:46,480 INFO] Step 143500/210000; acc:  61.32; ppl:  5.28; xent: 1.66; lr: 0.00000528;   0/659 tok/s; 237141 sec
[2020-04-01 13:31:46,502 INFO] Saving checkpoint ../models/model_step_143500.pt
[2020-04-01 13:32:09,648 INFO] Loading train dataset from ../bert_data/cnndm.train.119.bert.pt, number of examples: 2000
[2020-04-01 13:33:13,779 INFO] Step 143550/210000; acc:  53.64; ppl:  8.64; xent: 2.16; lr: 0.00000528;   0/606 tok/s; 237229 sec
[2020-04-01 13:34:38,306 INFO] Step 143600/210000; acc:  55.20; ppl:  8.10; xent: 2.09; lr: 0.00000528;   0/642 tok/s; 237313 sec
[2020-04-01 13:34:57,543 INFO] Loading train dataset from ../bert_data/cnndm.train.12.bert.pt, number of examples: 2001
[2020-04-01 13:36:03,337 INFO] Step 143650/210000; acc:  57.51; ppl:  8.47; xent: 2.14; lr: 0.00000528;   0/677 tok/s; 237398 sec
[2020-04-01 13:37:27,611 INFO] Step 143700/210000; acc:  49.57; ppl: 11.17; xent: 2.41; lr: 0.00000528;   0/638 tok/s; 237482 sec
[2020-04-01 13:37:47,110 INFO] Loading train dataset from ../bert_data/cnndm.train.120.bert.pt, number of examples: 2001
[2020-04-01 13:38:53,229 INFO] Step 143750/210000; acc:  56.72; ppl:  7.29; xent: 1.99; lr: 0.00000528;   0/982 tok/s; 237568 sec
[2020-04-01 13:40:17,571 INFO] Step 143800/210000; acc:  48.39; ppl: 12.23; xent: 2.50; lr: 0.00000527;   0/936 tok/s; 237652 sec
[2020-04-01 13:40:35,111 INFO] Loading train dataset from ../bert_data/cnndm.train.121.bert.pt, number of examples: 2001
[2020-04-01 13:41:42,672 INFO] Step 143850/210000; acc:  58.09; ppl:  7.21; xent: 1.98; lr: 0.00000527;   0/945 tok/s; 237737 sec
[2020-04-01 13:43:07,068 INFO] Step 143900/210000; acc:  51.89; ppl: 10.31; xent: 2.33; lr: 0.00000527;   0/967 tok/s; 237822 sec
[2020-04-01 13:43:22,868 INFO] Loading train dataset from ../bert_data/cnndm.train.122.bert.pt, number of examples: 2000
[2020-04-01 13:44:32,184 INFO] Step 143950/210000; acc:  53.14; ppl:  9.30; xent: 2.23; lr: 0.00000527;   0/505 tok/s; 237907 sec
[2020-04-01 13:45:56,709 INFO] Step 144000/210000; acc:  55.77; ppl:  7.39; xent: 2.00; lr: 0.00000527;   0/568 tok/s; 237992 sec
[2020-04-01 13:45:56,731 INFO] Saving checkpoint ../models/model_step_144000.pt
[2020-04-01 13:46:13,154 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001
[2020-04-01 13:47:23,819 INFO] Step 144050/210000; acc:  54.94; ppl:  8.31; xent: 2.12; lr: 0.00000527;   0/628 tok/s; 238079 sec
[2020-04-01 13:48:47,587 INFO] Step 144100/210000; acc:  53.92; ppl:  8.31; xent: 2.12; lr: 0.00000527;   0/616 tok/s; 238162 sec
[2020-04-01 13:49:00,193 INFO] Loading train dataset from ../bert_data/cnndm.train.124.bert.pt, number of examples: 2001
[2020-04-01 13:50:13,067 INFO] Step 144150/210000; acc:  52.48; ppl: 10.35; xent: 2.34; lr: 0.00000527;   0/699 tok/s; 238248 sec
[2020-04-01 13:51:37,387 INFO] Step 144200/210000; acc:  52.38; ppl:  8.12; xent: 2.09; lr: 0.00000527;   0/578 tok/s; 238332 sec
[2020-04-01 13:51:50,407 INFO] Loading train dataset from ../bert_data/cnndm.train.125.bert.pt, number of examples: 2000
[2020-04-01 13:53:03,363 INFO] Step 144250/210000; acc:  54.10; ppl:  8.62; xent: 2.15; lr: 0.00000527;   0/791 tok/s; 238418 sec
[2020-04-01 13:54:27,823 INFO] Step 144300/210000; acc:  58.41; ppl:  6.97; xent: 1.94; lr: 0.00000526;   0/811 tok/s; 238503 sec
[2020-04-01 13:54:36,968 INFO] Loading train dataset from ../bert_data/cnndm.train.126.bert.pt, number of examples: 1999
[2020-04-01 13:55:52,542 INFO] Step 144350/210000; acc:  50.28; ppl: 11.80; xent: 2.47; lr: 0.00000526;   0/949 tok/s; 238587 sec
[2020-04-01 13:57:17,367 INFO] Step 144400/210000; acc:  53.29; ppl:  9.05; xent: 2.20; lr: 0.00000526;   0/846 tok/s; 238672 sec
[2020-04-01 13:57:24,849 INFO] Loading train dataset from ../bert_data/cnndm.train.127.bert.pt, number of examples: 2000
[2020-04-01 13:58:42,521 INFO] Step 144450/210000; acc:  62.92; ppl:  5.52; xent: 1.71; lr: 0.00000526;   0/877 tok/s; 238757 sec
[2020-04-01 14:00:06,912 INFO] Step 144500/210000; acc:  48.95; ppl: 10.77; xent: 2.38; lr: 0.00000526;   0/755 tok/s; 238842 sec
[2020-04-01 14:00:06,932 INFO] Saving checkpoint ../models/model_step_144500.pt
[2020-04-01 14:00:16,579 INFO] Loading train dataset from ../bert_data/cnndm.train.128.bert.pt, number of examples: 2001
[2020-04-01 14:01:34,397 INFO] Step 144550/210000; acc:  51.18; ppl: 10.35; xent: 2.34; lr: 0.00000526;   0/815 tok/s; 238929 sec
[2020-04-01 14:02:58,580 INFO] Step 144600/210000; acc:  52.47; ppl:  9.82; xent: 2.28; lr: 0.00000526;   0/892 tok/s; 239013 sec
[2020-04-01 14:03:04,276 INFO] Loading train dataset from ../bert_data/cnndm.train.129.bert.pt, number of examples: 2000
[2020-04-01 14:04:23,601 INFO] Step 144650/210000; acc:  52.59; ppl:  9.76; xent: 2.28; lr: 0.00000526;   0/605 tok/s; 239098 sec
[2020-04-01 14:05:47,966 INFO] Step 144700/210000; acc:  53.44; ppl:  8.64; xent: 2.16; lr: 0.00000526;   0/607 tok/s; 239183 sec
[2020-04-01 14:05:52,121 INFO] Loading train dataset from ../bert_data/cnndm.train.13.bert.pt, number of examples: 2001
[2020-04-01 14:07:13,228 INFO] Step 144750/210000; acc:  53.85; ppl:  8.57; xent: 2.15; lr: 0.00000526;   0/714 tok/s; 239268 sec
[2020-04-01 14:08:38,029 INFO] Step 144800/210000; acc:  50.90; ppl:  9.59; xent: 2.26; lr: 0.00000526;   0/734 tok/s; 239353 sec
[2020-04-01 14:08:40,388 INFO] Loading train dataset from ../bert_data/cnndm.train.130.bert.pt, number of examples: 2000
[2020-04-01 14:10:02,802 INFO] Step 144850/210000; acc:  50.70; ppl: 10.87; xent: 2.39; lr: 0.00000525;   0/809 tok/s; 239438 sec
[2020-04-01 14:11:26,365 INFO] Step 144900/210000; acc:  54.87; ppl:  8.46; xent: 2.14; lr: 0.00000525;   0/895 tok/s; 239521 sec
[2020-04-01 14:11:28,748 INFO] Loading train dataset from ../bert_data/cnndm.train.131.bert.pt, number of examples: 2001
[2020-04-01 14:12:51,437 INFO] Step 144950/210000; acc:  51.78; ppl: 10.26; xent: 2.33; lr: 0.00000525;   0/902 tok/s; 239606 sec
[2020-04-01 14:14:16,104 INFO] Step 145000/210000; acc:  51.23; ppl: 10.45; xent: 2.35; lr: 0.00000525;   0/633 tok/s; 239691 sec
[2020-04-01 14:14:16,108 INFO] Saving checkpoint ../models/model_step_145000.pt
[2020-04-01 14:14:18,743 INFO] Loading train dataset from ../bert_data/cnndm.train.132.bert.pt, number of examples: 2001
[2020-04-01 14:15:42,730 INFO] Step 145050/210000; acc:  56.61; ppl:  7.40; xent: 2.00; lr: 0.00000525;   0/885 tok/s; 239778 sec
[2020-04-01 14:17:07,638 INFO] Step 145100/210000; acc:  51.97; ppl:  9.36; xent: 2.24; lr: 0.00000525;   0/768 tok/s; 239862 sec
[2020-04-01 14:17:07,961 INFO] Loading train dataset from ../bert_data/cnndm.train.133.bert.pt, number of examples: 1999
[2020-04-01 14:18:31,962 INFO] Step 145150/210000; acc:  51.78; ppl:  9.29; xent: 2.23; lr: 0.00000525;   0/811 tok/s; 239947 sec
[2020-04-01 14:19:55,109 INFO] Loading train dataset from ../bert_data/cnndm.train.134.bert.pt, number of examples: 2001
[2020-04-01 14:19:56,954 INFO] Step 145200/210000; acc:  48.69; ppl: 11.37; xent: 2.43; lr: 0.00000525;   0/423 tok/s; 240032 sec
[2020-04-01 14:21:21,115 INFO] Step 145250/210000; acc:  50.18; ppl: 11.45; xent: 2.44; lr: 0.00000525;   0/495 tok/s; 240116 sec
[2020-04-01 14:22:42,817 INFO] Loading train dataset from ../bert_data/cnndm.train.135.bert.pt, number of examples: 1999
[2020-04-01 14:22:46,142 INFO] Step 145300/210000; acc:  49.67; ppl: 10.88; xent: 2.39; lr: 0.00000525;   0/633 tok/s; 240201 sec
[2020-04-01 14:24:10,191 INFO] Step 145350/210000; acc:  53.90; ppl:  8.15; xent: 2.10; lr: 0.00000525;   0/622 tok/s; 240285 sec
[2020-04-01 14:25:29,931 INFO] Loading train dataset from ../bert_data/cnndm.train.136.bert.pt, number of examples: 2001
[2020-04-01 14:25:34,960 INFO] Step 145400/210000; acc:  52.60; ppl:  9.02; xent: 2.20; lr: 0.00000525;   0/665 tok/s; 240370 sec
[2020-04-01 14:26:58,727 INFO] Step 145450/210000; acc:  56.23; ppl:  7.62; xent: 2.03; lr: 0.00000524;   0/647 tok/s; 240454 sec
[2020-04-01 14:28:18,708 INFO] Loading train dataset from ../bert_data/cnndm.train.137.bert.pt, number of examples: 2000
[2020-04-01 14:28:23,717 INFO] Step 145500/210000; acc:  53.41; ppl:  8.57; xent: 2.15; lr: 0.00000524;   0/717 tok/s; 240539 sec
[2020-04-01 14:28:23,719 INFO] Saving checkpoint ../models/model_step_145500.pt
[2020-04-01 14:29:50,058 INFO] Step 145550/210000; acc:  55.03; ppl:  7.84; xent: 2.06; lr: 0.00000524;   0/758 tok/s; 240625 sec
[2020-04-01 14:31:07,681 INFO] Loading train dataset from ../bert_data/cnndm.train.138.bert.pt, number of examples: 2000
[2020-04-01 14:31:14,324 INFO] Step 145600/210000; acc:  55.36; ppl:  8.14; xent: 2.10; lr: 0.00000524;   0/853 tok/s; 240709 sec
[2020-04-01 14:32:39,059 INFO] Step 145650/210000; acc:  53.51; ppl:  8.76; xent: 2.17; lr: 0.00000524;   0/719 tok/s; 240794 sec
[2020-04-01 14:33:55,448 INFO] Loading train dataset from ../bert_data/cnndm.train.139.bert.pt, number of examples: 2001
[2020-04-01 14:34:03,771 INFO] Step 145700/210000; acc:  50.88; ppl:  9.90; xent: 2.29; lr: 0.00000524;   0/905 tok/s; 240879 sec
[2020-04-01 14:35:27,696 INFO] Step 145750/210000; acc:  54.45; ppl:  8.12; xent: 2.09; lr: 0.00000524;   0/947 tok/s; 240963 sec
[2020-04-01 14:36:44,245 INFO] Loading train dataset from ../bert_data/cnndm.train.14.bert.pt, number of examples: 1998
[2020-04-01 14:36:52,591 INFO] Step 145800/210000; acc:  51.03; ppl:  9.33; xent: 2.23; lr: 0.00000524;   0/885 tok/s; 241047 sec
[2020-04-01 14:38:17,080 INFO] Step 145850/210000; acc:  52.03; ppl: 10.10; xent: 2.31; lr: 0.00000524;   0/878 tok/s; 241132 sec
[2020-04-01 14:39:32,027 INFO] Loading train dataset from ../bert_data/cnndm.train.140.bert.pt, number of examples: 2000
[2020-04-01 14:39:42,176 INFO] Step 145900/210000; acc:  52.00; ppl:  9.92; xent: 2.30; lr: 0.00000524;   0/731 tok/s; 241217 sec
[2020-04-01 14:41:06,860 INFO] Step 145950/210000; acc:  60.19; ppl:  5.98; xent: 1.79; lr: 0.00000524;   0/743 tok/s; 241302 sec
[2020-04-01 14:42:20,187 INFO] Loading train dataset from ../bert_data/cnndm.train.141.bert.pt, number of examples: 1999
[2020-04-01 14:42:32,028 INFO] Step 146000/210000; acc:  52.53; ppl:  9.18; xent: 2.22; lr: 0.00000523;   0/575 tok/s; 241387 sec
[2020-04-01 14:42:32,032 INFO] Saving checkpoint ../models/model_step_146000.pt
[2020-04-01 14:43:58,405 INFO] Step 146050/210000; acc:  51.06; ppl:  8.59; xent: 2.15; lr: 0.00000523;   0/576 tok/s; 241473 sec
[2020-04-01 14:45:09,457 INFO] Loading train dataset from ../bert_data/cnndm.train.142.bert.pt, number of examples: 2000
[2020-04-01 14:45:22,920 INFO] Step 146100/210000; acc:  50.77; ppl: 10.03; xent: 2.31; lr: 0.00000523;   0/584 tok/s; 241558 sec
[2020-04-01 14:46:47,054 INFO] Step 146150/210000; acc:  51.43; ppl:  8.96; xent: 2.19; lr: 0.00000523;   0/531 tok/s; 241642 sec
[2020-04-01 14:47:58,971 INFO] Loading train dataset from ../bert_data/cnndm.train.143.bert.pt, number of examples: 1084
[2020-04-01 14:48:12,382 INFO] Step 146200/210000; acc:  52.37; ppl:  9.62; xent: 2.26; lr: 0.00000523;   0/678 tok/s; 241727 sec
[2020-04-01 14:49:30,438 INFO] Loading train dataset from ../bert_data/cnndm.train.15.bert.pt, number of examples: 1999
[2020-04-01 14:49:37,139 INFO] Step 146250/210000; acc:  53.43; ppl:  9.07; xent: 2.20; lr: 0.00000523;   0/887 tok/s; 241812 sec
[2020-04-01 14:51:01,002 INFO] Step 146300/210000; acc:  53.52; ppl:  8.60; xent: 2.15; lr: 0.00000523;   0/806 tok/s; 241896 sec
[2020-04-01 14:52:17,418 INFO] Loading train dataset from ../bert_data/cnndm.train.150.bert.pt, number of examples: 1985
[2020-04-01 14:52:24,984 INFO] Step 146350/210000; acc:  66.57; ppl:  4.08; xent: 1.41; lr: 0.00000523;   0/530 tok/s; 241980 sec
[2020-04-01 14:53:38,039 INFO] Step 146400/210000; acc:  62.11; ppl:  4.60; xent: 1.53; lr: 0.00000523;   0/539 tok/s; 242053 sec
[2020-04-01 14:54:35,483 INFO] Loading train dataset from ../bert_data/cnndm.train.151.bert.pt, number of examples: 1990
[2020-04-01 14:54:51,865 INFO] Step 146450/210000; acc:  72.89; ppl:  3.07; xent: 1.12; lr: 0.00000523;   0/449 tok/s; 242127 sec
[2020-04-01 14:56:05,328 INFO] Step 146500/210000; acc:  66.67; ppl:  4.08; xent: 1.41; lr: 0.00000523;   0/657 tok/s; 242200 sec
[2020-04-01 14:56:05,331 INFO] Saving checkpoint ../models/model_step_146500.pt
[2020-04-01 14:56:58,376 INFO] Loading train dataset from ../bert_data/cnndm.train.152.bert.pt, number of examples: 993
[2020-04-01 14:57:21,664 INFO] Step 146550/210000; acc:  66.30; ppl:  4.00; xent: 1.39; lr: 0.00000522;   0/680 tok/s; 242276 sec
[2020-04-01 14:58:09,621 INFO] Loading train dataset from ../bert_data/cnndm.train.153.bert.pt, number of examples: 1982
[2020-04-01 14:58:39,471 INFO] Step 146600/210000; acc:  45.88; ppl: 12.50; xent: 2.53; lr: 0.00000522;   0/1132 tok/s; 242354 sec
[2020-04-01 15:00:03,363 INFO] Step 146650/210000; acc:  59.78; ppl:  5.45; xent: 1.70; lr: 0.00000522;   0/464 tok/s; 242438 sec
[2020-04-01 15:00:47,014 INFO] Loading train dataset from ../bert_data/cnndm.train.154.bert.pt, number of examples: 1978
[2020-04-01 15:01:27,141 INFO] Step 146700/210000; acc:  46.97; ppl: 10.66; xent: 2.37; lr: 0.00000522;   0/1311 tok/s; 242522 sec
[2020-04-01 15:02:50,586 INFO] Step 146750/210000; acc:  55.68; ppl:  6.22; xent: 1.83; lr: 0.00000522;   0/646 tok/s; 242605 sec
[2020-04-01 15:03:24,826 INFO] Loading train dataset from ../bert_data/cnndm.train.155.bert.pt, number of examples: 1988
[2020-04-01 15:04:14,753 INFO] Step 146800/210000; acc:  50.33; ppl:  8.10; xent: 2.09; lr: 0.00000522;   0/705 tok/s; 242690 sec
[2020-04-01 15:05:37,053 INFO] Step 146850/210000; acc:  48.57; ppl:  9.17; xent: 2.22; lr: 0.00000522;   0/1053 tok/s; 242772 sec
[2020-04-01 15:06:02,628 INFO] Loading train dataset from ../bert_data/cnndm.train.156.bert.pt, number of examples: 1993
[2020-04-01 15:07:00,808 INFO] Step 146900/210000; acc:  57.02; ppl:  5.58; xent: 1.72; lr: 0.00000522;   0/450 tok/s; 242856 sec
[2020-04-01 15:08:24,572 INFO] Step 146950/210000; acc:  46.60; ppl: 10.64; xent: 2.36; lr: 0.00000522;   0/1205 tok/s; 242939 sec
[2020-04-01 15:08:41,908 INFO] Loading train dataset from ../bert_data/cnndm.train.157.bert.pt, number of examples: 1981
[2020-04-01 15:09:48,876 INFO] Step 147000/210000; acc:  60.28; ppl:  5.12; xent: 1.63; lr: 0.00000522;   0/461 tok/s; 243024 sec
[2020-04-01 15:09:48,880 INFO] Saving checkpoint ../models/model_step_147000.pt
[2020-04-01 15:11:15,912 INFO] Step 147050/210000; acc:  43.18; ppl: 13.21; xent: 2.58; lr: 0.00000522;   0/1085 tok/s; 243111 sec
[2020-04-01 15:11:23,134 INFO] Loading train dataset from ../bert_data/cnndm.train.158.bert.pt, number of examples: 1987
[2020-04-01 15:12:39,108 INFO] Step 147100/210000; acc:  51.20; ppl:  7.88; xent: 2.06; lr: 0.00000521;   0/680 tok/s; 243194 sec
[2020-04-01 15:14:01,673 INFO] Loading train dataset from ../bert_data/cnndm.train.159.bert.pt, number of examples: 1986
[2020-04-01 15:14:03,445 INFO] Step 147150/210000; acc:  50.63; ppl:  9.08; xent: 2.21; lr: 0.00000521;   0/805 tok/s; 243278 sec
[2020-04-01 15:15:26,636 INFO] Step 147200/210000; acc:  53.57; ppl:  7.17; xent: 1.97; lr: 0.00000521;   0/699 tok/s; 243361 sec
[2020-04-01 15:16:40,333 INFO] Loading train dataset from ../bert_data/cnndm.train.16.bert.pt, number of examples: 2001
[2020-04-01 15:16:50,518 INFO] Step 147250/210000; acc:  52.40; ppl:  9.22; xent: 2.22; lr: 0.00000521;   0/526 tok/s; 243445 sec
[2020-04-01 15:18:14,646 INFO] Step 147300/210000; acc:  55.54; ppl:  8.06; xent: 2.09; lr: 0.00000521;   0/815 tok/s; 243529 sec
[2020-04-01 15:19:27,383 INFO] Loading train dataset from ../bert_data/cnndm.train.160.bert.pt, number of examples: 1983
[2020-04-01 15:19:38,936 INFO] Step 147350/210000; acc:  57.35; ppl:  5.83; xent: 1.76; lr: 0.00000521;   0/551 tok/s; 243614 sec
[2020-04-01 15:21:02,246 INFO] Step 147400/210000; acc:  49.89; ppl:  8.71; xent: 2.16; lr: 0.00000521;   0/961 tok/s; 243697 sec
[2020-04-01 15:22:06,373 INFO] Loading train dataset from ../bert_data/cnndm.train.161.bert.pt, number of examples: 1990
[2020-04-01 15:22:26,207 INFO] Step 147450/210000; acc:  55.63; ppl:  6.56; xent: 1.88; lr: 0.00000521;   0/590 tok/s; 243781 sec
[2020-04-01 15:23:48,483 INFO] Step 147500/210000; acc:  46.39; ppl: 10.66; xent: 2.37; lr: 0.00000521;   0/1196 tok/s; 243863 sec
[2020-04-01 15:23:48,487 INFO] Saving checkpoint ../models/model_step_147500.pt
[2020-04-01 15:24:48,332 INFO] Loading train dataset from ../bert_data/cnndm.train.162.bert.pt, number of examples: 1986
[2020-04-01 15:25:14,925 INFO] Step 147550/210000; acc:  60.72; ppl:  5.45; xent: 1.70; lr: 0.00000521;   0/500 tok/s; 243950 sec
[2020-04-01 15:26:38,571 INFO] Step 147600/210000; acc:  50.36; ppl:  9.79; xent: 2.28; lr: 0.00000521;   0/1125 tok/s; 244033 sec
[2020-04-01 15:27:25,467 INFO] Loading train dataset from ../bert_data/cnndm.train.163.bert.pt, number of examples: 1982
[2020-04-01 15:28:01,807 INFO] Step 147650/210000; acc:  54.91; ppl:  6.80; xent: 1.92; lr: 0.00000520;   0/762 tok/s; 244117 sec
[2020-04-01 15:29:25,240 INFO] Step 147700/210000; acc:  42.96; ppl: 13.39; xent: 2.59; lr: 0.00000520;   0/1039 tok/s; 244200 sec
[2020-04-01 15:30:03,601 INFO] Loading train dataset from ../bert_data/cnndm.train.164.bert.pt, number of examples: 1984
[2020-04-01 15:30:47,889 INFO] Step 147750/210000; acc:  46.80; ppl: 10.03; xent: 2.31; lr: 0.00000520;   0/985 tok/s; 244283 sec
[2020-04-01 15:32:11,563 INFO] Step 147800/210000; acc:  51.45; ppl:  7.97; xent: 2.08; lr: 0.00000520;   0/679 tok/s; 244366 sec
[2020-04-01 15:32:40,320 INFO] Loading train dataset from ../bert_data/cnndm.train.165.bert.pt, number of examples: 1984
[2020-04-01 15:33:34,938 INFO] Step 147850/210000; acc:  50.80; ppl:  9.07; xent: 2.20; lr: 0.00000520;   0/1112 tok/s; 244450 sec
[2020-04-01 15:34:58,145 INFO] Step 147900/210000; acc:  58.96; ppl:  5.20; xent: 1.65; lr: 0.00000520;   0/517 tok/s; 244533 sec
[2020-04-01 15:35:18,168 INFO] Loading train dataset from ../bert_data/cnndm.train.166.bert.pt, number of examples: 1370
[2020-04-01 15:36:21,455 INFO] Step 147950/210000; acc:  45.72; ppl: 11.29; xent: 2.42; lr: 0.00000520;   0/1239 tok/s; 244616 sec
[2020-04-01 15:37:09,126 INFO] Loading train dataset from ../bert_data/cnndm.train.167.bert.pt, number of examples: 1089
[2020-04-01 15:37:44,377 INFO] Step 148000/210000; acc:  60.99; ppl:  4.54; xent: 1.51; lr: 0.00000520;   0/809 tok/s; 244699 sec
[2020-04-01 15:37:44,404 INFO] Saving checkpoint ../models/model_step_148000.pt
[2020-04-01 15:38:35,167 INFO] Loading train dataset from ../bert_data/cnndm.train.168.bert.pt, number of examples: 1991
[2020-04-01 15:39:07,788 INFO] Step 148050/210000; acc:  46.17; ppl: 10.59; xent: 2.36; lr: 0.00000520;   0/1137 tok/s; 244783 sec
[2020-04-01 15:40:28,719 INFO] Step 148100/210000; acc:  57.72; ppl:  5.60; xent: 1.72; lr: 0.00000520;   0/641 tok/s; 244864 sec
[2020-04-01 15:41:11,332 INFO] Loading train dataset from ../bert_data/cnndm.train.169.bert.pt, number of examples: 1995
[2020-04-01 15:41:50,674 INFO] Step 148150/210000; acc:  49.13; ppl:  9.39; xent: 2.24; lr: 0.00000520;   0/1256 tok/s; 244945 sec
[2020-04-01 15:43:12,126 INFO] Step 148200/210000; acc:  59.56; ppl:  4.99; xent: 1.61; lr: 0.00000520;   0/591 tok/s; 245027 sec
[2020-04-01 15:43:46,797 INFO] Loading train dataset from ../bert_data/cnndm.train.17.bert.pt, number of examples: 2000
[2020-04-01 15:44:35,359 INFO] Step 148250/210000; acc:  56.73; ppl:  7.71; xent: 2.04; lr: 0.00000519;   0/829 tok/s; 245110 sec
[2020-04-01 15:45:59,710 INFO] Step 148300/210000; acc:  56.06; ppl:  8.10; xent: 2.09; lr: 0.00000519;   0/872 tok/s; 245195 sec
[2020-04-01 15:46:33,838 INFO] Loading train dataset from ../bert_data/cnndm.train.170.bert.pt, number of examples: 1981
[2020-04-01 15:47:22,929 INFO] Step 148350/210000; acc:  61.48; ppl:  4.89; xent: 1.59; lr: 0.00000519;   0/369 tok/s; 245278 sec
[2020-04-01 15:48:44,291 INFO] Step 148400/210000; acc:  44.42; ppl: 12.16; xent: 2.50; lr: 0.00000519;   0/1042 tok/s; 245359 sec
[2020-04-01 15:49:07,860 INFO] Loading train dataset from ../bert_data/cnndm.train.171.bert.pt, number of examples: 1990
[2020-04-01 15:50:06,590 INFO] Step 148450/210000; acc:  64.22; ppl:  4.09; xent: 1.41; lr: 0.00000519;   0/620 tok/s; 245441 sec
[2020-04-01 15:51:27,662 INFO] Step 148500/210000; acc:  46.91; ppl:  9.45; xent: 2.25; lr: 0.00000519;   0/1136 tok/s; 245522 sec
[2020-04-01 15:51:27,665 INFO] Saving checkpoint ../models/model_step_148500.pt
[2020-04-01 15:51:44,935 INFO] Loading train dataset from ../bert_data/cnndm.train.172.bert.pt, number of examples: 1983
[2020-04-01 15:52:51,277 INFO] Step 148550/210000; acc:  54.21; ppl:  5.95; xent: 1.78; lr: 0.00000519;   0/686 tok/s; 245606 sec
[2020-04-01 15:54:12,252 INFO] Step 148600/210000; acc:  45.52; ppl: 11.61; xent: 2.45; lr: 0.00000519;   0/1224 tok/s; 245687 sec
[2020-04-01 15:54:19,405 INFO] Loading train dataset from ../bert_data/cnndm.train.18.bert.pt, number of examples: 1998
[2020-04-01 15:55:37,086 INFO] Step 148650/210000; acc:  53.31; ppl:  9.57; xent: 2.26; lr: 0.00000519;   0/516 tok/s; 245772 sec
[2020-04-01 15:57:01,486 INFO] Step 148700/210000; acc:  56.39; ppl:  7.23; xent: 1.98; lr: 0.00000519;   0/858 tok/s; 245856 sec
[2020-04-01 15:57:07,186 INFO] Loading train dataset from ../bert_data/cnndm.train.19.bert.pt, number of examples: 2000
[2020-04-01 15:58:26,303 INFO] Step 148750/210000; acc:  57.95; ppl:  7.98; xent: 2.08; lr: 0.00000519;   0/848 tok/s; 245941 sec
[2020-04-01 15:59:50,764 INFO] Step 148800/210000; acc:  54.92; ppl:  8.36; xent: 2.12; lr: 0.00000518;   0/966 tok/s; 246026 sec
[2020-04-01 15:59:54,951 INFO] Loading train dataset from ../bert_data/cnndm.train.2.bert.pt, number of examples: 2001
[2020-04-01 16:01:15,582 INFO] Step 148850/210000; acc:  53.51; ppl:  9.35; xent: 2.24; lr: 0.00000518;   0/549 tok/s; 246110 sec
[2020-04-01 16:02:39,899 INFO] Step 148900/210000; acc:  49.88; ppl: 11.69; xent: 2.46; lr: 0.00000518;   0/738 tok/s; 246195 sec
[2020-04-01 16:02:42,285 INFO] Loading train dataset from ../bert_data/cnndm.train.20.bert.pt, number of examples: 2000
[2020-04-01 16:04:05,039 INFO] Step 148950/210000; acc:  51.55; ppl:  9.63; xent: 2.26; lr: 0.00000518;   0/744 tok/s; 246280 sec
[2020-04-01 16:05:29,911 INFO] Step 149000/210000; acc:  55.08; ppl:  7.83; xent: 2.06; lr: 0.00000518;   0/498 tok/s; 246365 sec
[2020-04-01 16:05:29,914 INFO] Saving checkpoint ../models/model_step_149000.pt
[2020-04-01 16:05:32,582 INFO] Loading train dataset from ../bert_data/cnndm.train.21.bert.pt, number of examples: 2001
[2020-04-01 16:06:56,554 INFO] Step 149050/210000; acc:  50.71; ppl: 10.00; xent: 2.30; lr: 0.00000518;   0/729 tok/s; 246451 sec
[2020-04-01 16:08:20,913 INFO] Step 149100/210000; acc:  54.78; ppl:  8.64; xent: 2.16; lr: 0.00000518;   0/577 tok/s; 246536 sec
[2020-04-01 16:08:21,261 INFO] Loading train dataset from ../bert_data/cnndm.train.22.bert.pt, number of examples: 1999
[2020-04-01 16:09:45,112 INFO] Step 149150/210000; acc:  52.22; ppl: 10.74; xent: 2.37; lr: 0.00000518;   0/796 tok/s; 246620 sec
[2020-04-01 16:11:08,517 INFO] Loading train dataset from ../bert_data/cnndm.train.23.bert.pt, number of examples: 2001
[2020-04-01 16:11:10,282 INFO] Step 149200/210000; acc:  56.99; ppl:  8.15; xent: 2.10; lr: 0.00000518;   0/609 tok/s; 246705 sec
[2020-04-01 16:12:34,429 INFO] Step 149250/210000; acc:  49.03; ppl: 11.62; xent: 2.45; lr: 0.00000518;   0/847 tok/s; 246789 sec
[2020-04-01 16:13:58,344 INFO] Loading train dataset from ../bert_data/cnndm.train.24.bert.pt, number of examples: 2001
[2020-04-01 16:14:00,001 INFO] Step 149300/210000; acc:  56.65; ppl:  8.12; xent: 2.09; lr: 0.00000518;   0/419 tok/s; 246875 sec
[2020-04-01 16:15:24,367 INFO] Step 149350/210000; acc:  52.31; ppl:  8.70; xent: 2.16; lr: 0.00000518;   0/682 tok/s; 246959 sec
[2020-04-01 16:16:45,883 INFO] Loading train dataset from ../bert_data/cnndm.train.25.bert.pt, number of examples: 2001
[2020-04-01 16:16:49,274 INFO] Step 149400/210000; acc:  52.49; ppl:  9.39; xent: 2.24; lr: 0.00000517;   0/580 tok/s; 247044 sec
[2020-04-01 16:18:13,768 INFO] Step 149450/210000; acc:  58.48; ppl:  6.51; xent: 1.87; lr: 0.00000517;   0/567 tok/s; 247129 sec
[2020-04-01 16:19:35,637 INFO] Loading train dataset from ../bert_data/cnndm.train.26.bert.pt, number of examples: 2000
[2020-04-01 16:19:39,018 INFO] Step 149500/210000; acc:  52.29; ppl:  8.75; xent: 2.17; lr: 0.00000517;   0/613 tok/s; 247214 sec
[2020-04-01 16:19:39,020 INFO] Saving checkpoint ../models/model_step_149500.pt
[2020-04-01 16:21:05,425 INFO] Step 149550/210000; acc:  53.95; ppl:  8.23; xent: 2.11; lr: 0.00000517;   0/559 tok/s; 247300 sec
[2020-04-01 16:22:25,327 INFO] Loading train dataset from ../bert_data/cnndm.train.27.bert.pt, number of examples: 2001
[2020-04-01 16:22:30,339 INFO] Step 149600/210000; acc:  58.07; ppl:  7.13; xent: 1.96; lr: 0.00000517;   0/731 tok/s; 247385 sec
[2020-04-01 16:23:55,038 INFO] Step 149650/210000; acc:  63.12; ppl:  5.26; xent: 1.66; lr: 0.00000517;   0/758 tok/s; 247470 sec
[2020-04-01 16:25:13,444 INFO] Loading train dataset from ../bert_data/cnndm.train.28.bert.pt, number of examples: 2000
[2020-04-01 16:25:20,271 INFO] Step 149700/210000; acc:  55.04; ppl:  8.14; xent: 2.10; lr: 0.00000517;   0/827 tok/s; 247555 sec
[2020-04-01 16:26:44,611 INFO] Step 149750/210000; acc:  52.08; ppl:  9.79; xent: 2.28; lr: 0.00000517;   0/854 tok/s; 247639 sec
[2020-04-01 16:28:01,266 INFO] Loading train dataset from ../bert_data/cnndm.train.29.bert.pt, number of examples: 1999
[2020-04-01 16:28:09,690 INFO] Step 149800/210000; acc:  49.18; ppl: 11.53; xent: 2.44; lr: 0.00000517;   0/924 tok/s; 247725 sec
[2020-04-01 16:29:33,824 INFO] Step 149850/210000; acc:  53.63; ppl:  8.74; xent: 2.17; lr: 0.00000517;   0/691 tok/s; 247809 sec
[2020-04-01 16:30:48,838 INFO] Loading train dataset from ../bert_data/cnndm.train.3.bert.pt, number of examples: 2001
[2020-04-01 16:30:58,938 INFO] Step 149900/210000; acc:  58.29; ppl:  6.49; xent: 1.87; lr: 0.00000517;   0/853 tok/s; 247894 sec
[2020-04-01 16:32:22,883 INFO] Step 149950/210000; acc:  52.67; ppl: 10.33; xent: 2.33; lr: 0.00000516;   0/883 tok/s; 247978 sec
[2020-04-01 16:33:37,647 INFO] Loading train dataset from ../bert_data/cnndm.train.30.bert.pt, number of examples: 1996
[2020-04-01 16:33:47,700 INFO] Step 150000/210000; acc:  49.77; ppl: 10.74; xent: 2.37; lr: 0.00000516;   0/665 tok/s; 248063 sec
[2020-04-01 16:33:47,703 INFO] Saving checkpoint ../models/model_step_150000.pt
[2020-04-01 16:35:14,272 INFO] Step 150050/210000; acc:  54.59; ppl:  7.03; xent: 1.95; lr: 0.00000516;   0/792 tok/s; 248149 sec
[2020-04-01 16:36:27,737 INFO] Loading train dataset from ../bert_data/cnndm.train.31.bert.pt, number of examples: 2000
[2020-04-01 16:36:39,495 INFO] Step 150100/210000; acc:  57.28; ppl:  7.43; xent: 2.01; lr: 0.00000516;   0/607 tok/s; 248234 sec
[2020-04-01 16:38:03,852 INFO] Step 150150/210000; acc:  51.17; ppl: 10.27; xent: 2.33; lr: 0.00000516;   0/663 tok/s; 248319 sec
[2020-04-01 16:39:15,106 INFO] Loading train dataset from ../bert_data/cnndm.train.32.bert.pt, number of examples: 1998
[2020-04-01 16:39:28,547 INFO] Step 150200/210000; acc:  54.82; ppl:  8.42; xent: 2.13; lr: 0.00000516;   0/619 tok/s; 248403 sec
[2020-04-01 16:40:52,679 INFO] Step 150250/210000; acc:  60.06; ppl:  5.95; xent: 1.78; lr: 0.00000516;   0/596 tok/s; 248488 sec
[2020-04-01 16:42:02,297 INFO] Loading train dataset from ../bert_data/cnndm.train.33.bert.pt, number of examples: 1999
[2020-04-01 16:42:17,545 INFO] Step 150300/210000; acc:  60.96; ppl:  5.84; xent: 1.76; lr: 0.00000516;   0/664 tok/s; 248572 sec
[2020-04-01 16:43:41,493 INFO] Step 150350/210000; acc:  55.67; ppl:  7.20; xent: 1.97; lr: 0.00000516;   0/635 tok/s; 248656 sec
[2020-04-01 16:44:51,132 INFO] Loading train dataset from ../bert_data/cnndm.train.34.bert.pt, number of examples: 2000
[2020-04-01 16:45:06,068 INFO] Step 150400/210000; acc:  52.48; ppl: 10.20; xent: 2.32; lr: 0.00000516;   0/807 tok/s; 248741 sec
[2020-04-01 16:46:30,219 INFO] Step 150450/210000; acc:  52.47; ppl:  9.12; xent: 2.21; lr: 0.00000516;   0/708 tok/s; 248825 sec
[2020-04-01 16:47:38,057 INFO] Loading train dataset from ../bert_data/cnndm.train.35.bert.pt, number of examples: 1998
[2020-04-01 16:47:54,977 INFO] Step 150500/210000; acc:  49.30; ppl: 10.89; xent: 2.39; lr: 0.00000516;   0/917 tok/s; 248910 sec
[2020-04-01 16:47:54,980 INFO] Saving checkpoint ../models/model_step_150500.pt
[2020-04-01 16:49:21,150 INFO] Step 150550/210000; acc:  53.35; ppl:  9.41; xent: 2.24; lr: 0.00000515;   0/929 tok/s; 248996 sec
[2020-04-01 16:50:27,451 INFO] Loading train dataset from ../bert_data/cnndm.train.36.bert.pt, number of examples: 2000
[2020-04-01 16:50:46,118 INFO] Step 150600/210000; acc:  55.69; ppl:  7.34; xent: 1.99; lr: 0.00000515;   0/738 tok/s; 249081 sec
[2020-04-01 16:52:10,533 INFO] Step 150650/210000; acc:  50.78; ppl:  9.61; xent: 2.26; lr: 0.00000515;   0/585 tok/s; 249165 sec
[2020-04-01 16:53:15,490 INFO] Loading train dataset from ../bert_data/cnndm.train.37.bert.pt, number of examples: 1999
[2020-04-01 16:53:35,714 INFO] Step 150700/210000; acc:  56.28; ppl:  7.11; xent: 1.96; lr: 0.00000515;   0/580 tok/s; 249251 sec
[2020-04-01 16:54:59,842 INFO] Step 150750/210000; acc:  52.49; ppl:  7.53; xent: 2.02; lr: 0.00000515;   0/495 tok/s; 249335 sec
[2020-04-01 16:56:04,689 INFO] Loading train dataset from ../bert_data/cnndm.train.38.bert.pt, number of examples: 2001
[2020-04-01 16:56:24,838 INFO] Step 150800/210000; acc:  58.43; ppl:  6.85; xent: 1.92; lr: 0.00000515;   0/582 tok/s; 249420 sec
[2020-04-01 16:57:49,384 INFO] Step 150850/210000; acc:  56.86; ppl:  7.59; xent: 2.03; lr: 0.00000515;   0/656 tok/s; 249504 sec
[2020-04-01 16:58:52,647 INFO] Loading train dataset from ../bert_data/cnndm.train.39.bert.pt, number of examples: 2000
[2020-04-01 16:59:14,579 INFO] Step 150900/210000; acc:  49.07; ppl: 11.65; xent: 2.46; lr: 0.00000515;   0/758 tok/s; 249589 sec
[2020-04-01 17:00:38,494 INFO] Step 150950/210000; acc:  52.24; ppl:  8.71; xent: 2.16; lr: 0.00000515;   0/735 tok/s; 249673 sec
[2020-04-01 17:01:39,925 INFO] Loading train dataset from ../bert_data/cnndm.train.4.bert.pt, number of examples: 2001
[2020-04-01 17:02:03,467 INFO] Step 151000/210000; acc:  52.48; ppl:  9.29; xent: 2.23; lr: 0.00000515;   0/816 tok/s; 249758 sec
[2020-04-01 17:02:03,471 INFO] Saving checkpoint ../models/model_step_151000.pt
[2020-04-01 17:03:30,194 INFO] Step 151050/210000; acc:  51.67; ppl: 10.52; xent: 2.35; lr: 0.00000515;   0/771 tok/s; 249845 sec
[2020-04-01 17:04:31,202 INFO] Loading train dataset from ../bert_data/cnndm.train.40.bert.pt, number of examples: 1999
[2020-04-01 17:04:54,902 INFO] Step 151100/210000; acc:  49.23; ppl: 12.49; xent: 2.52; lr: 0.00000515;   0/755 tok/s; 249930 sec
[2020-04-01 17:06:19,298 INFO] Step 151150/210000; acc:  54.39; ppl:  8.94; xent: 2.19; lr: 0.00000514;   0/926 tok/s; 250014 sec
[2020-04-01 17:07:19,260 INFO] Loading train dataset from ../bert_data/cnndm.train.41.bert.pt, number of examples: 2000
[2020-04-01 17:07:44,562 INFO] Step 151200/210000; acc:  52.61; ppl:  8.85; xent: 2.18; lr: 0.00000514;   0/1007 tok/s; 250099 sec
[2020-04-01 17:09:08,133 INFO] Step 151250/210000; acc:  55.28; ppl:  9.30; xent: 2.23; lr: 0.00000514;   0/849 tok/s; 250183 sec
[2020-04-01 17:10:05,989 INFO] Loading train dataset from ../bert_data/cnndm.train.42.bert.pt, number of examples: 2000
[2020-04-01 17:10:33,272 INFO] Step 151300/210000; acc:  58.95; ppl:  6.67; xent: 1.90; lr: 0.00000514;   0/721 tok/s; 250268 sec
[2020-04-01 17:11:57,603 INFO] Step 151350/210000; acc:  53.43; ppl: 10.03; xent: 2.31; lr: 0.00000514;   0/574 tok/s; 250352 sec
[2020-04-01 17:12:53,983 INFO] Loading train dataset from ../bert_data/cnndm.train.43.bert.pt, number of examples: 2001
[2020-04-01 17:13:22,412 INFO] Step 151400/210000; acc:  51.92; ppl: 10.08; xent: 2.31; lr: 0.00000514;   0/900 tok/s; 250437 sec
[2020-04-01 17:14:46,803 INFO] Step 151450/210000; acc:  60.40; ppl:  5.91; xent: 1.78; lr: 0.00000514;   0/841 tok/s; 250522 sec
[2020-04-01 17:15:43,517 INFO] Loading train dataset from ../bert_data/cnndm.train.44.bert.pt, number of examples: 1998
[2020-04-01 17:16:12,286 INFO] Step 151500/210000; acc:  58.73; ppl:  6.76; xent: 1.91; lr: 0.00000514;   0/590 tok/s; 250607 sec
[2020-04-01 17:16:12,288 INFO] Saving checkpoint ../models/model_step_151500.pt
[2020-04-01 17:17:39,008 INFO] Step 151550/210000; acc:  58.62; ppl:  6.33; xent: 1.85; lr: 0.00000514;   0/601 tok/s; 250694 sec
[2020-04-01 17:18:34,463 INFO] Loading train dataset from ../bert_data/cnndm.train.45.bert.pt, number of examples: 2001
[2020-04-01 17:19:04,771 INFO] Step 151600/210000; acc:  55.02; ppl:  8.44; xent: 2.13; lr: 0.00000514;   0/733 tok/s; 250780 sec
[2020-04-01 17:20:29,030 INFO] Step 151650/210000; acc:  55.90; ppl:  7.59; xent: 2.03; lr: 0.00000514;   0/636 tok/s; 250864 sec
[2020-04-01 17:21:22,215 INFO] Loading train dataset from ../bert_data/cnndm.train.46.bert.pt, number of examples: 2001
[2020-04-01 17:21:54,112 INFO] Step 151700/210000; acc:  55.75; ppl:  7.82; xent: 2.06; lr: 0.00000513;   0/788 tok/s; 250949 sec
[2020-04-01 17:23:17,754 INFO] Step 151750/210000; acc:  54.02; ppl:  8.33; xent: 2.12; lr: 0.00000513;   0/615 tok/s; 251033 sec
[2020-04-01 17:24:10,780 INFO] Loading train dataset from ../bert_data/cnndm.train.47.bert.pt, number of examples: 2000
[2020-04-01 17:24:42,934 INFO] Step 151800/210000; acc:  50.16; ppl: 11.09; xent: 2.41; lr: 0.00000513;   0/922 tok/s; 251118 sec
[2020-04-01 17:26:07,050 INFO] Step 151850/210000; acc:  56.82; ppl:  6.72; xent: 1.90; lr: 0.00000513;   0/632 tok/s; 251202 sec
[2020-04-01 17:26:58,241 INFO] Loading train dataset from ../bert_data/cnndm.train.48.bert.pt, number of examples: 2000
[2020-04-01 17:27:31,933 INFO] Step 151900/210000; acc:  56.94; ppl:  7.21; xent: 1.98; lr: 0.00000513;   0/850 tok/s; 251287 sec
[2020-04-01 17:28:56,144 INFO] Step 151950/210000; acc:  52.21; ppl:  9.82; xent: 2.28; lr: 0.00000513;   0/1001 tok/s; 251371 sec
[2020-04-01 17:29:46,233 INFO] Loading train dataset from ../bert_data/cnndm.train.49.bert.pt, number of examples: 2001
[2020-04-01 17:30:21,781 INFO] Step 152000/210000; acc:  50.48; ppl:  9.89; xent: 2.29; lr: 0.00000513;   0/871 tok/s; 251457 sec
[2020-04-01 17:30:21,784 INFO] Saving checkpoint ../models/model_step_152000.pt
[2020-04-01 17:31:48,162 INFO] Step 152050/210000; acc:  54.22; ppl:  8.34; xent: 2.12; lr: 0.00000513;   0/861 tok/s; 251543 sec
[2020-04-01 17:32:38,057 INFO] Loading train dataset from ../bert_data/cnndm.train.5.bert.pt, number of examples: 2001
[2020-04-01 17:33:13,266 INFO] Step 152100/210000; acc:  57.98; ppl:  6.94; xent: 1.94; lr: 0.00000513;   0/516 tok/s; 251628 sec
[2020-04-01 17:34:37,499 INFO] Step 152150/210000; acc:  53.45; ppl:  8.52; xent: 2.14; lr: 0.00000513;   0/530 tok/s; 251712 sec
[2020-04-01 17:35:25,375 INFO] Loading train dataset from ../bert_data/cnndm.train.50.bert.pt, number of examples: 2001
[2020-04-01 17:36:02,103 INFO] Step 152200/210000; acc:  51.56; ppl:  9.35; xent: 2.24; lr: 0.00000513;   0/567 tok/s; 251797 sec
[2020-04-01 17:37:26,074 INFO] Step 152250/210000; acc:  56.87; ppl:  6.70; xent: 1.90; lr: 0.00000513;   0/830 tok/s; 251881 sec
[2020-04-01 17:38:14,133 INFO] Loading train dataset from ../bert_data/cnndm.train.51.bert.pt, number of examples: 2000
[2020-04-01 17:38:51,063 INFO] Step 152300/210000; acc:  57.57; ppl:  6.78; xent: 1.91; lr: 0.00000512;   0/579 tok/s; 251966 sec
[2020-04-01 17:40:15,325 INFO] Step 152350/210000; acc:  55.94; ppl:  7.82; xent: 2.06; lr: 0.00000512;   0/631 tok/s; 252050 sec
[2020-04-01 17:41:01,242 INFO] Loading train dataset from ../bert_data/cnndm.train.52.bert.pt, number of examples: 2001
[2020-04-01 17:41:40,141 INFO] Step 152400/210000; acc:  55.56; ppl:  7.60; xent: 2.03; lr: 0.00000512;   0/759 tok/s; 252135 sec
[2020-04-01 17:43:04,105 INFO] Step 152450/210000; acc:  54.31; ppl:  8.65; xent: 2.16; lr: 0.00000512;   0/689 tok/s; 252219 sec
[2020-04-01 17:43:48,752 INFO] Loading train dataset from ../bert_data/cnndm.train.53.bert.pt, number of examples: 1999
[2020-04-01 17:44:29,186 INFO] Step 152500/210000; acc:  57.25; ppl:  7.52; xent: 2.02; lr: 0.00000512;   0/755 tok/s; 252304 sec
[2020-04-01 17:44:29,190 INFO] Saving checkpoint ../models/model_step_152500.pt
[2020-04-01 17:45:55,666 INFO] Step 152550/210000; acc:  54.70; ppl:  8.34; xent: 2.12; lr: 0.00000512;   0/643 tok/s; 252390 sec
[2020-04-01 17:46:40,935 INFO] Loading train dataset from ../bert_data/cnndm.train.54.bert.pt, number of examples: 2001
[2020-04-01 17:47:20,953 INFO] Step 152600/210000; acc:  58.89; ppl:  6.68; xent: 1.90; lr: 0.00000512;   0/900 tok/s; 252476 sec
[2020-04-01 17:48:45,557 INFO] Step 152650/210000; acc:  52.66; ppl:  9.43; xent: 2.24; lr: 0.00000512;   0/752 tok/s; 252560 sec
[2020-04-01 17:49:28,433 INFO] Loading train dataset from ../bert_data/cnndm.train.55.bert.pt, number of examples: 1999
[2020-04-01 17:50:10,448 INFO] Step 152700/210000; acc:  53.45; ppl:  9.23; xent: 2.22; lr: 0.00000512;   0/931 tok/s; 252645 sec
[2020-04-01 17:51:34,583 INFO] Step 152750/210000; acc:  51.32; ppl:  9.15; xent: 2.21; lr: 0.00000512;   0/809 tok/s; 252729 sec
[2020-04-01 17:52:15,990 INFO] Loading train dataset from ../bert_data/cnndm.train.56.bert.pt, number of examples: 2001
[2020-04-01 17:52:59,810 INFO] Step 152800/210000; acc:  48.40; ppl: 12.30; xent: 2.51; lr: 0.00000512;   0/982 tok/s; 252815 sec
[2020-04-01 17:54:23,925 INFO] Step 152850/210000; acc:  55.16; ppl:  8.07; xent: 2.09; lr: 0.00000512;   0/797 tok/s; 252899 sec
[2020-04-01 17:55:03,516 INFO] Loading train dataset from ../bert_data/cnndm.train.57.bert.pt, number of examples: 1999
[2020-04-01 17:55:48,849 INFO] Step 152900/210000; acc:  55.57; ppl:  6.91; xent: 1.93; lr: 0.00000511;   0/546 tok/s; 252984 sec
[2020-04-01 17:57:13,311 INFO] Step 152950/210000; acc:  53.76; ppl:  8.61; xent: 2.15; lr: 0.00000511;   0/554 tok/s; 253068 sec
[2020-04-01 17:57:52,837 INFO] Loading train dataset from ../bert_data/cnndm.train.58.bert.pt, number of examples: 2000
[2020-04-01 17:58:38,189 INFO] Step 153000/210000; acc:  56.99; ppl:  7.50; xent: 2.01; lr: 0.00000511;   0/613 tok/s; 253153 sec
[2020-04-01 17:58:38,192 INFO] Saving checkpoint ../models/model_step_153000.pt
[2020-04-01 18:00:04,573 INFO] Step 153050/210000; acc:  55.16; ppl:  8.33; xent: 2.12; lr: 0.00000511;   0/570 tok/s; 253239 sec
[2020-04-01 18:00:42,648 INFO] Loading train dataset from ../bert_data/cnndm.train.59.bert.pt, number of examples: 2000
[2020-04-01 18:01:29,896 INFO] Step 153100/210000; acc:  51.82; ppl:  8.98; xent: 2.20; lr: 0.00000511;   0/707 tok/s; 253325 sec
[2020-04-01 18:02:54,042 INFO] Step 153150/210000; acc:  56.58; ppl:  7.04; xent: 1.95; lr: 0.00000511;   0/640 tok/s; 253409 sec
[2020-04-01 18:03:30,179 INFO] Loading train dataset from ../bert_data/cnndm.train.6.bert.pt, number of examples: 2001
[2020-04-01 18:04:19,169 INFO] Step 153200/210000; acc:  49.01; ppl: 11.45; xent: 2.44; lr: 0.00000511;   0/931 tok/s; 253494 sec
[2020-04-01 18:05:43,922 INFO] Step 153250/210000; acc:  59.10; ppl:  6.19; xent: 1.82; lr: 0.00000511;   0/907 tok/s; 253579 sec
[2020-04-01 18:06:18,419 INFO] Loading train dataset from ../bert_data/cnndm.train.60.bert.pt, number of examples: 2000
[2020-04-01 18:07:08,794 INFO] Step 153300/210000; acc:  50.26; ppl: 10.57; xent: 2.36; lr: 0.00000511;   0/924 tok/s; 253664 sec
[2020-04-01 18:08:33,142 INFO] Step 153350/210000; acc:  57.00; ppl:  7.68; xent: 2.04; lr: 0.00000511;   0/895 tok/s; 253748 sec
[2020-04-01 18:09:05,878 INFO] Loading train dataset from ../bert_data/cnndm.train.61.bert.pt, number of examples: 2001
[2020-04-01 18:09:57,957 INFO] Step 153400/210000; acc:  54.07; ppl:  7.65; xent: 2.04; lr: 0.00000511;   0/708 tok/s; 253833 sec
[2020-04-01 18:11:22,264 INFO] Step 153450/210000; acc:  56.78; ppl:  7.25; xent: 1.98; lr: 0.00000511;   0/644 tok/s; 253917 sec
[2020-04-01 18:11:52,967 INFO] Loading train dataset from ../bert_data/cnndm.train.62.bert.pt, number of examples: 2001
[2020-04-01 18:12:46,815 INFO] Step 153500/210000; acc:  55.41; ppl:  7.66; xent: 2.04; lr: 0.00000510;   0/573 tok/s; 254002 sec
[2020-04-01 18:12:46,819 INFO] Saving checkpoint ../models/model_step_153500.pt
[2020-04-01 18:14:13,566 INFO] Step 153550/210000; acc:  54.21; ppl:  9.33; xent: 2.23; lr: 0.00000510;   0/596 tok/s; 254088 sec
[2020-04-01 18:14:44,509 INFO] Loading train dataset from ../bert_data/cnndm.train.63.bert.pt, number of examples: 2001
[2020-04-01 18:15:38,454 INFO] Step 153600/210000; acc:  59.66; ppl:  6.35; xent: 1.85; lr: 0.00000510;   0/566 tok/s; 254173 sec
[2020-04-01 18:17:02,425 INFO] Step 153650/210000; acc:  55.42; ppl:  8.05; xent: 2.09; lr: 0.00000510;   0/786 tok/s; 254257 sec
[2020-04-01 18:17:31,837 INFO] Loading train dataset from ../bert_data/cnndm.train.64.bert.pt, number of examples: 2001
[2020-04-01 18:18:27,489 INFO] Step 153700/210000; acc:  51.32; ppl: 10.17; xent: 2.32; lr: 0.00000510;   0/590 tok/s; 254342 sec
[2020-04-01 18:19:51,981 INFO] Step 153750/210000; acc:  54.58; ppl:  7.93; xent: 2.07; lr: 0.00000510;   0/799 tok/s; 254427 sec
[2020-04-01 18:20:19,649 INFO] Loading train dataset from ../bert_data/cnndm.train.65.bert.pt, number of examples: 2000
[2020-04-01 18:21:16,760 INFO] Step 153800/210000; acc:  60.09; ppl:  6.11; xent: 1.81; lr: 0.00000510;   0/700 tok/s; 254512 sec
[2020-04-01 18:22:41,386 INFO] Step 153850/210000; acc:  47.75; ppl: 12.34; xent: 2.51; lr: 0.00000510;   0/691 tok/s; 254596 sec
[2020-04-01 18:23:09,118 INFO] Loading train dataset from ../bert_data/cnndm.train.66.bert.pt, number of examples: 2001
[2020-04-01 18:24:06,467 INFO] Step 153900/210000; acc:  52.08; ppl:  9.66; xent: 2.27; lr: 0.00000510;   0/856 tok/s; 254681 sec
[2020-04-01 18:25:31,121 INFO] Step 153950/210000; acc:  54.00; ppl:  9.34; xent: 2.23; lr: 0.00000510;   0/617 tok/s; 254766 sec
[2020-04-01 18:25:56,870 INFO] Loading train dataset from ../bert_data/cnndm.train.67.bert.pt, number of examples: 1999
[2020-04-01 18:26:56,288 INFO] Step 154000/210000; acc:  53.69; ppl:  8.04; xent: 2.08; lr: 0.00000510;   0/910 tok/s; 254851 sec
[2020-04-01 18:26:56,293 INFO] Saving checkpoint ../models/model_step_154000.pt
[2020-04-01 18:28:22,521 INFO] Step 154050/210000; acc:  54.49; ppl:  7.90; xent: 2.07; lr: 0.00000510;   0/735 tok/s; 254937 sec
[2020-04-01 18:28:47,132 INFO] Loading train dataset from ../bert_data/cnndm.train.68.bert.pt, number of examples: 1999
[2020-04-01 18:29:47,621 INFO] Step 154100/210000; acc:  53.44; ppl:  8.97; xent: 2.19; lr: 0.00000509;   0/914 tok/s; 255022 sec
[2020-04-01 18:31:12,484 INFO] Step 154150/210000; acc:  57.75; ppl:  7.15; xent: 1.97; lr: 0.00000509;   0/804 tok/s; 255107 sec
[2020-04-01 18:31:35,201 INFO] Loading train dataset from ../bert_data/cnndm.train.69.bert.pt, number of examples: 2000
[2020-04-01 18:32:37,426 INFO] Step 154200/210000; acc:  52.64; ppl:  9.23; xent: 2.22; lr: 0.00000509;   0/616 tok/s; 255192 sec
[2020-04-01 18:34:01,798 INFO] Step 154250/210000; acc:  59.19; ppl:  5.96; xent: 1.78; lr: 0.00000509;   0/639 tok/s; 255277 sec
[2020-04-01 18:34:24,342 INFO] Loading train dataset from ../bert_data/cnndm.train.7.bert.pt, number of examples: 2001
[2020-04-01 18:35:26,422 INFO] Step 154300/210000; acc:  58.48; ppl:  6.28; xent: 1.84; lr: 0.00000509;   0/649 tok/s; 255361 sec
[2020-04-01 18:36:50,990 INFO] Step 154350/210000; acc:  54.86; ppl:  7.61; xent: 2.03; lr: 0.00000509;   0/659 tok/s; 255446 sec
[2020-04-01 18:37:12,114 INFO] Loading train dataset from ../bert_data/cnndm.train.70.bert.pt, number of examples: 2000
[2020-04-01 18:38:16,171 INFO] Step 154400/210000; acc:  54.83; ppl:  8.76; xent: 2.17; lr: 0.00000509;   0/730 tok/s; 255531 sec
[2020-04-01 18:39:40,191 INFO] Step 154450/210000; acc:  55.50; ppl:  7.76; xent: 2.05; lr: 0.00000509;   0/657 tok/s; 255615 sec
[2020-04-01 18:40:01,316 INFO] Loading train dataset from ../bert_data/cnndm.train.71.bert.pt, number of examples: 1999
[2020-04-01 18:41:05,335 INFO] Step 154500/210000; acc:  49.44; ppl: 10.28; xent: 2.33; lr: 0.00000509;   0/748 tok/s; 255700 sec
[2020-04-01 18:41:05,339 INFO] Saving checkpoint ../models/model_step_154500.pt
[2020-04-01 18:42:31,898 INFO] Step 154550/210000; acc:  55.04; ppl:  8.26; xent: 2.11; lr: 0.00000509;   0/779 tok/s; 255787 sec
[2020-04-01 18:42:51,205 INFO] Loading train dataset from ../bert_data/cnndm.train.72.bert.pt, number of examples: 2000
[2020-04-01 18:43:56,969 INFO] Step 154600/210000; acc:  55.91; ppl:  8.05; xent: 2.09; lr: 0.00000509;   0/764 tok/s; 255872 sec
[2020-04-01 18:45:21,488 INFO] Step 154650/210000; acc:  53.01; ppl:  8.45; xent: 2.13; lr: 0.00000509;   0/742 tok/s; 255956 sec
[2020-04-01 18:45:38,925 INFO] Loading train dataset from ../bert_data/cnndm.train.73.bert.pt, number of examples: 2000
[2020-04-01 18:46:46,398 INFO] Step 154700/210000; acc:  53.51; ppl:  8.50; xent: 2.14; lr: 0.00000508;   0/1019 tok/s; 256041 sec
[2020-04-01 18:48:10,335 INFO] Step 154750/210000; acc:  54.01; ppl:  8.43; xent: 2.13; lr: 0.00000508;   0/911 tok/s; 256125 sec
[2020-04-01 18:48:26,259 INFO] Loading train dataset from ../bert_data/cnndm.train.74.bert.pt, number of examples: 2000
[2020-04-01 18:49:35,312 INFO] Step 154800/210000; acc:  57.45; ppl:  7.61; xent: 2.03; lr: 0.00000508;   0/833 tok/s; 256210 sec
[2020-04-01 18:50:59,291 INFO] Step 154850/210000; acc:  56.21; ppl:  7.40; xent: 2.00; lr: 0.00000508;   0/838 tok/s; 256294 sec
[2020-04-01 18:51:13,560 INFO] Loading train dataset from ../bert_data/cnndm.train.75.bert.pt, number of examples: 1999
[2020-04-01 18:52:24,682 INFO] Step 154900/210000; acc:  49.36; ppl:  9.95; xent: 2.30; lr: 0.00000508;   0/848 tok/s; 256380 sec
[2020-04-01 18:53:48,777 INFO] Step 154950/210000; acc:  54.79; ppl:  7.97; xent: 2.08; lr: 0.00000508;   0/903 tok/s; 256464 sec
[2020-04-01 18:54:03,340 INFO] Loading train dataset from ../bert_data/cnndm.train.76.bert.pt, number of examples: 1999
[2020-04-01 18:55:14,083 INFO] Step 155000/210000; acc:  51.36; ppl:  8.65; xent: 2.16; lr: 0.00000508;   0/640 tok/s; 256549 sec
[2020-04-01 18:55:14,087 INFO] Saving checkpoint ../models/model_step_155000.pt
[2020-04-01 18:56:40,431 INFO] Step 155050/210000; acc:  54.01; ppl:  8.24; xent: 2.11; lr: 0.00000508;   0/553 tok/s; 256635 sec
[2020-04-01 18:56:52,930 INFO] Loading train dataset from ../bert_data/cnndm.train.77.bert.pt, number of examples: 2001
[2020-04-01 18:58:05,893 INFO] Step 155100/210000; acc:  57.68; ppl:  7.25; xent: 1.98; lr: 0.00000508;   0/740 tok/s; 256721 sec
[2020-04-01 18:59:30,061 INFO] Step 155150/210000; acc:  53.63; ppl:  8.67; xent: 2.16; lr: 0.00000508;   0/716 tok/s; 256805 sec
[2020-04-01 18:59:40,978 INFO] Loading train dataset from ../bert_data/cnndm.train.78.bert.pt, number of examples: 2001
[2020-04-01 19:00:54,667 INFO] Step 155200/210000; acc:  52.64; ppl:  8.53; xent: 2.14; lr: 0.00000508;   0/859 tok/s; 256889 sec
[2020-04-01 19:02:18,894 INFO] Step 155250/210000; acc:  56.54; ppl:  7.60; xent: 2.03; lr: 0.00000508;   0/853 tok/s; 256974 sec
[2020-04-01 19:02:30,241 INFO] Loading train dataset from ../bert_data/cnndm.train.79.bert.pt, number of examples: 1999
[2020-04-01 19:03:44,264 INFO] Step 155300/210000; acc:  51.96; ppl: 10.78; xent: 2.38; lr: 0.00000508;   0/875 tok/s; 257059 sec
[2020-04-01 19:05:08,637 INFO] Step 155350/210000; acc:  51.07; ppl:  9.39; xent: 2.24; lr: 0.00000507;   0/901 tok/s; 257143 sec
[2020-04-01 19:05:17,725 INFO] Loading train dataset from ../bert_data/cnndm.train.8.bert.pt, number of examples: 2000
[2020-04-01 19:06:33,750 INFO] Step 155400/210000; acc:  49.73; ppl: 10.52; xent: 2.35; lr: 0.00000507;   0/888 tok/s; 257229 sec
[2020-04-01 19:07:58,213 INFO] Step 155450/210000; acc:  54.47; ppl:  7.97; xent: 2.08; lr: 0.00000507;   0/780 tok/s; 257313 sec
[2020-04-01 19:08:05,824 INFO] Loading train dataset from ../bert_data/cnndm.train.80.bert.pt, number of examples: 1999
[2020-04-01 19:09:23,595 INFO] Step 155500/210000; acc:  53.51; ppl:  7.87; xent: 2.06; lr: 0.00000507;   0/522 tok/s; 257398 sec
[2020-04-01 19:09:23,599 INFO] Saving checkpoint ../models/model_step_155500.pt
[2020-04-01 19:10:50,010 INFO] Step 155550/210000; acc:  55.00; ppl:  8.22; xent: 2.11; lr: 0.00000507;   0/933 tok/s; 257485 sec
[2020-04-01 19:10:55,754 INFO] Loading train dataset from ../bert_data/cnndm.train.81.bert.pt, number of examples: 2000
[2020-04-01 19:12:15,052 INFO] Step 155600/210000; acc:  57.21; ppl:  6.95; xent: 1.94; lr: 0.00000507;   0/541 tok/s; 257570 sec
[2020-04-01 19:13:38,866 INFO] Step 155650/210000; acc:  49.04; ppl: 11.16; xent: 2.41; lr: 0.00000507;   0/777 tok/s; 257654 sec
[2020-04-01 19:13:42,986 INFO] Loading train dataset from ../bert_data/cnndm.train.82.bert.pt, number of examples: 2001
[2020-04-01 19:15:03,432 INFO] Step 155700/210000; acc:  53.65; ppl:  8.26; xent: 2.11; lr: 0.00000507;   0/632 tok/s; 257738 sec
[2020-04-01 19:16:28,145 INFO] Step 155750/210000; acc:  62.46; ppl:  5.73; xent: 1.75; lr: 0.00000507;   0/743 tok/s; 257823 sec
[2020-04-01 19:16:30,584 INFO] Loading train dataset from ../bert_data/cnndm.train.83.bert.pt, number of examples: 1999
[2020-04-01 19:17:53,513 INFO] Step 155800/210000; acc:  52.69; ppl:  9.32; xent: 2.23; lr: 0.00000507;   0/785 tok/s; 257908 sec
[2020-04-01 19:19:18,596 INFO] Step 155850/210000; acc:  56.09; ppl:  6.89; xent: 1.93; lr: 0.00000507;   0/656 tok/s; 257993 sec
[2020-04-01 19:19:18,961 INFO] Loading train dataset from ../bert_data/cnndm.train.84.bert.pt, number of examples: 2000
[2020-04-01 19:20:43,239 INFO] Step 155900/210000; acc:  56.39; ppl:  7.64; xent: 2.03; lr: 0.00000507;   0/844 tok/s; 258078 sec
[2020-04-01 19:22:06,423 INFO] Loading train dataset from ../bert_data/cnndm.train.85.bert.pt, number of examples: 2001
[2020-04-01 19:22:08,307 INFO] Step 155950/210000; acc:  57.07; ppl:  7.37; xent: 2.00; lr: 0.00000506;   0/644 tok/s; 258163 sec
[2020-04-01 19:23:32,597 INFO] Step 156000/210000; acc:  46.48; ppl: 13.33; xent: 2.59; lr: 0.00000506;   0/893 tok/s; 258247 sec
[2020-04-01 19:23:32,601 INFO] Saving checkpoint ../models/model_step_156000.pt
[2020-04-01 19:24:57,883 INFO] Loading train dataset from ../bert_data/cnndm.train.86.bert.pt, number of examples: 1999
[2020-04-01 19:24:59,624 INFO] Step 156050/210000; acc:  52.98; ppl:  8.45; xent: 2.13; lr: 0.00000506;   0/390 tok/s; 258334 sec
[2020-04-01 19:26:24,042 INFO] Step 156100/210000; acc:  54.94; ppl:  8.74; xent: 2.17; lr: 0.00000506;   0/908 tok/s; 258419 sec
[2020-04-01 19:27:47,013 INFO] Loading train dataset from ../bert_data/cnndm.train.87.bert.pt, number of examples: 1999
[2020-04-01 19:27:50,339 INFO] Step 156150/210000; acc:  60.34; ppl:  5.94; xent: 1.78; lr: 0.00000506;   0/636 tok/s; 258505 sec
[2020-04-01 19:29:15,127 INFO] Step 156200/210000; acc:  55.65; ppl:  7.52; xent: 2.02; lr: 0.00000506;   0/624 tok/s; 258590 sec
[2020-04-01 19:30:35,502 INFO] Loading train dataset from ../bert_data/cnndm.train.88.bert.pt, number of examples: 1999
[2020-04-01 19:30:40,411 INFO] Step 156250/210000; acc:  55.04; ppl:  8.03; xent: 2.08; lr: 0.00000506;   0/717 tok/s; 258675 sec
[2020-04-01 19:32:04,680 INFO] Step 156300/210000; acc:  54.72; ppl:  8.49; xent: 2.14; lr: 0.00000506;   0/786 tok/s; 258760 sec
[2020-04-01 19:33:22,615 INFO] Loading train dataset from ../bert_data/cnndm.train.89.bert.pt, number of examples: 2001
[2020-04-01 19:33:29,312 INFO] Step 156350/210000; acc:  56.87; ppl:  7.14; xent: 1.97; lr: 0.00000506;   0/869 tok/s; 258844 sec
[2020-04-01 19:34:53,055 INFO] Step 156400/210000; acc:  57.08; ppl:  6.80; xent: 1.92; lr: 0.00000506;   0/724 tok/s; 258928 sec
[2020-04-01 19:36:09,763 INFO] Loading train dataset from ../bert_data/cnndm.train.9.bert.pt, number of examples: 1999
[2020-04-01 19:36:18,177 INFO] Step 156450/210000; acc:  52.73; ppl:  8.77; xent: 2.17; lr: 0.00000506;   0/906 tok/s; 259013 sec
[2020-04-01 19:37:42,193 INFO] Step 156500/210000; acc:  53.18; ppl:  8.91; xent: 2.19; lr: 0.00000506;   0/930 tok/s; 259097 sec
[2020-04-01 19:37:42,220 INFO] Saving checkpoint ../models/model_step_156500.pt
[2020-04-01 19:39:01,317 INFO] Loading train dataset from ../bert_data/cnndm.train.90.bert.pt, number of examples: 2000
[2020-04-01 19:39:09,928 INFO] Step 156550/210000; acc:  47.45; ppl: 11.80; xent: 2.47; lr: 0.00000505;   0/1037 tok/s; 259185 sec
[2020-04-01 19:40:34,684 INFO] Step 156600/210000; acc:  56.02; ppl:  7.05; xent: 1.95; lr: 0.00000505;   0/924 tok/s; 259270 sec
[2020-04-01 19:41:49,970 INFO] Loading train dataset from ../bert_data/cnndm.train.9000.bert.pt, number of examples: 1984
[2020-04-01 19:42:00,035 INFO] Step 156650/210000; acc:  50.30; ppl:  8.43; xent: 2.13; lr: 0.00000505;   0/467 tok/s; 259355 sec
[2020-04-01 19:43:22,910 INFO] Step 156700/210000; acc:  44.42; ppl: 12.58; xent: 2.53; lr: 0.00000505;   0/1206 tok/s; 259438 sec
[2020-04-01 19:44:26,181 INFO] Loading train dataset from ../bert_data/cnndm.train.9001.bert.pt, number of examples: 1983
[2020-04-01 19:44:45,969 INFO] Step 156750/210000; acc:  52.37; ppl:  8.02; xent: 2.08; lr: 0.00000505;   0/633 tok/s; 259521 sec
[2020-04-01 19:46:08,682 INFO] Step 156800/210000; acc:  44.87; ppl: 12.85; xent: 2.55; lr: 0.00000505;   0/1201 tok/s; 259604 sec
[2020-04-01 19:47:03,562 INFO] Loading train dataset from ../bert_data/cnndm.train.9002.bert.pt, number of examples: 1990
[2020-04-01 19:47:31,995 INFO] Step 156850/210000; acc:  53.08; ppl:  7.43; xent: 2.01; lr: 0.00000505;   0/639 tok/s; 259687 sec
[2020-04-01 19:48:54,468 INFO] Step 156900/210000; acc:  44.00; ppl: 13.15; xent: 2.58; lr: 0.00000505;   0/1213 tok/s; 259769 sec
[2020-04-01 19:49:41,139 INFO] Loading train dataset from ../bert_data/cnndm.train.9003.bert.pt, number of examples: 1985
[2020-04-01 19:50:17,934 INFO] Step 156950/210000; acc:  47.43; ppl:  9.42; xent: 2.24; lr: 0.00000505;   0/716 tok/s; 259853 sec
[2020-04-01 19:51:40,643 INFO] Step 157000/210000; acc:  44.37; ppl: 13.10; xent: 2.57; lr: 0.00000505;   0/1153 tok/s; 259935 sec
[2020-04-01 19:51:40,647 INFO] Saving checkpoint ../models/model_step_157000.pt
[2020-04-01 19:52:21,776 INFO] Loading train dataset from ../bert_data/cnndm.train.9004.bert.pt, number of examples: 1981
[2020-04-01 19:53:06,172 INFO] Step 157050/210000; acc:  52.68; ppl:  7.95; xent: 2.07; lr: 0.00000505;   0/763 tok/s; 260021 sec
[2020-04-01 19:54:29,546 INFO] Step 157100/210000; acc:  47.78; ppl:  9.91; xent: 2.29; lr: 0.00000505;   0/1224 tok/s; 260104 sec
[2020-04-01 19:55:00,130 INFO] Loading train dataset from ../bert_data/cnndm.train.9005.bert.pt, number of examples: 1986
[2020-04-01 19:55:52,801 INFO] Step 157150/210000; acc:  50.27; ppl:  8.57; xent: 2.15; lr: 0.00000505;   0/914 tok/s; 260188 sec
[2020-04-01 19:57:16,346 INFO] Step 157200/210000; acc:  45.88; ppl: 12.51; xent: 2.53; lr: 0.00000504;   0/888 tok/s; 260271 sec
[2020-04-01 19:57:38,256 INFO] Loading train dataset from ../bert_data/cnndm.train.9006.bert.pt, number of examples: 1993
[2020-04-01 19:58:39,558 INFO] Step 157250/210000; acc:  47.87; ppl:  9.89; xent: 2.29; lr: 0.00000504;   0/1104 tok/s; 260354 sec
[2020-04-01 20:00:03,328 INFO] Step 157300/210000; acc:  54.11; ppl:  7.11; xent: 1.96; lr: 0.00000504;   0/567 tok/s; 260438 sec
[2020-04-01 20:00:17,253 INFO] Loading train dataset from ../bert_data/cnndm.train.9007.bert.pt, number of examples: 1983
[2020-04-01 20:01:26,934 INFO] Step 157350/210000; acc:  48.05; ppl:  9.85; xent: 2.29; lr: 0.00000504;   0/1002 tok/s; 260522 sec
[2020-04-01 20:02:50,060 INFO] Step 157400/210000; acc:  56.04; ppl:  7.43; xent: 2.01; lr: 0.00000504;   0/441 tok/s; 260605 sec
[2020-04-01 20:02:56,287 INFO] Loading train dataset from ../bert_data/cnndm.train.9008.bert.pt, number of examples: 1990
[2020-04-01 20:04:13,926 INFO] Step 157450/210000; acc:  49.33; ppl:  9.89; xent: 2.29; lr: 0.00000504;   0/1323 tok/s; 260689 sec
[2020-04-01 20:05:34,793 INFO] Loading train dataset from ../bert_data/cnndm.train.9009.bert.pt, number of examples: 1988
[2020-04-01 20:05:38,070 INFO] Step 157500/210000; acc:  52.71; ppl:  8.78; xent: 2.17; lr: 0.00000504;   0/676 tok/s; 260773 sec
[2020-04-01 20:05:38,072 INFO] Saving checkpoint ../models/model_step_157500.pt
[2020-04-01 20:07:04,033 INFO] Step 157550/210000; acc:  47.38; ppl:  9.64; xent: 2.27; lr: 0.00000504;   0/1228 tok/s; 260859 sec
[2020-04-01 20:08:15,198 INFO] Loading train dataset from ../bert_data/cnndm.train.9010.bert.pt, number of examples: 1984
[2020-04-01 20:08:28,257 INFO] Step 157600/210000; acc:  49.39; ppl:  9.65; xent: 2.27; lr: 0.00000504;   0/767 tok/s; 260943 sec
[2020-04-01 20:09:51,736 INFO] Step 157650/210000; acc:  48.97; ppl: 10.96; xent: 2.39; lr: 0.00000504;   0/971 tok/s; 261027 sec
[2020-04-01 20:10:53,243 INFO] Loading train dataset from ../bert_data/cnndm.train.9011.bert.pt, number of examples: 1983
[2020-04-01 20:11:14,881 INFO] Step 157700/210000; acc:  48.58; ppl: 10.67; xent: 2.37; lr: 0.00000504;   0/923 tok/s; 261110 sec
[2020-04-01 20:12:38,251 INFO] Step 157750/210000; acc:  47.82; ppl: 10.93; xent: 2.39; lr: 0.00000504;   0/956 tok/s; 261193 sec
[2020-04-01 20:13:31,378 INFO] Loading train dataset from ../bert_data/cnndm.train.9012.bert.pt, number of examples: 1985
[2020-04-01 20:14:00,691 INFO] Step 157800/210000; acc:  48.26; ppl: 10.22; xent: 2.32; lr: 0.00000503;   0/1002 tok/s; 261276 sec
[2020-04-01 20:15:23,779 INFO] Step 157850/210000; acc:  50.34; ppl:  8.82; xent: 2.18; lr: 0.00000503;   0/723 tok/s; 261359 sec
[2020-04-01 20:16:09,223 INFO] Loading train dataset from ../bert_data/cnndm.train.9013.bert.pt, number of examples: 1987
[2020-04-01 20:16:47,559 INFO] Step 157900/210000; acc:  48.21; ppl: 10.53; xent: 2.35; lr: 0.00000503;   0/1198 tok/s; 261442 sec
[2020-04-01 20:18:10,250 INFO] Step 157950/210000; acc:  59.15; ppl:  5.43; xent: 1.69; lr: 0.00000503;   0/447 tok/s; 261525 sec
[2020-04-01 20:18:46,940 INFO] Loading train dataset from ../bert_data/cnndm.train.9014.bert.pt, number of examples: 1976
[2020-04-01 20:19:33,537 INFO] Step 158000/210000; acc:  46.97; ppl: 11.15; xent: 2.41; lr: 0.00000503;   0/1211 tok/s; 261608 sec
[2020-04-01 20:19:33,540 INFO] Saving checkpoint ../models/model_step_158000.pt
[2020-04-01 20:20:59,692 INFO] Step 158050/210000; acc:  52.70; ppl:  8.27; xent: 2.11; lr: 0.00000503;   0/801 tok/s; 261695 sec
[2020-04-01 20:21:25,136 INFO] Loading train dataset from ../bert_data/cnndm.train.9015.bert.pt, number of examples: 1982
[2020-04-01 20:22:23,608 INFO] Step 158100/210000; acc:  57.34; ppl:  5.58; xent: 1.72; lr: 0.00000503;   0/475 tok/s; 261778 sec
[2020-04-01 20:23:45,456 INFO] Step 158150/210000; acc:  45.07; ppl: 11.97; xent: 2.48; lr: 0.00000503;   0/1105 tok/s; 261860 sec
[2020-04-01 20:24:03,043 INFO] Loading train dataset from ../bert_data/cnndm.train.9016.bert.pt, number of examples: 1984
[2020-04-01 20:25:08,604 INFO] Step 158200/210000; acc:  56.02; ppl:  5.57; xent: 1.72; lr: 0.00000503;   0/452 tok/s; 261943 sec
[2020-04-01 20:26:30,682 INFO] Step 158250/210000; acc:  49.56; ppl:  8.92; xent: 2.19; lr: 0.00000503;   0/1078 tok/s; 262026 sec
[2020-04-01 20:26:39,692 INFO] Loading train dataset from ../bert_data/cnndm.train.9017.bert.pt, number of examples: 1984
[2020-04-01 20:27:54,391 INFO] Step 158300/210000; acc:  55.24; ppl:  7.13; xent: 1.96; lr: 0.00000503;   0/471 tok/s; 262109 sec
[2020-04-01 20:29:17,320 INFO] Step 158350/210000; acc:  47.46; ppl: 10.40; xent: 2.34; lr: 0.00000503;   0/997 tok/s; 262192 sec
[2020-04-01 20:29:17,621 INFO] Loading train dataset from ../bert_data/cnndm.train.9018.bert.pt, number of examples: 1989
[2020-04-01 20:30:40,374 INFO] Step 158400/210000; acc:  49.94; ppl:  8.52; xent: 2.14; lr: 0.00000503;   0/548 tok/s; 262275 sec
[2020-04-01 20:31:55,651 INFO] Loading train dataset from ../bert_data/cnndm.train.9019.bert.pt, number of examples: 1986
[2020-04-01 20:32:03,999 INFO] Step 158450/210000; acc:  42.84; ppl: 14.28; xent: 2.66; lr: 0.00000502;   0/1331 tok/s; 262359 sec
[2020-04-01 20:33:25,911 INFO] Step 158500/210000; acc:  54.66; ppl:  6.24; xent: 1.83; lr: 0.00000502;   0/578 tok/s; 262441 sec
[2020-04-01 20:33:25,914 INFO] Saving checkpoint ../models/model_step_158500.pt
[2020-04-01 20:34:35,219 INFO] Loading train dataset from ../bert_data/cnndm.train.9020.bert.pt, number of examples: 1992
[2020-04-01 20:34:51,680 INFO] Step 158550/210000; acc:  48.07; ppl: 10.33; xent: 2.34; lr: 0.00000502;   0/1260 tok/s; 262527 sec
[2020-04-01 20:36:13,206 INFO] Step 158600/210000; acc:  55.32; ppl:  6.73; xent: 1.91; lr: 0.00000502;   0/749 tok/s; 262608 sec
[2020-04-01 20:37:11,616 INFO] Loading train dataset from ../bert_data/cnndm.train.9021.bert.pt, number of examples: 1988
[2020-04-01 20:37:36,253 INFO] Step 158650/210000; acc:  46.90; ppl: 10.94; xent: 2.39; lr: 0.00000502;   0/1166 tok/s; 262691 sec
[2020-04-01 20:38:58,970 INFO] Step 158700/210000; acc:  54.06; ppl:  6.65; xent: 1.89; lr: 0.00000502;   0/681 tok/s; 262774 sec
[2020-04-01 20:39:50,642 INFO] Loading train dataset from ../bert_data/cnndm.train.9022.bert.pt, number of examples: 1991
[2020-04-01 20:40:22,279 INFO] Step 158750/210000; acc:  48.59; ppl:  9.95; xent: 2.30; lr: 0.00000502;   0/1245 tok/s; 262857 sec
[2020-04-01 20:41:44,409 INFO] Step 158800/210000; acc:  52.58; ppl:  7.18; xent: 1.97; lr: 0.00000502;   0/584 tok/s; 262939 sec
[2020-04-01 20:42:27,374 INFO] Loading train dataset from ../bert_data/cnndm.train.9023.bert.pt, number of examples: 1986
[2020-04-01 20:43:06,680 INFO] Step 158850/210000; acc:  48.56; ppl: 10.77; xent: 2.38; lr: 0.00000502;   0/1272 tok/s; 263022 sec
[2020-04-01 20:44:29,026 INFO] Step 158900/210000; acc:  57.40; ppl:  5.06; xent: 1.62; lr: 0.00000502;   0/618 tok/s; 263104 sec
[2020-04-01 20:45:04,781 INFO] Loading train dataset from ../bert_data/cnndm.train.9024.bert.pt, number of examples: 1989
[2020-04-01 20:45:52,981 INFO] Step 158950/210000; acc:  45.71; ppl: 11.60; xent: 2.45; lr: 0.00000502;   0/1157 tok/s; 263188 sec
[2020-04-01 20:47:15,574 INFO] Step 159000/210000; acc:  54.18; ppl:  6.89; xent: 1.93; lr: 0.00000502;   0/512 tok/s; 263270 sec
[2020-04-01 20:47:15,577 INFO] Saving checkpoint ../models/model_step_159000.pt
[2020-04-01 20:47:44,881 INFO] Loading train dataset from ../bert_data/cnndm.train.9025.bert.pt, number of examples: 1984
[2020-04-01 20:48:40,960 INFO] Step 159050/210000; acc:  47.89; ppl: 10.20; xent: 2.32; lr: 0.00000501;   0/1036 tok/s; 263356 sec
[2020-04-01 20:50:04,024 INFO] Step 159100/210000; acc:  48.55; ppl:  9.74; xent: 2.28; lr: 0.00000501;   0/770 tok/s; 263439 sec
[2020-04-01 20:50:23,084 INFO] Loading train dataset from ../bert_data/cnndm.train.9026.bert.pt, number of examples: 1986
[2020-04-01 20:51:28,118 INFO] Step 159150/210000; acc:  50.62; ppl:  8.44; xent: 2.13; lr: 0.00000501;   0/625 tok/s; 263523 sec
[2020-04-01 20:52:50,869 INFO] Step 159200/210000; acc:  48.30; ppl:  9.84; xent: 2.29; lr: 0.00000501;   0/1033 tok/s; 263606 sec
[2020-04-01 20:53:02,178 INFO] Loading train dataset from ../bert_data/cnndm.train.9027.bert.pt, number of examples: 1989
[2020-04-01 20:54:15,013 INFO] Step 159250/210000; acc:  52.00; ppl:  8.69; xent: 2.16; lr: 0.00000501;   0/582 tok/s; 263690 sec
[2020-04-01 20:55:37,845 INFO] Step 159300/210000; acc:  47.66; ppl:  9.59; xent: 2.26; lr: 0.00000501;   0/897 tok/s; 263773 sec
[2020-04-01 20:55:40,191 INFO] Loading train dataset from ../bert_data/cnndm.train.9028.bert.pt, number of examples: 1986
[2020-04-01 20:57:02,270 INFO] Step 159350/210000; acc:  53.14; ppl:  7.71; xent: 2.04; lr: 0.00000501;   0/543 tok/s; 263857 sec
[2020-04-01 20:58:18,721 INFO] Loading train dataset from ../bert_data/cnndm.train.9029.bert.pt, number of examples: 1981
[2020-04-01 20:58:25,245 INFO] Step 159400/210000; acc:  47.08; ppl: 10.73; xent: 2.37; lr: 0.00000501;   0/1025 tok/s; 263940 sec
[2020-04-01 20:59:49,028 INFO] Step 159450/210000; acc:  59.22; ppl:  5.26; xent: 1.66; lr: 0.00000501;   0/499 tok/s; 264024 sec
[2020-04-01 21:00:58,712 INFO] Loading train dataset from ../bert_data/cnndm.train.9030.bert.pt, number of examples: 1984
[2020-04-01 21:01:13,753 INFO] Step 159500/210000; acc:  47.68; ppl: 10.61; xent: 2.36; lr: 0.00000501;   0/1269 tok/s; 264109 sec
[2020-04-01 21:01:13,756 INFO] Saving checkpoint ../models/model_step_159500.pt
[2020-04-01 21:02:38,362 INFO] Step 159550/210000; acc:  53.56; ppl:  7.57; xent: 2.02; lr: 0.00000501;   0/609 tok/s; 264193 sec
[2020-04-01 21:03:36,978 INFO] Loading train dataset from ../bert_data/cnndm.train.9031.bert.pt, number of examples: 1975
[2020-04-01 21:04:01,675 INFO] Step 159600/210000; acc:  47.34; ppl: 10.38; xent: 2.34; lr: 0.00000501;   0/1207 tok/s; 264276 sec
[2020-04-01 21:05:25,540 INFO] Step 159650/210000; acc:  54.47; ppl:  7.37; xent: 2.00; lr: 0.00000501;   0/882 tok/s; 264360 sec
[2020-04-01 21:06:15,100 INFO] Loading train dataset from ../bert_data/cnndm.train.9032.bert.pt, number of examples: 1986
[2020-04-01 21:06:49,500 INFO] Step 159700/210000; acc:  58.56; ppl:  5.16; xent: 1.64; lr: 0.00000500;   0/507 tok/s; 264444 sec
[2020-04-01 21:08:12,580 INFO] Step 159750/210000; acc:  48.11; ppl: 10.21; xent: 2.32; lr: 0.00000500;   0/1136 tok/s; 264527 sec
[2020-04-01 21:08:53,164 INFO] Loading train dataset from ../bert_data/cnndm.train.9033.bert.pt, number of examples: 1986
[2020-04-01 21:09:35,981 INFO] Step 159800/210000; acc:  52.38; ppl:  7.25; xent: 1.98; lr: 0.00000500;   0/601 tok/s; 264611 sec
[2020-04-01 21:10:57,891 INFO] Step 159850/210000; acc:  48.59; ppl: 10.80; xent: 2.38; lr: 0.00000500;   0/1072 tok/s; 264693 sec
[2020-04-01 21:11:30,495 INFO] Loading train dataset from ../bert_data/cnndm.train.9034.bert.pt, number of examples: 1989
[2020-04-01 21:12:21,252 INFO] Step 159900/210000; acc:  50.45; ppl:  8.72; xent: 2.17; lr: 0.00000500;   0/744 tok/s; 264776 sec
[2020-04-01 21:13:44,933 INFO] Step 159950/210000; acc:  43.01; ppl: 13.61; xent: 2.61; lr: 0.00000500;   0/1247 tok/s; 264860 sec
[2020-04-01 21:14:08,601 INFO] Loading train dataset from ../bert_data/cnndm.train.9035.bert.pt, number of examples: 1988
[2020-04-01 21:15:07,615 INFO] Step 160000/210000; acc:  53.83; ppl:  6.80; xent: 1.92; lr: 0.00000500;   0/797 tok/s; 264942 sec
[2020-04-01 21:15:07,619 INFO] Saving checkpoint ../models/model_step_160000.pt
[2020-04-01 21:16:31,616 INFO] Step 160050/210000; acc:  45.97; ppl: 11.75; xent: 2.46; lr: 0.00000500;   0/1243 tok/s; 265026 sec
[2020-04-01 21:16:47,294 INFO] Loading train dataset from ../bert_data/cnndm.train.9036.bert.pt, number of examples: 1984
[2020-04-01 21:17:55,585 INFO] Step 160100/210000; acc:  48.43; ppl:  9.92; xent: 2.29; lr: 0.00000500;   0/924 tok/s; 265110 sec
[2020-04-01 21:19:18,796 INFO] Step 160150/210000; acc:  52.88; ppl:  6.96; xent: 1.94; lr: 0.00000500;   0/514 tok/s; 265194 sec
[2020-04-01 21:19:22,593 INFO] Loading train dataset from ../bert_data/cnndm.train.9037.bert.pt, number of examples: 1983
[2020-04-01 21:20:41,962 INFO] Step 160200/210000; acc:  44.87; ppl: 13.58; xent: 2.61; lr: 0.00000500;   0/1252 tok/s; 265277 sec
[2020-04-01 21:21:59,858 INFO] Loading train dataset from ../bert_data/cnndm.train.9038.bert.pt, number of examples: 1982
[2020-04-01 21:22:04,751 INFO] Step 160250/210000; acc:  53.65; ppl:  8.19; xent: 2.10; lr: 0.00000500;   0/760 tok/s; 265360 sec
[2020-04-01 21:23:28,069 INFO] Step 160300/210000; acc:  43.36; ppl: 12.60; xent: 2.53; lr: 0.00000500;   0/1054 tok/s; 265443 sec
[2020-04-01 21:24:37,909 INFO] Loading train dataset from ../bert_data/cnndm.train.9039.bert.pt, number of examples: 1983
[2020-04-01 21:24:51,306 INFO] Step 160350/210000; acc:  48.20; ppl: 10.31; xent: 2.33; lr: 0.00000499;   0/959 tok/s; 265526 sec
[2020-04-01 21:26:12,790 INFO] Step 160400/210000; acc:  45.29; ppl: 11.62; xent: 2.45; lr: 0.00000499;   0/775 tok/s; 265608 sec
[2020-04-01 21:27:12,757 INFO] Loading train dataset from ../bert_data/cnndm.train.9040.bert.pt, number of examples: 1982
[2020-04-01 21:27:36,667 INFO] Step 160450/210000; acc:  47.06; ppl:  9.96; xent: 2.30; lr: 0.00000499;   0/1008 tok/s; 265691 sec
[2020-04-01 21:28:59,987 INFO] Step 160500/210000; acc:  58.51; ppl:  5.36; xent: 1.68; lr: 0.00000499;   0/433 tok/s; 265775 sec
[2020-04-01 21:28:59,990 INFO] Saving checkpoint ../models/model_step_160500.pt
[2020-04-01 21:29:52,456 INFO] Loading train dataset from ../bert_data/cnndm.train.9041.bert.pt, number of examples: 1986
[2020-04-01 21:30:25,962 INFO] Step 160550/210000; acc:  47.44; ppl: 10.40; xent: 2.34; lr: 0.00000499;   0/1251 tok/s; 265861 sec
[2020-04-01 21:31:48,405 INFO] Step 160600/210000; acc:  54.94; ppl:  7.53; xent: 2.02; lr: 0.00000499;   0/618 tok/s; 265943 sec
[2020-04-01 21:32:31,634 INFO] Loading train dataset from ../bert_data/cnndm.train.9042.bert.pt, number of examples: 1984
[2020-04-01 21:33:11,092 INFO] Step 160650/210000; acc:  44.67; ppl: 13.38; xent: 2.59; lr: 0.00000499;   0/1195 tok/s; 266026 sec
[2020-04-01 21:34:32,706 INFO] Step 160700/210000; acc:  55.59; ppl:  6.68; xent: 1.90; lr: 0.00000499;   0/603 tok/s; 266108 sec
[2020-04-01 21:35:07,761 INFO] Loading train dataset from ../bert_data/cnndm.train.9043.bert.pt, number of examples: 1984
[2020-04-01 21:35:55,853 INFO] Step 160750/210000; acc:  43.86; ppl: 13.51; xent: 2.60; lr: 0.00000499;   0/1258 tok/s; 266191 sec
[2020-04-01 21:37:19,242 INFO] Step 160800/210000; acc:  50.70; ppl:  8.13; xent: 2.10; lr: 0.00000499;   0/863 tok/s; 266274 sec
[2020-04-01 21:37:46,771 INFO] Loading train dataset from ../bert_data/cnndm.train.9044.bert.pt, number of examples: 1988
[2020-04-01 21:38:43,577 INFO] Step 160850/210000; acc:  49.91; ppl:  7.75; xent: 2.05; lr: 0.00000499;   0/662 tok/s; 266358 sec
[2020-04-01 21:40:05,823 INFO] Step 160900/210000; acc:  50.04; ppl:  8.79; xent: 2.17; lr: 0.00000499;   0/804 tok/s; 266441 sec
[2020-04-01 21:40:23,726 INFO] Loading train dataset from ../bert_data/cnndm.train.9045.bert.pt, number of examples: 1983
[2020-04-01 21:41:29,635 INFO] Step 160950/210000; acc:  59.42; ppl:  5.25; xent: 1.66; lr: 0.00000499;   0/411 tok/s; 266524 sec
[2020-04-01 21:42:52,704 INFO] Step 161000/210000; acc:  47.54; ppl: 11.10; xent: 2.41; lr: 0.00000498;   0/1135 tok/s; 266608 sec
[2020-04-01 21:42:52,708 INFO] Saving checkpoint ../models/model_step_161000.pt
[2020-04-01 21:43:03,838 INFO] Loading train dataset from ../bert_data/cnndm.train.9046.bert.pt, number of examples: 1983
[2020-04-01 21:44:19,055 INFO] Step 161050/210000; acc:  59.09; ppl:  5.49; xent: 1.70; lr: 0.00000498;   0/567 tok/s; 266694 sec
[2020-04-01 21:45:40,924 INFO] Loading train dataset from ../bert_data/cnndm.train.9047.bert.pt, number of examples: 1983
[2020-04-01 21:45:42,896 INFO] Step 161100/210000; acc:  48.72; ppl:  9.53; xent: 2.25; lr: 0.00000498;   0/755 tok/s; 266778 sec
[2020-04-01 21:47:05,482 INFO] Step 161150/210000; acc:  57.17; ppl:  6.16; xent: 1.82; lr: 0.00000498;   0/628 tok/s; 266860 sec
[2020-04-01 21:48:19,298 INFO] Loading train dataset from ../bert_data/cnndm.train.9048.bert.pt, number of examples: 1984
[2020-04-01 21:48:29,341 INFO] Step 161200/210000; acc:  47.97; ppl: 10.11; xent: 2.31; lr: 0.00000498;   0/926 tok/s; 266944 sec
[2020-04-01 21:49:52,073 INFO] Step 161250/210000; acc:  58.23; ppl:  6.02; xent: 1.80; lr: 0.00000498;   0/791 tok/s; 267027 sec
[2020-04-01 21:50:57,227 INFO] Loading train dataset from ../bert_data/cnndm.train.9049.bert.pt, number of examples: 1978
[2020-04-01 21:51:15,942 INFO] Step 161300/210000; acc:  53.10; ppl:  7.84; xent: 2.06; lr: 0.00000498;   0/498 tok/s; 267111 sec
[2020-04-01 21:52:38,920 INFO] Step 161350/210000; acc:  46.76; ppl: 10.07; xent: 2.31; lr: 0.00000498;   0/1023 tok/s; 267194 sec
[2020-04-01 21:53:35,825 INFO] Loading train dataset from ../bert_data/cnndm.train.9050.bert.pt, number of examples: 1978
[2020-04-01 21:54:01,653 INFO] Step 161400/210000; acc:  53.68; ppl:  7.65; xent: 2.03; lr: 0.00000498;   0/491 tok/s; 267276 sec
[2020-04-01 21:55:24,451 INFO] Step 161450/210000; acc:  44.95; ppl: 12.31; xent: 2.51; lr: 0.00000498;   0/1241 tok/s; 267359 sec
[2020-04-01 21:56:11,642 INFO] Loading train dataset from ../bert_data/cnndm.train.9051.bert.pt, number of examples: 1981
[2020-04-01 21:56:48,070 INFO] Step 161500/210000; acc:  55.46; ppl:  6.19; xent: 1.82; lr: 0.00000498;   0/635 tok/s; 267443 sec
[2020-04-01 21:56:48,073 INFO] Saving checkpoint ../models/model_step_161500.pt
[2020-04-01 21:58:12,988 INFO] Step 161550/210000; acc:  46.40; ppl: 10.84; xent: 2.38; lr: 0.00000498;   0/1169 tok/s; 267528 sec
[2020-04-01 21:58:51,498 INFO] Loading train dataset from ../bert_data/cnndm.train.9052.bert.pt, number of examples: 1981
[2020-04-01 21:59:36,000 INFO] Step 161600/210000; acc:  51.66; ppl:  8.07; xent: 2.09; lr: 0.00000498;   0/994 tok/s; 267611 sec
[2020-04-01 22:00:59,800 INFO] Step 161650/210000; acc:  57.96; ppl:  5.30; xent: 1.67; lr: 0.00000497;   0/474 tok/s; 267695 sec
[2020-04-01 22:01:28,273 INFO] Loading train dataset from ../bert_data/cnndm.train.9053.bert.pt, number of examples: 1981
[2020-04-01 22:02:22,451 INFO] Step 161700/210000; acc:  46.92; ppl: 10.34; xent: 2.34; lr: 0.00000497;   0/1028 tok/s; 267777 sec
[2020-04-01 22:03:45,098 INFO] Step 161750/210000; acc:  60.60; ppl:  4.98; xent: 1.60; lr: 0.00000497;   0/478 tok/s; 267860 sec
[2020-04-01 22:04:03,800 INFO] Loading train dataset from ../bert_data/cnndm.train.9054.bert.pt, number of examples: 1987
[2020-04-01 22:05:08,439 INFO] Step 161800/210000; acc:  49.36; ppl:  8.79; xent: 2.17; lr: 0.00000497;   0/1178 tok/s; 267943 sec
[2020-04-01 22:06:31,612 INFO] Step 161850/210000; acc:  56.26; ppl:  6.80; xent: 1.92; lr: 0.00000497;   0/659 tok/s; 268026 sec
[2020-04-01 22:06:43,866 INFO] Loading train dataset from ../bert_data/cnndm.train.9055.bert.pt, number of examples: 1984
[2020-04-01 22:07:55,656 INFO] Step 161900/210000; acc:  46.16; ppl: 11.50; xent: 2.44; lr: 0.00000497;   0/1070 tok/s; 268110 sec
[2020-04-01 22:09:18,798 INFO] Step 161950/210000; acc:  53.56; ppl:  7.36; xent: 2.00; lr: 0.00000497;   0/896 tok/s; 268194 sec
[2020-04-01 22:09:21,103 INFO] Loading train dataset from ../bert_data/cnndm.train.9056.bert.pt, number of examples: 1985
[2020-04-01 22:10:41,973 INFO] Step 162000/210000; acc:  50.60; ppl:  8.45; xent: 2.13; lr: 0.00000497;   0/730 tok/s; 268277 sec
[2020-04-01 22:10:41,977 INFO] Saving checkpoint ../models/model_step_162000.pt
[2020-04-01 22:12:00,782 INFO] Loading train dataset from ../bert_data/cnndm.train.9057.bert.pt, number of examples: 1986
[2020-04-01 22:12:07,374 INFO] Step 162050/210000; acc:  49.69; ppl:  9.08; xent: 2.21; lr: 0.00000497;   0/1173 tok/s; 268362 sec
[2020-04-01 22:13:30,463 INFO] Step 162100/210000; acc:  55.31; ppl:  5.99; xent: 1.79; lr: 0.00000497;   0/436 tok/s; 268445 sec
[2020-04-01 22:14:37,376 INFO] Loading train dataset from ../bert_data/cnndm.train.9058.bert.pt, number of examples: 1979
[2020-04-01 22:14:54,451 INFO] Step 162150/210000; acc:  48.24; ppl: 10.14; xent: 2.32; lr: 0.00000497;   0/1128 tok/s; 268529 sec
[2020-04-01 22:16:16,802 INFO] Step 162200/210000; acc:  58.48; ppl:  5.40; xent: 1.69; lr: 0.00000497;   0/575 tok/s; 268612 sec
[2020-04-01 22:17:15,215 INFO] Loading train dataset from ../bert_data/cnndm.train.9059.bert.pt, number of examples: 1987
[2020-04-01 22:17:40,435 INFO] Step 162250/210000; acc:  47.09; ppl: 11.04; xent: 2.40; lr: 0.00000497;   0/719 tok/s; 268695 sec
[2020-04-01 22:19:02,706 INFO] Step 162300/210000; acc:  49.23; ppl:  9.03; xent: 2.20; lr: 0.00000496;   0/911 tok/s; 268778 sec
[2020-04-01 22:19:53,563 INFO] Loading train dataset from ../bert_data/cnndm.train.9060.bert.pt, number of examples: 1992
[2020-04-01 22:20:26,573 INFO] Step 162350/210000; acc:  57.75; ppl:  6.59; xent: 1.89; lr: 0.00000496;   0/416 tok/s; 268861 sec
[2020-04-01 22:21:49,075 INFO] Step 162400/210000; acc:  52.22; ppl:  7.51; xent: 2.02; lr: 0.00000496;   0/1013 tok/s; 268944 sec
[2020-04-01 22:22:31,497 INFO] Loading train dataset from ../bert_data/cnndm.train.9061.bert.pt, number of examples: 1985
[2020-04-01 22:23:13,126 INFO] Step 162450/210000; acc:  57.19; ppl:  5.93; xent: 1.78; lr: 0.00000496;   0/541 tok/s; 269028 sec
[2020-04-01 22:24:35,496 INFO] Step 162500/210000; acc:  50.65; ppl:  8.01; xent: 2.08; lr: 0.00000496;   0/796 tok/s; 269110 sec
[2020-04-01 22:24:35,500 INFO] Saving checkpoint ../models/model_step_162500.pt
[2020-04-01 22:25:11,905 INFO] Loading train dataset from ../bert_data/cnndm.train.9062.bert.pt, number of examples: 1982
[2020-04-01 22:26:01,243 INFO] Step 162550/210000; acc:  54.28; ppl:  7.67; xent: 2.04; lr: 0.00000496;   0/734 tok/s; 269196 sec
[2020-04-01 22:27:23,579 INFO] Step 162600/210000; acc:  55.13; ppl:  6.42; xent: 1.86; lr: 0.00000496;   0/716 tok/s; 269278 sec
[2020-04-01 22:27:48,839 INFO] Loading train dataset from ../bert_data/cnndm.train.9063.bert.pt, number of examples: 1988
[2020-04-01 22:28:46,865 INFO] Step 162650/210000; acc:  53.15; ppl:  7.64; xent: 2.03; lr: 0.00000496;   0/571 tok/s; 269362 sec
[2020-04-01 22:30:10,239 INFO] Step 162700/210000; acc:  48.09; ppl: 10.51; xent: 2.35; lr: 0.00000496;   0/1149 tok/s; 269445 sec
[2020-04-01 22:30:27,638 INFO] Loading train dataset from ../bert_data/cnndm.train.9064.bert.pt, number of examples: 1984
[2020-04-01 22:31:33,277 INFO] Step 162750/210000; acc:  60.17; ppl:  5.29; xent: 1.67; lr: 0.00000496;   0/584 tok/s; 269528 sec
[2020-04-01 22:32:55,783 INFO] Step 162800/210000; acc:  47.59; ppl: 10.98; xent: 2.40; lr: 0.00000496;   0/1082 tok/s; 269611 sec
[2020-04-01 22:33:04,515 INFO] Loading train dataset from ../bert_data/cnndm.train.9065.bert.pt, number of examples: 1986
[2020-04-01 22:34:19,451 INFO] Step 162850/210000; acc:  53.89; ppl:  6.07; xent: 1.80; lr: 0.00000496;   0/554 tok/s; 269694 sec
[2020-04-01 22:35:41,612 INFO] Step 162900/210000; acc:  48.74; ppl: 10.70; xent: 2.37; lr: 0.00000496;   0/967 tok/s; 269776 sec
[2020-04-01 22:35:41,917 INFO] Loading train dataset from ../bert_data/cnndm.train.9066.bert.pt, number of examples: 1988
[2020-04-01 22:37:05,325 INFO] Step 162950/210000; acc:  52.95; ppl:  7.38; xent: 2.00; lr: 0.00000495;   0/616 tok/s; 269860 sec
[2020-04-01 22:38:19,825 INFO] Loading train dataset from ../bert_data/cnndm.train.9067.bert.pt, number of examples: 1986
[2020-04-01 22:38:27,712 INFO] Step 163000/210000; acc:  46.48; ppl: 11.60; xent: 2.45; lr: 0.00000495;   0/1299 tok/s; 269943 sec
[2020-04-01 22:38:27,714 INFO] Saving checkpoint ../models/model_step_163000.pt
[2020-04-01 22:39:52,742 INFO] Step 163050/210000; acc:  53.67; ppl:  6.85; xent: 1.92; lr: 0.00000495;   0/710 tok/s; 270028 sec
[2020-04-01 22:40:59,266 INFO] Loading train dataset from ../bert_data/cnndm.train.9068.bert.pt, number of examples: 1988
[2020-04-01 22:41:15,863 INFO] Step 163100/210000; acc:  49.77; ppl:  9.37; xent: 2.24; lr: 0.00000495;   0/1006 tok/s; 270111 sec
[2020-04-01 22:42:38,760 INFO] Step 163150/210000; acc:  57.37; ppl:  6.52; xent: 1.87; lr: 0.00000495;   0/674 tok/s; 270194 sec
[2020-04-01 22:43:37,634 INFO] Loading train dataset from ../bert_data/cnndm.train.9069.bert.pt, number of examples: 1982
[2020-04-01 22:44:02,271 INFO] Step 163200/210000; acc:  52.93; ppl:  7.80; xent: 2.05; lr: 0.00000495;   0/719 tok/s; 270277 sec
[2020-04-01 22:45:24,549 INFO] Step 163250/210000; acc:  48.02; ppl: 10.08; xent: 2.31; lr: 0.00000495;   0/1046 tok/s; 270359 sec
[2020-04-01 22:46:13,137 INFO] Loading train dataset from ../bert_data/cnndm.train.9070.bert.pt, number of examples: 1988
[2020-04-01 22:46:48,058 INFO] Step 163300/210000; acc:  60.97; ppl:  4.80; xent: 1.57; lr: 0.00000495;   0/517 tok/s; 270443 sec
[2020-04-01 22:48:10,300 INFO] Step 163350/210000; acc:  49.63; ppl:  9.08; xent: 2.21; lr: 0.00000495;   0/1173 tok/s; 270525 sec
[2020-04-01 22:48:50,401 INFO] Loading train dataset from ../bert_data/cnndm.train.9071.bert.pt, number of examples: 1988
[2020-04-01 22:49:33,200 INFO] Step 163400/210000; acc:  60.85; ppl:  5.33; xent: 1.67; lr: 0.00000495;   0/623 tok/s; 270608 sec
[2020-04-01 22:50:56,387 INFO] Step 163450/210000; acc:  50.37; ppl:  8.18; xent: 2.10; lr: 0.00000495;   0/1055 tok/s; 270691 sec
[2020-04-01 22:51:28,206 INFO] Loading train dataset from ../bert_data/cnndm.train.9072.bert.pt, number of examples: 1992
[2020-04-01 22:52:19,200 INFO] Step 163500/210000; acc:  55.57; ppl:  6.80; xent: 1.92; lr: 0.00000495;   0/563 tok/s; 270774 sec
[2020-04-01 22:52:19,203 INFO] Saving checkpoint ../models/model_step_163500.pt
[2020-04-01 22:53:44,734 INFO] Step 163550/210000; acc:  48.02; ppl: 12.19; xent: 2.50; lr: 0.00000495;   0/1232 tok/s; 270860 sec
[2020-04-01 22:54:08,754 INFO] Loading train dataset from ../bert_data/cnndm.train.9073.bert.pt, number of examples: 1987
[2020-04-01 22:55:08,028 INFO] Step 163600/210000; acc:  53.39; ppl:  7.50; xent: 2.01; lr: 0.00000494;   0/706 tok/s; 270943 sec
[2020-04-01 22:56:31,246 INFO] Step 163650/210000; acc:  49.94; ppl:  9.56; xent: 2.26; lr: 0.00000494;   0/1076 tok/s; 271026 sec
[2020-04-01 22:56:46,681 INFO] Loading train dataset from ../bert_data/cnndm.train.9074.bert.pt, number of examples: 1991
[2020-04-01 22:57:54,850 INFO] Step 163700/210000; acc:  54.48; ppl:  6.74; xent: 1.91; lr: 0.00000494;   0/708 tok/s; 271110 sec
[2020-04-01 22:59:17,822 INFO] Step 163750/210000; acc:  44.28; ppl: 13.34; xent: 2.59; lr: 0.00000494;   0/1015 tok/s; 271193 sec
[2020-04-01 22:59:24,763 INFO] Loading train dataset from ../bert_data/cnndm.train.9075.bert.pt, number of examples: 1983
[2020-04-01 23:00:40,437 INFO] Step 163800/210000; acc:  48.98; ppl:  9.05; xent: 2.20; lr: 0.00000494;   0/932 tok/s; 271275 sec
[2020-04-01 23:02:02,156 INFO] Loading train dataset from ../bert_data/cnndm.train.9076.bert.pt, number of examples: 1986
[2020-04-01 23:02:03,719 INFO] Step 163850/210000; acc:  62.06; ppl:  4.68; xent: 1.54; lr: 0.00000494;   0/366 tok/s; 271359 sec
[2020-04-01 23:03:25,875 INFO] Step 163900/210000; acc:  52.80; ppl:  7.46; xent: 2.01; lr: 0.00000494;   0/990 tok/s; 271441 sec
[2020-04-01 23:04:37,876 INFO] Loading train dataset from ../bert_data/cnndm.train.9077.bert.pt, number of examples: 1990
[2020-04-01 23:04:49,829 INFO] Step 163950/210000; acc:  58.96; ppl:  5.38; xent: 1.68; lr: 0.00000494;   0/496 tok/s; 271525 sec
[2020-04-01 23:06:12,739 INFO] Step 164000/210000; acc:  46.27; ppl: 11.22; xent: 2.42; lr: 0.00000494;   0/1160 tok/s; 271608 sec
[2020-04-01 23:06:12,743 INFO] Saving checkpoint ../models/model_step_164000.pt
[2020-04-01 23:07:18,409 INFO] Loading train dataset from ../bert_data/cnndm.train.9078.bert.pt, number of examples: 1992
[2020-04-01 23:07:37,869 INFO] Step 164050/210000; acc:  52.69; ppl:  7.51; xent: 2.02; lr: 0.00000494;   0/621 tok/s; 271693 sec
[2020-04-01 23:09:00,440 INFO] Step 164100/210000; acc:  46.99; ppl: 10.59; xent: 2.36; lr: 0.00000494;   0/1184 tok/s; 271775 sec
[2020-04-01 23:09:57,508 INFO] Loading train dataset from ../bert_data/cnndm.train.9079.bert.pt, number of examples: 1984
[2020-04-01 23:10:23,951 INFO] Step 164150/210000; acc:  55.47; ppl:  6.28; xent: 1.84; lr: 0.00000494;   0/504 tok/s; 271859 sec
[2020-04-01 23:11:46,460 INFO] Step 164200/210000; acc:  50.50; ppl:  9.18; xent: 2.22; lr: 0.00000494;   0/1148 tok/s; 271941 sec
[2020-04-01 23:12:34,486 INFO] Loading train dataset from ../bert_data/cnndm.train.9080.bert.pt, number of examples: 1988
[2020-04-01 23:13:09,819 INFO] Step 164250/210000; acc:  58.46; ppl:  5.40; xent: 1.69; lr: 0.00000493;   0/624 tok/s; 272025 sec
[2020-04-01 23:14:32,536 INFO] Step 164300/210000; acc:  47.79; ppl: 10.39; xent: 2.34; lr: 0.00000493;   0/1313 tok/s; 272107 sec
[2020-04-01 23:15:11,434 INFO] Loading train dataset from ../bert_data/cnndm.train.9081.bert.pt, number of examples: 1991
[2020-04-01 23:15:55,854 INFO] Step 164350/210000; acc:  55.51; ppl:  6.72; xent: 1.90; lr: 0.00000493;   0/724 tok/s; 272191 sec
[2020-04-01 23:17:19,542 INFO] Step 164400/210000; acc:  46.30; ppl: 11.69; xent: 2.46; lr: 0.00000493;   0/1250 tok/s; 272274 sec
[2020-04-01 23:17:49,756 INFO] Loading train dataset from ../bert_data/cnndm.train.9082.bert.pt, number of examples: 1985
[2020-04-01 23:18:42,317 INFO] Step 164450/210000; acc:  51.43; ppl:  8.73; xent: 2.17; lr: 0.00000493;   0/816 tok/s; 272357 sec
[2020-04-01 23:20:05,036 INFO] Step 164500/210000; acc:  47.21; ppl: 11.54; xent: 2.45; lr: 0.00000493;   0/1022 tok/s; 272440 sec
[2020-04-01 23:20:05,040 INFO] Saving checkpoint ../models/model_step_164500.pt
[2020-04-01 23:20:29,063 INFO] Loading train dataset from ../bert_data/cnndm.train.9083.bert.pt, number of examples: 1986
[2020-04-01 23:21:29,954 INFO] Step 164550/210000; acc:  51.25; ppl:  7.73; xent: 2.04; lr: 0.00000493;   0/763 tok/s; 272525 sec
[2020-04-01 23:22:53,591 INFO] Step 164600/210000; acc:  51.60; ppl:  8.42; xent: 2.13; lr: 0.00000493;   0/859 tok/s; 272608 sec
[2020-04-01 23:23:06,898 INFO] Loading train dataset from ../bert_data/cnndm.train.9084.bert.pt, number of examples: 1982
[2020-04-01 23:24:16,776 INFO] Step 164650/210000; acc:  48.72; ppl: 10.16; xent: 2.32; lr: 0.00000493;   0/983 tok/s; 272692 sec
[2020-04-01 23:25:39,004 INFO] Step 164700/210000; acc:  46.57; ppl: 11.33; xent: 2.43; lr: 0.00000493;   0/906 tok/s; 272774 sec
[2020-04-01 23:25:44,500 INFO] Loading train dataset from ../bert_data/cnndm.train.9085.bert.pt, number of examples: 1985
[2020-04-01 23:27:02,223 INFO] Step 164750/210000; acc:  48.19; ppl: 10.11; xent: 2.31; lr: 0.00000493;   0/1161 tok/s; 272857 sec
[2020-04-01 23:28:22,449 INFO] Loading train dataset from ../bert_data/cnndm.train.9086.bert.pt, number of examples: 1983
[2020-04-01 23:28:25,911 INFO] Step 164800/210000; acc:  55.46; ppl:  6.57; xent: 1.88; lr: 0.00000493;   0/525 tok/s; 272941 sec
[2020-04-01 23:29:48,345 INFO] Step 164850/210000; acc:  47.84; ppl: 11.06; xent: 2.40; lr: 0.00000493;   0/1124 tok/s; 273023 sec
[2020-04-01 23:31:00,382 INFO] Loading train dataset from ../bert_data/cnndm.train.9087.bert.pt, number of examples: 1986
[2020-04-01 23:31:11,987 INFO] Step 164900/210000; acc:  51.26; ppl:  8.51; xent: 2.14; lr: 0.00000493;   0/601 tok/s; 273107 sec
[2020-04-01 23:32:34,984 INFO] Step 164950/210000; acc:  46.65; ppl: 10.37; xent: 2.34; lr: 0.00000492;   0/1289 tok/s; 273190 sec
[2020-04-01 23:33:37,952 INFO] Loading train dataset from ../bert_data/cnndm.train.9088.bert.pt, number of examples: 1986
[2020-04-01 23:33:57,820 INFO] Step 165000/210000; acc:  51.21; ppl:  7.52; xent: 2.02; lr: 0.00000492;   0/673 tok/s; 273273 sec
[2020-04-01 23:33:57,823 INFO] Saving checkpoint ../models/model_step_165000.pt
[2020-04-01 23:35:22,449 INFO] Step 165050/210000; acc:  49.33; ppl:  8.97; xent: 2.19; lr: 0.00000492;   0/1180 tok/s; 273357 sec
[2020-04-01 23:36:17,485 INFO] Loading train dataset from ../bert_data/cnndm.train.9089.bert.pt, number of examples: 1986
[2020-04-01 23:36:45,456 INFO] Step 165100/210000; acc:  51.46; ppl:  7.21; xent: 1.98; lr: 0.00000492;   0/647 tok/s; 273440 sec
[2020-04-01 23:38:08,084 INFO] Step 165150/210000; acc:  47.54; ppl:  9.85; xent: 2.29; lr: 0.00000492;   0/1238 tok/s; 273523 sec
[2020-04-01 23:38:55,114 INFO] Loading train dataset from ../bert_data/cnndm.train.9090.bert.pt, number of examples: 20
[2020-04-01 23:38:56,975 INFO] Loading train dataset from ../bert_data/cnndm.train.9100.bert.pt, number of examples: 1988
[2020-04-01 23:39:27,368 INFO] Step 165200/210000; acc:  64.58; ppl:  4.35; xent: 1.47; lr: 0.00000492;   0/488 tok/s; 273602 sec
[2020-04-01 23:40:39,714 INFO] Step 165250/210000; acc:  58.43; ppl:  5.59; xent: 1.72; lr: 0.00000492;   0/628 tok/s; 273675 sec
[2020-04-01 23:41:13,530 INFO] Loading train dataset from ../bert_data/cnndm.train.9101.bert.pt, number of examples: 1983
[2020-04-01 23:41:52,744 INFO] Step 165300/210000; acc:  63.95; ppl:  5.04; xent: 1.62; lr: 0.00000492;   0/580 tok/s; 273748 sec
[2020-04-01 23:43:05,128 INFO] Step 165350/210000; acc:  61.27; ppl:  4.74; xent: 1.56; lr: 0.00000492;   0/559 tok/s; 273820 sec
[2020-04-01 23:43:32,035 INFO] Loading train dataset from ../bert_data/cnndm.train.9102.bert.pt, number of examples: 1992
[2020-04-01 23:44:19,202 INFO] Step 165400/210000; acc:  71.49; ppl:  3.41; xent: 1.23; lr: 0.00000492;   0/486 tok/s; 273894 sec
[2020-04-01 23:45:33,463 INFO] Step 165450/210000; acc:  64.90; ppl:  3.92; xent: 1.37; lr: 0.00000492;   0/373 tok/s; 273968 sec
[2020-04-01 23:45:52,669 INFO] Loading train dataset from ../bert_data/cnndm.train.9103.bert.pt, number of examples: 1978
[2020-04-01 23:46:47,156 INFO] Step 165500/210000; acc:  68.56; ppl:  3.92; xent: 1.37; lr: 0.00000492;   0/383 tok/s; 274042 sec
[2020-04-01 23:46:47,159 INFO] Saving checkpoint ../models/model_step_165500.pt
[2020-04-01 23:48:02,640 INFO] Step 165550/210000; acc:  57.23; ppl:  5.52; xent: 1.71; lr: 0.00000492;   0/745 tok/s; 274117 sec
[2020-04-01 23:48:13,565 INFO] Loading train dataset from ../bert_data/cnndm.train.9104.bert.pt, number of examples: 1986
[2020-04-01 23:49:16,566 INFO] Step 165600/210000; acc:  67.92; ppl:  3.74; xent: 1.32; lr: 0.00000491;   0/581 tok/s; 274191 sec
[2020-04-01 23:50:28,999 INFO] Step 165650/210000; acc:  62.86; ppl:  4.55; xent: 1.51; lr: 0.00000491;   0/535 tok/s; 274264 sec
[2020-04-01 23:50:30,879 INFO] Loading train dataset from ../bert_data/cnndm.train.9105.bert.pt, number of examples: 1988
[2020-04-01 23:51:41,788 INFO] Step 165700/210000; acc:  67.01; ppl:  3.75; xent: 1.32; lr: 0.00000491;   0/569 tok/s; 274337 sec
[2020-04-01 23:52:49,391 INFO] Loading train dataset from ../bert_data/cnndm.train.9106.bert.pt, number of examples: 1978
[2020-04-01 23:52:55,503 INFO] Step 165750/210000; acc:  66.52; ppl:  4.26; xent: 1.45; lr: 0.00000491;   0/492 tok/s; 274410 sec
[2020-04-01 23:54:08,923 INFO] Step 165800/210000; acc:  57.14; ppl:  5.95; xent: 1.78; lr: 0.00000491;   0/641 tok/s; 274484 sec
[2020-04-01 23:55:09,292 INFO] Loading train dataset from ../bert_data/cnndm.train.9107.bert.pt, number of examples: 1985
[2020-04-01 23:55:23,279 INFO] Step 165850/210000; acc:  65.44; ppl:  4.21; xent: 1.44; lr: 0.00000491;   0/560 tok/s; 274558 sec
[2020-04-01 23:56:35,287 INFO] Step 165900/210000; acc:  66.92; ppl:  4.41; xent: 1.48; lr: 0.00000491;   0/561 tok/s; 274630 sec
[2020-04-01 23:57:27,533 INFO] Loading train dataset from ../bert_data/cnndm.train.9108.bert.pt, number of examples: 1989
[2020-04-01 23:57:47,398 INFO] Step 165950/210000; acc:  63.07; ppl:  4.51; xent: 1.51; lr: 0.00000491;   0/545 tok/s; 274702 sec
[2020-04-01 23:59:01,400 INFO] Step 166000/210000; acc:  63.97; ppl:  4.40; xent: 1.48; lr: 0.00000491;   0/600 tok/s; 274776 sec
[2020-04-01 23:59:01,404 INFO] Saving checkpoint ../models/model_step_166000.pt
[2020-04-01 23:59:47,452 INFO] Loading train dataset from ../bert_data/cnndm.train.9109.bert.pt, number of examples: 1987
[2020-04-02 00:00:16,571 INFO] Step 166050/210000; acc:  74.25; ppl:  3.09; xent: 1.13; lr: 0.00000491;   0/372 tok/s; 274851 sec
[2020-04-02 00:01:29,923 INFO] Step 166100/210000; acc:  64.29; ppl:  4.86; xent: 1.58; lr: 0.00000491;   0/651 tok/s; 274925 sec
[2020-04-02 00:02:06,870 INFO] Loading train dataset from ../bert_data/cnndm.train.9110.bert.pt, number of examples: 1985
[2020-04-02 00:02:43,077 INFO] Step 166150/210000; acc:  59.43; ppl:  5.94; xent: 1.78; lr: 0.00000491;   0/691 tok/s; 274998 sec
[2020-04-02 00:03:56,740 INFO] Step 166200/210000; acc:  60.13; ppl:  4.83; xent: 1.57; lr: 0.00000491;   0/533 tok/s; 275072 sec
[2020-04-02 00:04:26,333 INFO] Loading train dataset from ../bert_data/cnndm.train.9111.bert.pt, number of examples: 1985
[2020-04-02 00:05:09,820 INFO] Step 166250/210000; acc:  69.21; ppl:  3.67; xent: 1.30; lr: 0.00000491;   0/539 tok/s; 275145 sec
[2020-04-02 00:06:23,254 INFO] Step 166300/210000; acc:  59.72; ppl:  5.18; xent: 1.65; lr: 0.00000490;   0/628 tok/s; 275218 sec
[2020-04-02 00:06:46,212 INFO] Loading train dataset from ../bert_data/cnndm.train.9112.bert.pt, number of examples: 1983
[2020-04-02 00:07:37,248 INFO] Step 166350/210000; acc:  58.25; ppl:  5.76; xent: 1.75; lr: 0.00000490;   0/679 tok/s; 275292 sec
[2020-04-02 00:08:50,697 INFO] Step 166400/210000; acc:  76.08; ppl:  2.80; xent: 1.03; lr: 0.00000490;   0/472 tok/s; 275366 sec
[2020-04-02 00:09:04,236 INFO] Loading train dataset from ../bert_data/cnndm.train.9113.bert.pt, number of examples: 1979
[2020-04-02 00:10:03,772 INFO] Step 166450/210000; acc:  66.98; ppl:  4.03; xent: 1.39; lr: 0.00000490;   0/530 tok/s; 275439 sec
[2020-04-02 00:11:17,682 INFO] Step 166500/210000; acc:  72.99; ppl:  3.15; xent: 1.15; lr: 0.00000490;   0/439 tok/s; 275513 sec
[2020-04-02 00:11:17,684 INFO] Saving checkpoint ../models/model_step_166500.pt
[2020-04-02 00:11:26,317 INFO] Loading train dataset from ../bert_data/cnndm.train.9114.bert.pt, number of examples: 1989
[2020-04-02 00:12:34,239 INFO] Step 166550/210000; acc:  65.85; ppl:  3.64; xent: 1.29; lr: 0.00000490;   0/442 tok/s; 275589 sec
[2020-04-02 00:13:44,740 INFO] Loading train dataset from ../bert_data/cnndm.train.9115.bert.pt, number of examples: 1989
[2020-04-02 00:13:47,679 INFO] Step 166600/210000; acc:  67.68; ppl:  3.60; xent: 1.28; lr: 0.00000490;   0/423 tok/s; 275663 sec
[2020-04-02 00:14:59,905 INFO] Step 166650/210000; acc:  69.18; ppl:  3.54; xent: 1.26; lr: 0.00000490;   0/568 tok/s; 275735 sec
[2020-04-02 00:16:03,155 INFO] Loading train dataset from ../bert_data/cnndm.train.9116.bert.pt, number of examples: 1985
[2020-04-02 00:16:13,187 INFO] Step 166700/210000; acc:  59.95; ppl:  5.13; xent: 1.63; lr: 0.00000490;   0/634 tok/s; 275808 sec
[2020-04-02 00:17:27,682 INFO] Step 166750/210000; acc:  63.87; ppl:  4.61; xent: 1.53; lr: 0.00000490;   0/557 tok/s; 275883 sec
[2020-04-02 00:18:22,306 INFO] Loading train dataset from ../bert_data/cnndm.train.9117.bert.pt, number of examples: 1979
[2020-04-02 00:18:41,112 INFO] Step 166800/210000; acc:  70.23; ppl:  3.39; xent: 1.22; lr: 0.00000490;   0/508 tok/s; 275956 sec
[2020-04-02 00:19:54,121 INFO] Step 166850/210000; acc:  69.79; ppl:  3.41; xent: 1.23; lr: 0.00000490;   0/412 tok/s; 276029 sec
[2020-04-02 00:20:40,943 INFO] Loading train dataset from ../bert_data/cnndm.train.9118.bert.pt, number of examples: 1980
[2020-04-02 00:21:07,564 INFO] Step 166900/210000; acc:  63.23; ppl:  4.69; xent: 1.54; lr: 0.00000490;   0/636 tok/s; 276102 sec
[2020-04-02 00:22:21,304 INFO] Step 166950/210000; acc:  66.62; ppl:  3.93; xent: 1.37; lr: 0.00000489;   0/552 tok/s; 276176 sec
[2020-04-02 00:23:01,042 INFO] Loading train dataset from ../bert_data/cnndm.train.9119.bert.pt, number of examples: 1985
[2020-04-02 00:23:34,281 INFO] Step 167000/210000; acc:  72.21; ppl:  3.23; xent: 1.17; lr: 0.00000489;   0/476 tok/s; 276249 sec
[2020-04-02 00:23:34,291 INFO] Saving checkpoint ../models/model_step_167000.pt
[2020-04-02 00:24:49,751 INFO] Step 167050/210000; acc:  61.58; ppl:  4.67; xent: 1.54; lr: 0.00000489;   0/693 tok/s; 276325 sec
[2020-04-02 00:25:22,428 INFO] Loading train dataset from ../bert_data/cnndm.train.9120.bert.pt, number of examples: 1981
[2020-04-02 00:26:03,489 INFO] Step 167100/210000; acc:  61.45; ppl:  4.65; xent: 1.54; lr: 0.00000489;   0/595 tok/s; 276398 sec
[2020-04-02 00:27:16,502 INFO] Step 167150/210000; acc:  64.65; ppl:  4.31; xent: 1.46; lr: 0.00000489;   0/561 tok/s; 276471 sec
[2020-04-02 00:27:40,170 INFO] Loading train dataset from ../bert_data/cnndm.train.9121.bert.pt, number of examples: 1985
[2020-04-02 00:28:29,968 INFO] Step 167200/210000; acc:  65.81; ppl:  4.23; xent: 1.44; lr: 0.00000489;   0/563 tok/s; 276545 sec
[2020-04-02 00:29:44,683 INFO] Step 167250/210000; acc:  66.16; ppl:  4.34; xent: 1.47; lr: 0.00000489;   0/472 tok/s; 276620 sec
[2020-04-02 00:30:00,817 INFO] Loading train dataset from ../bert_data/cnndm.train.9122.bert.pt, number of examples: 1986
[2020-04-02 00:30:58,707 INFO] Step 167300/210000; acc:  76.17; ppl:  2.67; xent: 0.98; lr: 0.00000489;   0/399 tok/s; 276694 sec
[2020-04-02 00:32:11,320 INFO] Step 167350/210000; acc:  63.19; ppl:  4.61; xent: 1.53; lr: 0.00000489;   0/613 tok/s; 276766 sec
[2020-04-02 00:32:20,495 INFO] Loading train dataset from ../bert_data/cnndm.train.9123.bert.pt, number of examples: 1983
[2020-04-02 00:33:25,234 INFO] Step 167400/210000; acc:  60.72; ppl:  5.29; xent: 1.67; lr: 0.00000489;   0/594 tok/s; 276840 sec
[2020-04-02 00:34:38,727 INFO] Step 167450/210000; acc:  65.56; ppl:  4.09; xent: 1.41; lr: 0.00000489;   0/543 tok/s; 276914 sec
[2020-04-02 00:34:38,911 INFO] Loading train dataset from ../bert_data/cnndm.train.9124.bert.pt, number of examples: 1983
[2020-04-02 00:35:52,484 INFO] Step 167500/210000; acc:  63.04; ppl:  4.50; xent: 1.50; lr: 0.00000489;   0/600 tok/s; 276987 sec
[2020-04-02 00:35:52,487 INFO] Saving checkpoint ../models/model_step_167500.pt
[2020-04-02 00:36:59,953 INFO] Loading train dataset from ../bert_data/cnndm.train.9125.bert.pt, number of examples: 1991
[2020-04-02 00:37:07,273 INFO] Step 167550/210000; acc:  69.25; ppl:  3.28; xent: 1.19; lr: 0.00000489;   0/517 tok/s; 277062 sec
[2020-04-02 00:38:20,470 INFO] Step 167600/210000; acc:  68.04; ppl:  3.47; xent: 1.24; lr: 0.00000489;   0/440 tok/s; 277135 sec
[2020-04-02 00:39:18,544 INFO] Loading train dataset from ../bert_data/cnndm.train.9126.bert.pt, number of examples: 1985
[2020-04-02 00:39:33,318 INFO] Step 167650/210000; acc:  64.32; ppl:  4.44; xent: 1.49; lr: 0.00000488;   0/537 tok/s; 277208 sec
[2020-04-02 00:40:46,523 INFO] Step 167700/210000; acc:  71.61; ppl:  3.24; xent: 1.17; lr: 0.00000488;   0/561 tok/s; 277281 sec
[2020-04-02 00:41:36,331 INFO] Loading train dataset from ../bert_data/cnndm.train.9127.bert.pt, number of examples: 1980
[2020-04-02 00:41:59,795 INFO] Step 167750/210000; acc:  60.69; ppl:  5.68; xent: 1.74; lr: 0.00000488;   0/586 tok/s; 277355 sec
[2020-04-02 00:43:12,014 INFO] Step 167800/210000; acc:  64.32; ppl:  4.41; xent: 1.48; lr: 0.00000488;   0/597 tok/s; 277427 sec
[2020-04-02 00:43:54,138 INFO] Loading train dataset from ../bert_data/cnndm.train.9128.bert.pt, number of examples: 1981
[2020-04-02 00:44:26,647 INFO] Step 167850/210000; acc:  69.90; ppl:  3.52; xent: 1.26; lr: 0.00000488;   0/549 tok/s; 277501 sec
[2020-04-02 00:45:38,975 INFO] Step 167900/210000; acc:  75.49; ppl:  2.94; xent: 1.08; lr: 0.00000488;   0/470 tok/s; 277574 sec
[2020-04-02 00:46:12,785 INFO] Loading train dataset from ../bert_data/cnndm.train.9129.bert.pt, number of examples: 1987
[2020-04-02 00:46:52,200 INFO] Step 167950/210000; acc:  63.09; ppl:  4.56; xent: 1.52; lr: 0.00000488;   0/515 tok/s; 277647 sec
[2020-04-02 00:48:05,420 INFO] Step 168000/210000; acc:  64.45; ppl:  4.12; xent: 1.42; lr: 0.00000488;   0/469 tok/s; 277720 sec
[2020-04-02 00:48:05,423 INFO] Saving checkpoint ../models/model_step_168000.pt
[2020-04-02 00:48:34,344 INFO] Loading train dataset from ../bert_data/cnndm.train.9130.bert.pt, number of examples: 1986
[2020-04-02 00:49:21,387 INFO] Step 168050/210000; acc:  69.52; ppl:  3.37; xent: 1.21; lr: 0.00000488;   0/606 tok/s; 277796 sec
[2020-04-02 00:50:34,127 INFO] Step 168100/210000; acc:  69.66; ppl:  3.82; xent: 1.34; lr: 0.00000488;   0/488 tok/s; 277869 sec
[2020-04-02 00:50:53,322 INFO] Loading train dataset from ../bert_data/cnndm.train.9131.bert.pt, number of examples: 1984
[2020-04-02 00:51:47,885 INFO] Step 168150/210000; acc:  73.58; ppl:  2.80; xent: 1.03; lr: 0.00000488;   0/424 tok/s; 277943 sec
[2020-04-02 00:53:00,770 INFO] Step 168200/210000; acc:  67.54; ppl:  3.73; xent: 1.32; lr: 0.00000488;   0/593 tok/s; 278016 sec
[2020-04-02 00:53:11,752 INFO] Loading train dataset from ../bert_data/cnndm.train.9132.bert.pt, number of examples: 1990
[2020-04-02 00:54:14,561 INFO] Step 168250/210000; acc:  72.04; ppl:  3.07; xent: 1.12; lr: 0.00000488;   0/498 tok/s; 278089 sec
[2020-04-02 00:55:27,668 INFO] Step 168300/210000; acc:  65.15; ppl:  4.58; xent: 1.52; lr: 0.00000488;   0/585 tok/s; 278162 sec
[2020-04-02 00:55:32,405 INFO] Loading train dataset from ../bert_data/cnndm.train.9133.bert.pt, number of examples: 1978
[2020-04-02 00:56:41,314 INFO] Step 168350/210000; acc:  72.38; ppl:  3.05; xent: 1.11; lr: 0.00000487;   0/489 tok/s; 278236 sec
[2020-04-02 00:57:50,148 INFO] Loading train dataset from ../bert_data/cnndm.train.9134.bert.pt, number of examples: 156
[2020-04-02 00:57:54,622 INFO] Step 168400/210000; acc:  72.34; ppl:  3.18; xent: 1.16; lr: 0.00000487;   0/488 tok/s; 278309 sec
[2020-04-02 00:58:00,822 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-04-02 00:59:17,990 INFO] Step 168450/210000; acc:  47.98; ppl: 13.60; xent: 2.61; lr: 0.00000487;   0/885 tok/s; 278393 sec
[2020-04-02 01:00:42,556 INFO] Step 168500/210000; acc:  53.82; ppl:  9.59; xent: 2.26; lr: 0.00000487;   0/896 tok/s; 278477 sec
[2020-04-02 01:00:42,560 INFO] Saving checkpoint ../models/model_step_168500.pt
[2020-04-02 01:00:50,697 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-04-02 01:02:09,708 INFO] Step 168550/210000; acc:  50.79; ppl: 10.12; xent: 2.31; lr: 0.00000487;   0/575 tok/s; 278565 sec
[2020-04-02 01:03:33,403 INFO] Step 168600/210000; acc:  50.26; ppl: 10.89; xent: 2.39; lr: 0.00000487;   0/586 tok/s; 278648 sec
[2020-04-02 01:03:39,498 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-04-02 01:04:58,544 INFO] Step 168650/210000; acc:  55.61; ppl:  9.02; xent: 2.20; lr: 0.00000487;   0/647 tok/s; 278733 sec
[2020-04-02 01:06:22,444 INFO] Step 168700/210000; acc:  49.10; ppl: 13.64; xent: 2.61; lr: 0.00000487;   0/671 tok/s; 278817 sec
[2020-04-02 01:06:26,549 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-04-02 01:07:47,413 INFO] Step 168750/210000; acc:  51.56; ppl: 11.64; xent: 2.45; lr: 0.00000487;   0/509 tok/s; 278902 sec
[2020-04-02 01:09:11,598 INFO] Step 168800/210000; acc:  53.57; ppl:  9.16; xent: 2.21; lr: 0.00000487;   0/614 tok/s; 278986 sec
[2020-04-02 01:09:13,964 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-04-02 01:10:36,139 INFO] Step 168850/210000; acc:  52.96; ppl:  9.81; xent: 2.28; lr: 0.00000487;   0/771 tok/s; 279071 sec
[2020-04-02 01:12:00,892 INFO] Step 168900/210000; acc:  51.84; ppl: 10.38; xent: 2.34; lr: 0.00000487;   0/624 tok/s; 279156 sec
[2020-04-02 01:12:01,237 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-04-02 01:13:25,829 INFO] Step 168950/210000; acc:  50.51; ppl: 11.57; xent: 2.45; lr: 0.00000487;   0/848 tok/s; 279241 sec
[2020-04-02 01:14:50,798 INFO] Step 169000/210000; acc:  49.84; ppl: 11.33; xent: 2.43; lr: 0.00000487;   0/887 tok/s; 279326 sec
[2020-04-02 01:14:50,802 INFO] Saving checkpoint ../models/model_step_169000.pt
[2020-04-02 01:14:53,460 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-04-02 01:16:17,423 INFO] Step 169050/210000; acc:  55.20; ppl:  8.58; xent: 2.15; lr: 0.00000486;   0/904 tok/s; 279412 sec
[2020-04-02 01:17:40,658 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-04-02 01:17:42,461 INFO] Step 169100/210000; acc:  50.29; ppl: 11.59; xent: 2.45; lr: 0.00000486;   0/567 tok/s; 279497 sec
[2020-04-02 01:19:06,662 INFO] Step 169150/210000; acc:  55.20; ppl:  8.67; xent: 2.16; lr: 0.00000486;   0/937 tok/s; 279581 sec
[2020-04-02 01:20:29,922 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-04-02 01:20:31,611 INFO] Step 169200/210000; acc:  57.01; ppl:  6.89; xent: 1.93; lr: 0.00000486;   0/418 tok/s; 279666 sec
[2020-04-02 01:21:56,293 INFO] Step 169250/210000; acc:  53.31; ppl:  9.42; xent: 2.24; lr: 0.00000486;   0/566 tok/s; 279751 sec
[2020-04-02 01:23:17,454 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-04-02 01:23:21,163 INFO] Step 169300/210000; acc:  54.78; ppl:  8.22; xent: 2.11; lr: 0.00000486;   0/581 tok/s; 279836 sec
[2020-04-02 01:24:45,641 INFO] Step 169350/210000; acc:  52.24; ppl:  9.50; xent: 2.25; lr: 0.00000486;   0/619 tok/s; 279920 sec
[2020-04-02 01:26:05,602 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-04-02 01:26:10,629 INFO] Step 169400/210000; acc:  54.88; ppl:  8.49; xent: 2.14; lr: 0.00000486;   0/749 tok/s; 280005 sec
[2020-04-02 01:27:34,737 INFO] Step 169450/210000; acc:  56.59; ppl:  8.03; xent: 2.08; lr: 0.00000486;   0/674 tok/s; 280090 sec
[2020-04-02 01:28:53,140 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-04-02 01:28:59,923 INFO] Step 169500/210000; acc:  54.80; ppl:  8.16; xent: 2.10; lr: 0.00000486;   0/798 tok/s; 280175 sec
[2020-04-02 01:28:59,925 INFO] Saving checkpoint ../models/model_step_169500.pt
[2020-04-02 01:30:26,617 INFO] Step 169550/210000; acc:  49.96; ppl: 10.29; xent: 2.33; lr: 0.00000486;   0/770 tok/s; 280261 sec
[2020-04-02 01:31:45,477 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-04-02 01:31:52,154 INFO] Step 169600/210000; acc:  53.73; ppl:  8.02; xent: 2.08; lr: 0.00000486;   0/909 tok/s; 280347 sec
[2020-04-02 01:33:16,181 INFO] Step 169650/210000; acc:  48.24; ppl: 12.78; xent: 2.55; lr: 0.00000486;   0/772 tok/s; 280431 sec
[2020-04-02 01:34:32,719 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-04-02 01:34:41,260 INFO] Step 169700/210000; acc:  50.45; ppl: 10.81; xent: 2.38; lr: 0.00000485;   0/857 tok/s; 280516 sec
[2020-04-02 01:36:05,702 INFO] Step 169750/210000; acc:  50.73; ppl: 10.77; xent: 2.38; lr: 0.00000485;   0/853 tok/s; 280601 sec
[2020-04-02 01:37:20,256 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-04-02 01:37:30,312 INFO] Step 169800/210000; acc:  50.32; ppl: 11.03; xent: 2.40; lr: 0.00000485;   0/764 tok/s; 280685 sec
[2020-04-02 01:38:54,644 INFO] Step 169850/210000; acc:  53.59; ppl:  9.79; xent: 2.28; lr: 0.00000485;   0/858 tok/s; 280769 sec
[2020-04-02 01:40:08,035 INFO] Loading train dataset from ../bert_data/cnndm.train.103.bert.pt, number of examples: 2000
[2020-04-02 01:40:19,855 INFO] Step 169900/210000; acc:  51.29; ppl:  8.79; xent: 2.17; lr: 0.00000485;   0/561 tok/s; 280855 sec
[2020-04-02 01:41:44,469 INFO] Step 169950/210000; acc:  55.41; ppl:  7.62; xent: 2.03; lr: 0.00000485;   0/716 tok/s; 280939 sec
[2020-04-02 01:42:57,873 INFO] Loading train dataset from ../bert_data/cnndm.train.104.bert.pt, number of examples: 2000
[2020-04-02 01:43:09,602 INFO] Step 170000/210000; acc:  47.18; ppl: 12.27; xent: 2.51; lr: 0.00000485;   0/659 tok/s; 281024 sec
[2020-04-02 01:43:09,605 INFO] Saving checkpoint ../models/model_step_170000.pt
[2020-04-02 01:44:36,432 INFO] Step 170050/210000; acc:  55.49; ppl:  8.04; xent: 2.08; lr: 0.00000485;   0/595 tok/s; 281111 sec
[2020-04-02 01:45:48,059 INFO] Loading train dataset from ../bert_data/cnndm.train.105.bert.pt, number of examples: 2001
[2020-04-02 01:46:01,500 INFO] Step 170100/210000; acc:  53.68; ppl:  8.33; xent: 2.12; lr: 0.00000485;   0/744 tok/s; 281196 sec
[2020-04-02 01:47:25,620 INFO] Step 170150/210000; acc:  53.39; ppl:  9.14; xent: 2.21; lr: 0.00000485;   0/833 tok/s; 281280 sec
[2020-04-02 01:48:35,498 INFO] Loading train dataset from ../bert_data/cnndm.train.106.bert.pt, number of examples: 1999
[2020-04-02 01:48:50,681 INFO] Step 170200/210000; acc:  56.54; ppl:  8.72; xent: 2.17; lr: 0.00000485;   0/854 tok/s; 281366 sec
[2020-04-02 01:50:15,346 INFO] Step 170250/210000; acc:  52.18; ppl: 10.32; xent: 2.33; lr: 0.00000485;   0/750 tok/s; 281450 sec
[2020-04-02 01:51:23,419 INFO] Loading train dataset from ../bert_data/cnndm.train.107.bert.pt, number of examples: 1999
[2020-04-02 01:51:40,478 INFO] Step 170300/210000; acc:  54.52; ppl:  8.34; xent: 2.12; lr: 0.00000485;   0/841 tok/s; 281535 sec
[2020-04-02 01:53:04,676 INFO] Step 170350/210000; acc:  54.49; ppl:  8.82; xent: 2.18; lr: 0.00000485;   0/862 tok/s; 281619 sec
[2020-04-02 01:54:11,381 INFO] Loading train dataset from ../bert_data/cnndm.train.108.bert.pt, number of examples: 2000
[2020-04-02 01:54:29,868 INFO] Step 170400/210000; acc:  50.00; ppl: 10.30; xent: 2.33; lr: 0.00000485;   0/827 tok/s; 281705 sec
[2020-04-02 01:55:53,778 INFO] Step 170450/210000; acc:  54.91; ppl:  8.49; xent: 2.14; lr: 0.00000484;   0/736 tok/s; 281789 sec
[2020-04-02 01:57:00,293 INFO] Loading train dataset from ../bert_data/cnndm.train.109.bert.pt, number of examples: 2000
[2020-04-02 01:57:18,596 INFO] Step 170500/210000; acc:  56.74; ppl:  7.05; xent: 1.95; lr: 0.00000484;   0/545 tok/s; 281873 sec
[2020-04-02 01:57:18,600 INFO] Saving checkpoint ../models/model_step_170500.pt
[2020-04-02 01:58:45,206 INFO] Step 170550/210000; acc:  53.96; ppl:  9.94; xent: 2.30; lr: 0.00000484;   0/811 tok/s; 281960 sec
[2020-04-02 01:59:50,193 INFO] Loading train dataset from ../bert_data/cnndm.train.11.bert.pt, number of examples: 1999
[2020-04-02 02:00:10,409 INFO] Step 170600/210000; acc:  53.94; ppl:  8.73; xent: 2.17; lr: 0.00000484;   0/650 tok/s; 282045 sec
[2020-04-02 02:01:35,128 INFO] Step 170650/210000; acc:  54.11; ppl:  8.69; xent: 2.16; lr: 0.00000484;   0/703 tok/s; 282130 sec
[2020-04-02 02:02:38,269 INFO] Loading train dataset from ../bert_data/cnndm.train.110.bert.pt, number of examples: 2000
[2020-04-02 02:03:00,203 INFO] Step 170700/210000; acc:  56.47; ppl:  7.47; xent: 2.01; lr: 0.00000484;   0/702 tok/s; 282215 sec
[2020-04-02 02:04:24,694 INFO] Step 170750/210000; acc:  53.44; ppl:  8.31; xent: 2.12; lr: 0.00000484;   0/670 tok/s; 282300 sec
[2020-04-02 02:05:26,352 INFO] Loading train dataset from ../bert_data/cnndm.train.111.bert.pt, number of examples: 2000
[2020-04-02 02:05:49,758 INFO] Step 170800/210000; acc:  52.65; ppl:  9.60; xent: 2.26; lr: 0.00000484;   0/911 tok/s; 282385 sec
[2020-04-02 02:07:13,866 INFO] Step 170850/210000; acc:  53.92; ppl:  8.33; xent: 2.12; lr: 0.00000484;   0/819 tok/s; 282469 sec
[2020-04-02 02:08:13,972 INFO] Loading train dataset from ../bert_data/cnndm.train.112.bert.pt, number of examples: 1999
[2020-04-02 02:08:39,163 INFO] Step 170900/210000; acc:  55.03; ppl:  8.85; xent: 2.18; lr: 0.00000484;   0/848 tok/s; 282554 sec
[2020-04-02 02:10:03,223 INFO] Step 170950/210000; acc:  51.53; ppl:  9.37; xent: 2.24; lr: 0.00000484;   0/826 tok/s; 282638 sec
[2020-04-02 02:11:01,718 INFO] Loading train dataset from ../bert_data/cnndm.train.113.bert.pt, number of examples: 1999
[2020-04-02 02:11:28,911 INFO] Step 171000/210000; acc:  53.86; ppl:  8.09; xent: 2.09; lr: 0.00000484;   0/899 tok/s; 282724 sec
[2020-04-02 02:11:28,915 INFO] Saving checkpoint ../models/model_step_171000.pt
[2020-04-02 02:12:55,666 INFO] Step 171050/210000; acc:  54.64; ppl:  8.82; xent: 2.18; lr: 0.00000484;   0/873 tok/s; 282810 sec
[2020-04-02 02:13:51,677 INFO] Loading train dataset from ../bert_data/cnndm.train.114.bert.pt, number of examples: 1998
[2020-04-02 02:14:20,390 INFO] Step 171100/210000; acc:  57.42; ppl:  6.60; xent: 1.89; lr: 0.00000484;   0/565 tok/s; 282895 sec
[2020-04-02 02:15:44,137 INFO] Step 171150/210000; acc:  52.80; ppl:  9.37; xent: 2.24; lr: 0.00000483;   0/864 tok/s; 282979 sec
[2020-04-02 02:16:40,897 INFO] Loading train dataset from ../bert_data/cnndm.train.115.bert.pt, number of examples: 2000
[2020-04-02 02:17:09,480 INFO] Step 171200/210000; acc:  51.54; ppl: 10.40; xent: 2.34; lr: 0.00000483;   0/573 tok/s; 283064 sec
[2020-04-02 02:18:34,192 INFO] Step 171250/210000; acc:  51.14; ppl:  9.24; xent: 2.22; lr: 0.00000483;   0/674 tok/s; 283149 sec
[2020-04-02 02:19:27,001 INFO] Loading train dataset from ../bert_data/cnndm.train.116.bert.pt, number of examples: 2000
[2020-04-02 02:19:59,237 INFO] Step 171300/210000; acc:  57.01; ppl:  7.26; xent: 1.98; lr: 0.00000483;   0/729 tok/s; 283234 sec
[2020-04-02 02:21:23,457 INFO] Step 171350/210000; acc:  52.31; ppl:  8.58; xent: 2.15; lr: 0.00000483;   0/765 tok/s; 283318 sec
[2020-04-02 02:22:16,271 INFO] Loading train dataset from ../bert_data/cnndm.train.117.bert.pt, number of examples: 2000
[2020-04-02 02:22:48,231 INFO] Step 171400/210000; acc:  51.67; ppl: 10.01; xent: 2.30; lr: 0.00000483;   0/922 tok/s; 283403 sec
[2020-04-02 02:24:12,483 INFO] Step 171450/210000; acc:  53.81; ppl:  8.81; xent: 2.18; lr: 0.00000483;   0/873 tok/s; 283487 sec
[2020-04-02 02:25:04,330 INFO] Loading train dataset from ../bert_data/cnndm.train.118.bert.pt, number of examples: 2001
[2020-04-02 02:25:38,296 INFO] Step 171500/210000; acc:  49.57; ppl: 10.33; xent: 2.34; lr: 0.00000483;   0/999 tok/s; 283573 sec
[2020-04-02 02:25:38,299 INFO] Saving checkpoint ../models/model_step_171500.pt
[2020-04-02 02:27:04,737 INFO] Step 171550/210000; acc:  53.07; ppl:  9.03; xent: 2.20; lr: 0.00000483;   0/809 tok/s; 283660 sec
[2020-04-02 02:27:54,199 INFO] Loading train dataset from ../bert_data/cnndm.train.119.bert.pt, number of examples: 2000
[2020-04-02 02:28:29,663 INFO] Step 171600/210000; acc:  52.20; ppl:  9.67; xent: 2.27; lr: 0.00000483;   0/994 tok/s; 283744 sec
[2020-04-02 02:29:53,954 INFO] Step 171650/210000; acc:  56.54; ppl:  7.75; xent: 2.05; lr: 0.00000483;   0/796 tok/s; 283829 sec
[2020-04-02 02:30:41,800 INFO] Loading train dataset from ../bert_data/cnndm.train.12.bert.pt, number of examples: 2001
[2020-04-02 02:31:18,719 INFO] Step 171700/210000; acc:  48.68; ppl:  9.85; xent: 2.29; lr: 0.00000483;   0/747 tok/s; 283914 sec
[2020-04-02 02:32:42,973 INFO] Step 171750/210000; acc:  48.52; ppl: 10.68; xent: 2.37; lr: 0.00000483;   0/1028 tok/s; 283998 sec
[2020-04-02 02:33:30,756 INFO] Loading train dataset from ../bert_data/cnndm.train.120.bert.pt, number of examples: 2001
[2020-04-02 02:34:07,864 INFO] Step 171800/210000; acc:  53.99; ppl:  7.30; xent: 1.99; lr: 0.00000483;   0/591 tok/s; 284083 sec
[2020-04-02 02:35:32,491 INFO] Step 171850/210000; acc:  50.99; ppl:  9.32; xent: 2.23; lr: 0.00000482;   0/584 tok/s; 284167 sec
[2020-04-02 02:36:18,764 INFO] Loading train dataset from ../bert_data/cnndm.train.121.bert.pt, number of examples: 2001
[2020-04-02 02:36:57,882 INFO] Step 171900/210000; acc:  57.32; ppl:  7.73; xent: 2.04; lr: 0.00000482;   0/700 tok/s; 284253 sec
[2020-04-02 02:38:22,273 INFO] Step 171950/210000; acc:  52.82; ppl:  9.40; xent: 2.24; lr: 0.00000482;   0/646 tok/s; 284337 sec
[2020-04-02 02:39:06,954 INFO] Loading train dataset from ../bert_data/cnndm.train.122.bert.pt, number of examples: 2000
[2020-04-02 02:39:47,608 INFO] Step 172000/210000; acc:  56.76; ppl:  7.35; xent: 2.00; lr: 0.00000482;   0/840 tok/s; 284422 sec
[2020-04-02 02:39:47,612 INFO] Saving checkpoint ../models/model_step_172000.pt
[2020-04-02 02:41:14,251 INFO] Step 172050/210000; acc:  55.50; ppl:  8.94; xent: 2.19; lr: 0.00000482;   0/779 tok/s; 284509 sec
[2020-04-02 02:41:57,216 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001
[2020-04-02 02:42:39,150 INFO] Step 172100/210000; acc:  53.89; ppl:  9.19; xent: 2.22; lr: 0.00000482;   0/867 tok/s; 284594 sec
[2020-04-02 02:44:03,281 INFO] Step 172150/210000; acc:  55.78; ppl:  7.97; xent: 2.08; lr: 0.00000482;   0/853 tok/s; 284678 sec
[2020-04-02 02:44:45,942 INFO] Loading train dataset from ../bert_data/cnndm.train.124.bert.pt, number of examples: 2001
[2020-04-02 02:45:28,098 INFO] Step 172200/210000; acc:  57.79; ppl:  7.17; xent: 1.97; lr: 0.00000482;   0/916 tok/s; 284763 sec
[2020-04-02 02:46:52,661 INFO] Step 172250/210000; acc:  57.07; ppl:  7.52; xent: 2.02; lr: 0.00000482;   0/914 tok/s; 284847 sec
[2020-04-02 02:47:33,795 INFO] Loading train dataset from ../bert_data/cnndm.train.125.bert.pt, number of examples: 2000
[2020-04-02 02:48:18,035 INFO] Step 172300/210000; acc:  54.49; ppl:  9.07; xent: 2.20; lr: 0.00000482;   0/501 tok/s; 284933 sec
[2020-04-02 02:49:42,920 INFO] Step 172350/210000; acc:  53.52; ppl:  8.75; xent: 2.17; lr: 0.00000482;   0/529 tok/s; 285018 sec
[2020-04-02 02:50:22,349 INFO] Loading train dataset from ../bert_data/cnndm.train.126.bert.pt, number of examples: 1999
[2020-04-02 02:51:07,759 INFO] Step 172400/210000; acc:  50.31; ppl:  9.25; xent: 2.22; lr: 0.00000482;   0/588 tok/s; 285103 sec
[2020-04-02 02:52:31,677 INFO] Step 172450/210000; acc:  57.60; ppl:  6.11; xent: 1.81; lr: 0.00000482;   0/579 tok/s; 285186 sec
[2020-04-02 02:53:09,826 INFO] Loading train dataset from ../bert_data/cnndm.train.127.bert.pt, number of examples: 2000
[2020-04-02 02:53:56,797 INFO] Step 172500/210000; acc:  54.03; ppl:  8.82; xent: 2.18; lr: 0.00000482;   0/714 tok/s; 285272 sec
[2020-04-02 02:53:56,800 INFO] Saving checkpoint ../models/model_step_172500.pt
[2020-04-02 02:55:23,602 INFO] Step 172550/210000; acc:  53.96; ppl:  8.48; xent: 2.14; lr: 0.00000481;   0/788 tok/s; 285358 sec
[2020-04-02 02:55:59,735 INFO] Loading train dataset from ../bert_data/cnndm.train.128.bert.pt, number of examples: 2001
[2020-04-02 02:56:48,519 INFO] Step 172600/210000; acc:  54.49; ppl:  8.46; xent: 2.13; lr: 0.00000481;   0/734 tok/s; 285443 sec
[2020-04-02 02:58:12,992 INFO] Step 172650/210000; acc:  50.72; ppl:  9.70; xent: 2.27; lr: 0.00000481;   0/704 tok/s; 285528 sec
[2020-04-02 02:58:47,351 INFO] Loading train dataset from ../bert_data/cnndm.train.129.bert.pt, number of examples: 2000
[2020-04-02 02:59:38,104 INFO] Step 172700/210000; acc:  47.97; ppl: 12.70; xent: 2.54; lr: 0.00000481;   0/886 tok/s; 285613 sec
[2020-04-02 03:01:02,003 INFO] Step 172750/210000; acc:  48.16; ppl: 12.72; xent: 2.54; lr: 0.00000481;   0/813 tok/s; 285697 sec
[2020-04-02 03:01:35,064 INFO] Loading train dataset from ../bert_data/cnndm.train.13.bert.pt, number of examples: 2001
[2020-04-02 03:02:27,459 INFO] Step 172800/210000; acc:  52.40; ppl:  9.24; xent: 2.22; lr: 0.00000481;   0/780 tok/s; 285782 sec
[2020-04-02 03:03:51,591 INFO] Step 172850/210000; acc:  53.74; ppl:  8.34; xent: 2.12; lr: 0.00000481;   0/876 tok/s; 285866 sec
[2020-04-02 03:04:24,701 INFO] Loading train dataset from ../bert_data/cnndm.train.130.bert.pt, number of examples: 2000
[2020-04-02 03:05:16,906 INFO] Step 172900/210000; acc:  57.95; ppl:  6.77; xent: 1.91; lr: 0.00000481;   0/547 tok/s; 285952 sec
[2020-04-02 03:06:40,870 INFO] Step 172950/210000; acc:  56.71; ppl:  6.78; xent: 1.91; lr: 0.00000481;   0/856 tok/s; 286036 sec
[2020-04-02 03:07:11,669 INFO] Loading train dataset from ../bert_data/cnndm.train.131.bert.pt, number of examples: 2001
[2020-04-02 03:08:05,612 INFO] Step 173000/210000; acc:  55.71; ppl:  7.55; xent: 2.02; lr: 0.00000481;   0/609 tok/s; 286120 sec
[2020-04-02 03:08:05,615 INFO] Saving checkpoint ../models/model_step_173000.pt
[2020-04-02 03:09:32,214 INFO] Step 173050/210000; acc:  62.55; ppl:  5.67; xent: 1.74; lr: 0.00000481;   0/484 tok/s; 286207 sec
[2020-04-02 03:10:01,507 INFO] Loading train dataset from ../bert_data/cnndm.train.132.bert.pt, number of examples: 2001
[2020-04-02 03:10:56,929 INFO] Step 173100/210000; acc:  57.79; ppl:  6.79; xent: 1.92; lr: 0.00000481;   0/603 tok/s; 286292 sec
[2020-04-02 03:12:21,087 INFO] Step 173150/210000; acc:  50.79; ppl:  9.39; xent: 2.24; lr: 0.00000481;   0/542 tok/s; 286376 sec
[2020-04-02 03:12:50,548 INFO] Loading train dataset from ../bert_data/cnndm.train.133.bert.pt, number of examples: 1999
[2020-04-02 03:13:46,037 INFO] Step 173200/210000; acc:  58.18; ppl:  6.92; xent: 1.93; lr: 0.00000481;   0/700 tok/s; 286461 sec
[2020-04-02 03:15:10,502 INFO] Step 173250/210000; acc:  54.81; ppl:  8.54; xent: 2.14; lr: 0.00000480;   0/674 tok/s; 286545 sec
[2020-04-02 03:15:38,202 INFO] Loading train dataset from ../bert_data/cnndm.train.134.bert.pt, number of examples: 2001
[2020-04-02 03:16:35,490 INFO] Step 173300/210000; acc:  51.90; ppl:  9.72; xent: 2.27; lr: 0.00000480;   0/920 tok/s; 286630 sec
[2020-04-02 03:17:59,764 INFO] Step 173350/210000; acc:  53.15; ppl:  8.74; xent: 2.17; lr: 0.00000480;   0/892 tok/s; 286715 sec
[2020-04-02 03:18:26,247 INFO] Loading train dataset from ../bert_data/cnndm.train.135.bert.pt, number of examples: 1999
[2020-04-02 03:19:24,941 INFO] Step 173400/210000; acc:  52.07; ppl:  9.97; xent: 2.30; lr: 0.00000480;   0/829 tok/s; 286800 sec
[2020-04-02 03:20:49,161 INFO] Step 173450/210000; acc:  52.20; ppl:  9.03; xent: 2.20; lr: 0.00000480;   0/890 tok/s; 286884 sec
[2020-04-02 03:21:15,382 INFO] Loading train dataset from ../bert_data/cnndm.train.136.bert.pt, number of examples: 2001
[2020-04-02 03:22:14,134 INFO] Step 173500/210000; acc:  50.13; ppl: 10.49; xent: 2.35; lr: 0.00000480;   0/912 tok/s; 286969 sec
[2020-04-02 03:22:14,137 INFO] Saving checkpoint ../models/model_step_173500.pt
[2020-04-02 03:23:40,705 INFO] Step 173550/210000; acc:  57.53; ppl:  7.44; xent: 2.01; lr: 0.00000480;   0/848 tok/s; 287056 sec
[2020-04-02 03:24:04,803 INFO] Loading train dataset from ../bert_data/cnndm.train.137.bert.pt, number of examples: 2000
[2020-04-02 03:25:05,316 INFO] Step 173600/210000; acc:  50.05; ppl: 10.59; xent: 2.36; lr: 0.00000480;   0/602 tok/s; 287140 sec
[2020-04-02 03:26:28,991 INFO] Step 173650/210000; acc:  53.41; ppl:  8.74; xent: 2.17; lr: 0.00000480;   0/789 tok/s; 287224 sec
[2020-04-02 03:26:51,671 INFO] Loading train dataset from ../bert_data/cnndm.train.138.bert.pt, number of examples: 2000
[2020-04-02 03:27:54,346 INFO] Step 173700/210000; acc:  48.59; ppl: 10.86; xent: 2.38; lr: 0.00000480;   0/684 tok/s; 287309 sec
[2020-04-02 03:29:18,374 INFO] Step 173750/210000; acc:  47.34; ppl: 11.38; xent: 2.43; lr: 0.00000480;   0/767 tok/s; 287393 sec
[2020-04-02 03:29:40,903 INFO] Loading train dataset from ../bert_data/cnndm.train.139.bert.pt, number of examples: 2001
[2020-04-02 03:30:42,836 INFO] Step 173800/210000; acc:  55.18; ppl:  7.87; xent: 2.06; lr: 0.00000480;   0/616 tok/s; 287478 sec
[2020-04-02 03:32:06,928 INFO] Step 173850/210000; acc:  54.09; ppl:  7.46; xent: 2.01; lr: 0.00000480;   0/620 tok/s; 287562 sec
[2020-04-02 03:32:28,029 INFO] Loading train dataset from ../bert_data/cnndm.train.14.bert.pt, number of examples: 1998
[2020-04-02 03:33:32,256 INFO] Step 173900/210000; acc:  56.99; ppl:  7.54; xent: 2.02; lr: 0.00000480;   0/616 tok/s; 287647 sec
[2020-04-02 03:34:56,814 INFO] Step 173950/210000; acc:  54.45; ppl:  6.93; xent: 1.94; lr: 0.00000480;   0/563 tok/s; 287732 sec
[2020-04-02 03:35:16,035 INFO] Loading train dataset from ../bert_data/cnndm.train.140.bert.pt, number of examples: 2000
[2020-04-02 03:36:21,861 INFO] Step 174000/210000; acc:  57.46; ppl:  6.99; xent: 1.94; lr: 0.00000479;   0/868 tok/s; 287817 sec
[2020-04-02 03:36:21,865 INFO] Saving checkpoint ../models/model_step_174000.pt
[2020-04-02 03:37:48,185 INFO] Step 174050/210000; acc:  54.86; ppl:  9.40; xent: 2.24; lr: 0.00000479;   0/725 tok/s; 287903 sec
[2020-04-02 03:38:05,880 INFO] Loading train dataset from ../bert_data/cnndm.train.141.bert.pt, number of examples: 1999
[2020-04-02 03:39:13,202 INFO] Step 174100/210000; acc:  55.44; ppl:  8.68; xent: 2.16; lr: 0.00000479;   0/863 tok/s; 287988 sec
[2020-04-02 03:40:37,125 INFO] Step 174150/210000; acc:  51.04; ppl:  8.82; xent: 2.18; lr: 0.00000479;   0/830 tok/s; 288072 sec
[2020-04-02 03:40:54,587 INFO] Loading train dataset from ../bert_data/cnndm.train.142.bert.pt, number of examples: 2000
[2020-04-02 03:42:01,978 INFO] Step 174200/210000; acc:  54.08; ppl:  8.21; xent: 2.11; lr: 0.00000479;   0/886 tok/s; 288157 sec
[2020-04-02 03:43:26,258 INFO] Step 174250/210000; acc:  52.51; ppl:  9.24; xent: 2.22; lr: 0.00000479;   0/804 tok/s; 288241 sec
[2020-04-02 03:43:42,005 INFO] Loading train dataset from ../bert_data/cnndm.train.143.bert.pt, number of examples: 1084
[2020-04-02 03:44:50,897 INFO] Step 174300/210000; acc:  51.83; ppl: 10.51; xent: 2.35; lr: 0.00000479;   0/868 tok/s; 288326 sec
[2020-04-02 03:45:13,327 INFO] Loading train dataset from ../bert_data/cnndm.train.15.bert.pt, number of examples: 1999
[2020-04-02 03:46:15,312 INFO] Step 174350/210000; acc:  51.62; ppl: 10.75; xent: 2.38; lr: 0.00000479;   0/655 tok/s; 288410 sec
[2020-04-02 03:47:39,201 INFO] Step 174400/210000; acc:  53.51; ppl:  8.01; xent: 2.08; lr: 0.00000479;   0/755 tok/s; 288494 sec
[2020-04-02 03:48:01,457 INFO] Loading train dataset from ../bert_data/cnndm.train.150.bert.pt, number of examples: 1985
[2020-04-02 03:48:55,880 INFO] Step 174450/210000; acc:  73.89; ppl:  2.74; xent: 1.01; lr: 0.00000479;   0/414 tok/s; 288571 sec
[2020-04-02 03:50:09,278 INFO] Step 174500/210000; acc:  71.17; ppl:  3.01; xent: 1.10; lr: 0.00000479;   0/547 tok/s; 288644 sec
[2020-04-02 03:50:09,281 INFO] Saving checkpoint ../models/model_step_174500.pt
[2020-04-02 03:50:22,322 INFO] Loading train dataset from ../bert_data/cnndm.train.151.bert.pt, number of examples: 1990
[2020-04-02 03:51:25,218 INFO] Step 174550/210000; acc:  69.03; ppl:  3.60; xent: 1.28; lr: 0.00000479;   0/633 tok/s; 288720 sec
[2020-04-02 03:52:39,663 INFO] Step 174600/210000; acc:  68.24; ppl:  3.40; xent: 1.22; lr: 0.00000479;   0/595 tok/s; 288794 sec
[2020-04-02 03:52:43,287 INFO] Loading train dataset from ../bert_data/cnndm.train.152.bert.pt, number of examples: 993
[2020-04-02 03:53:52,058 INFO] Loading train dataset from ../bert_data/cnndm.train.153.bert.pt, number of examples: 1982
[2020-04-02 03:53:53,633 INFO] Step 174650/210000; acc:  56.18; ppl:  6.48; xent: 1.87; lr: 0.00000479;   0/479 tok/s; 288868 sec
[2020-04-02 03:55:16,839 INFO] Step 174700/210000; acc:  47.95; ppl: 10.09; xent: 2.31; lr: 0.00000479;   0/828 tok/s; 288952 sec
[2020-04-02 03:56:31,847 INFO] Loading train dataset from ../bert_data/cnndm.train.154.bert.pt, number of examples: 1978
[2020-04-02 03:56:41,761 INFO] Step 174750/210000; acc:  53.08; ppl:  6.83; xent: 1.92; lr: 0.00000478;   0/497 tok/s; 289037 sec
[2020-04-02 03:58:05,103 INFO] Step 174800/210000; acc:  50.19; ppl:  9.08; xent: 2.21; lr: 0.00000478;   0/1038 tok/s; 289120 sec
[2020-04-02 03:59:09,815 INFO] Loading train dataset from ../bert_data/cnndm.train.155.bert.pt, number of examples: 1988
[2020-04-02 03:59:29,594 INFO] Step 174850/210000; acc:  53.24; ppl:  7.84; xent: 2.06; lr: 0.00000478;   0/753 tok/s; 289204 sec
[2020-04-02 04:00:52,613 INFO] Step 174900/210000; acc:  43.88; ppl: 12.09; xent: 2.49; lr: 0.00000478;   0/1297 tok/s; 289287 sec
[2020-04-02 04:01:48,787 INFO] Loading train dataset from ../bert_data/cnndm.train.156.bert.pt, number of examples: 1993
[2020-04-02 04:02:16,869 INFO] Step 174950/210000; acc:  49.72; ppl:  8.15; xent: 2.10; lr: 0.00000478;   0/916 tok/s; 289372 sec
[2020-04-02 04:03:40,650 INFO] Step 175000/210000; acc:  49.97; ppl:  7.92; xent: 2.07; lr: 0.00000478;   0/790 tok/s; 289455 sec
[2020-04-02 04:03:40,654 INFO] Saving checkpoint ../models/model_step_175000.pt
[2020-04-02 04:04:31,246 INFO] Loading train dataset from ../bert_data/cnndm.train.157.bert.pt, number of examples: 1981
[2020-04-02 04:05:07,858 INFO] Step 175050/210000; acc:  52.99; ppl:  8.17; xent: 2.10; lr: 0.00000478;   0/771 tok/s; 289543 sec
[2020-04-02 04:06:32,231 INFO] Step 175100/210000; acc:  50.34; ppl:  8.47; xent: 2.14; lr: 0.00000478;   0/630 tok/s; 289627 sec
[2020-04-02 04:07:10,054 INFO] Loading train dataset from ../bert_data/cnndm.train.158.bert.pt, number of examples: 1987
[2020-04-02 04:07:56,298 INFO] Step 175150/210000; acc:  45.55; ppl: 12.36; xent: 2.51; lr: 0.00000478;   0/1118 tok/s; 289711 sec
[2020-04-02 04:09:19,380 INFO] Step 175200/210000; acc:  58.43; ppl:  5.86; xent: 1.77; lr: 0.00000478;   0/579 tok/s; 289794 sec
[2020-04-02 04:09:48,120 INFO] Loading train dataset from ../bert_data/cnndm.train.159.bert.pt, number of examples: 1986
[2020-04-02 04:10:43,114 INFO] Step 175250/210000; acc:  45.98; ppl: 11.00; xent: 2.40; lr: 0.00000478;   0/1141 tok/s; 289878 sec
[2020-04-02 04:12:06,685 INFO] Step 175300/210000; acc:  56.20; ppl:  6.36; xent: 1.85; lr: 0.00000478;   0/577 tok/s; 289962 sec
[2020-04-02 04:12:25,438 INFO] Loading train dataset from ../bert_data/cnndm.train.16.bert.pt, number of examples: 2001
[2020-04-02 04:13:31,143 INFO] Step 175350/210000; acc:  54.37; ppl:  8.45; xent: 2.13; lr: 0.00000478;   0/840 tok/s; 290046 sec
[2020-04-02 04:14:55,341 INFO] Step 175400/210000; acc:  57.70; ppl:  7.03; xent: 1.95; lr: 0.00000478;   0/712 tok/s; 290130 sec
[2020-04-02 04:15:15,092 INFO] Loading train dataset from ../bert_data/cnndm.train.160.bert.pt, number of examples: 1983
[2020-04-02 04:16:20,582 INFO] Step 175450/210000; acc:  60.28; ppl:  5.00; xent: 1.61; lr: 0.00000477;   0/556 tok/s; 290215 sec
[2020-04-02 04:17:43,714 INFO] Step 175500/210000; acc:  47.69; ppl:  8.49; xent: 2.14; lr: 0.00000477;   0/946 tok/s; 290299 sec
[2020-04-02 04:17:43,718 INFO] Saving checkpoint ../models/model_step_175500.pt
[2020-04-02 04:17:57,230 INFO] Loading train dataset from ../bert_data/cnndm.train.161.bert.pt, number of examples: 1990
[2020-04-02 04:19:10,378 INFO] Step 175550/210000; acc:  61.42; ppl:  4.67; xent: 1.54; lr: 0.00000477;   0/476 tok/s; 290385 sec
[2020-04-02 04:20:32,779 INFO] Step 175600/210000; acc:  50.29; ppl:  8.72; xent: 2.17; lr: 0.00000477;   0/988 tok/s; 290468 sec
[2020-04-02 04:20:34,975 INFO] Loading train dataset from ../bert_data/cnndm.train.162.bert.pt, number of examples: 1986
[2020-04-02 04:21:57,262 INFO] Step 175650/210000; acc:  63.43; ppl:  4.10; xent: 1.41; lr: 0.00000477;   0/433 tok/s; 290552 sec
[2020-04-02 04:23:14,693 INFO] Loading train dataset from ../bert_data/cnndm.train.163.bert.pt, number of examples: 1982
[2020-04-02 04:23:21,138 INFO] Step 175700/210000; acc:  44.90; ppl: 12.54; xent: 2.53; lr: 0.00000477;   0/1095 tok/s; 290636 sec
[2020-04-02 04:24:44,671 INFO] Step 175750/210000; acc:  61.15; ppl:  5.45; xent: 1.70; lr: 0.00000477;   0/546 tok/s; 290719 sec
[2020-04-02 04:25:50,999 INFO] Loading train dataset from ../bert_data/cnndm.train.164.bert.pt, number of examples: 1984
[2020-04-02 04:26:07,219 INFO] Step 175800/210000; acc:  45.12; ppl: 12.15; xent: 2.50; lr: 0.00000477;   0/1169 tok/s; 290802 sec
[2020-04-02 04:27:31,000 INFO] Step 175850/210000; acc:  57.70; ppl:  6.10; xent: 1.81; lr: 0.00000477;   0/722 tok/s; 290886 sec
[2020-04-02 04:28:29,965 INFO] Loading train dataset from ../bert_data/cnndm.train.165.bert.pt, number of examples: 1984
[2020-04-02 04:28:55,222 INFO] Step 175900/210000; acc:  51.88; ppl:  7.91; xent: 2.07; lr: 0.00000477;   0/728 tok/s; 290970 sec
[2020-04-02 04:30:18,222 INFO] Step 175950/210000; acc:  51.75; ppl:  7.53; xent: 2.02; lr: 0.00000477;   0/847 tok/s; 291053 sec
[2020-04-02 04:31:07,039 INFO] Loading train dataset from ../bert_data/cnndm.train.166.bert.pt, number of examples: 1370
[2020-04-02 04:31:41,769 INFO] Step 176000/210000; acc:  63.11; ppl:  4.56; xent: 1.52; lr: 0.00000477;   0/452 tok/s; 291137 sec
[2020-04-02 04:31:41,772 INFO] Saving checkpoint ../models/model_step_176000.pt
[2020-04-02 04:33:00,074 INFO] Loading train dataset from ../bert_data/cnndm.train.167.bert.pt, number of examples: 1089
[2020-04-02 04:33:08,178 INFO] Step 176050/210000; acc:  52.99; ppl:  7.68; xent: 2.04; lr: 0.00000477;   0/1212 tok/s; 291223 sec
[2020-04-02 04:34:26,048 INFO] Loading train dataset from ../bert_data/cnndm.train.168.bert.pt, number of examples: 1991
[2020-04-02 04:34:29,211 INFO] Step 176100/210000; acc:  58.87; ppl:  5.08; xent: 1.62; lr: 0.00000477;   0/588 tok/s; 291304 sec
[2020-04-02 04:35:50,652 INFO] Step 176150/210000; acc:  50.78; ppl:  8.13; xent: 2.10; lr: 0.00000477;   0/825 tok/s; 291385 sec
[2020-04-02 04:37:00,924 INFO] Loading train dataset from ../bert_data/cnndm.train.169.bert.pt, number of examples: 1995
[2020-04-02 04:37:12,476 INFO] Step 176200/210000; acc:  60.21; ppl:  4.92; xent: 1.59; lr: 0.00000476;   0/522 tok/s; 291467 sec
[2020-04-02 04:38:33,886 INFO] Step 176250/210000; acc:  46.91; ppl: 10.74; xent: 2.37; lr: 0.00000476;   0/1146 tok/s; 291549 sec
[2020-04-02 04:39:36,570 INFO] Loading train dataset from ../bert_data/cnndm.train.17.bert.pt, number of examples: 2000
[2020-04-02 04:39:56,964 INFO] Step 176300/210000; acc:  55.85; ppl:  7.49; xent: 2.01; lr: 0.00000476;   0/549 tok/s; 291632 sec
[2020-04-02 04:41:21,421 INFO] Step 176350/210000; acc:  59.48; ppl:  6.07; xent: 1.80; lr: 0.00000476;   0/577 tok/s; 291716 sec
[2020-04-02 04:42:26,254 INFO] Loading train dataset from ../bert_data/cnndm.train.170.bert.pt, number of examples: 1981
[2020-04-02 04:42:45,668 INFO] Step 176400/210000; acc:  58.23; ppl:  5.66; xent: 1.73; lr: 0.00000476;   0/661 tok/s; 291800 sec
[2020-04-02 04:44:07,655 INFO] Step 176450/210000; acc:  53.15; ppl:  7.69; xent: 2.04; lr: 0.00000476;   0/883 tok/s; 291882 sec
[2020-04-02 04:44:58,922 INFO] Loading train dataset from ../bert_data/cnndm.train.171.bert.pt, number of examples: 1990
[2020-04-02 04:45:29,903 INFO] Step 176500/210000; acc:  47.82; ppl:  9.43; xent: 2.24; lr: 0.00000476;   0/1042 tok/s; 291965 sec
[2020-04-02 04:45:29,906 INFO] Saving checkpoint ../models/model_step_176500.pt
[2020-04-02 04:46:53,560 INFO] Step 176550/210000; acc:  61.42; ppl:  4.71; xent: 1.55; lr: 0.00000476;   0/515 tok/s; 292048 sec
[2020-04-02 04:47:38,366 INFO] Loading train dataset from ../bert_data/cnndm.train.172.bert.pt, number of examples: 1983
[2020-04-02 04:48:15,210 INFO] Step 176600/210000; acc:  53.24; ppl:  6.86; xent: 1.93; lr: 0.00000476;   0/1172 tok/s; 292130 sec
[2020-04-02 04:49:36,603 INFO] Step 176650/210000; acc:  63.71; ppl:  3.92; xent: 1.36; lr: 0.00000476;   0/566 tok/s; 292211 sec
[2020-04-02 04:50:11,271 INFO] Loading train dataset from ../bert_data/cnndm.train.18.bert.pt, number of examples: 1998
[2020-04-02 04:51:00,101 INFO] Step 176700/210000; acc:  54.23; ppl:  7.58; xent: 2.03; lr: 0.00000476;   0/921 tok/s; 292295 sec
[2020-04-02 04:52:24,607 INFO] Step 176750/210000; acc:  53.55; ppl:  9.42; xent: 2.24; lr: 0.00000476;   0/758 tok/s; 292379 sec
[2020-04-02 04:52:59,003 INFO] Loading train dataset from ../bert_data/cnndm.train.19.bert.pt, number of examples: 2000
[2020-04-02 04:53:49,487 INFO] Step 176800/210000; acc:  57.41; ppl:  7.50; xent: 2.01; lr: 0.00000476;   0/831 tok/s; 292464 sec
[2020-04-02 04:55:13,635 INFO] Step 176850/210000; acc:  51.59; ppl:  9.79; xent: 2.28; lr: 0.00000476;   0/644 tok/s; 292548 sec
[2020-04-02 04:55:46,676 INFO] Loading train dataset from ../bert_data/cnndm.train.2.bert.pt, number of examples: 2001
[2020-04-02 04:56:38,986 INFO] Step 176900/210000; acc:  48.61; ppl: 10.31; xent: 2.33; lr: 0.00000476;   0/863 tok/s; 292634 sec
[2020-04-02 04:58:02,908 INFO] Step 176950/210000; acc:  59.68; ppl:  6.08; xent: 1.80; lr: 0.00000475;   0/834 tok/s; 292718 sec
[2020-04-02 04:58:35,690 INFO] Loading train dataset from ../bert_data/cnndm.train.20.bert.pt, number of examples: 2000
[2020-04-02 04:59:28,073 INFO] Step 177000/210000; acc:  53.42; ppl:  8.48; xent: 2.14; lr: 0.00000475;   0/518 tok/s; 292803 sec
[2020-04-02 04:59:28,077 INFO] Saving checkpoint ../models/model_step_177000.pt
[2020-04-02 05:00:55,076 INFO] Step 177050/210000; acc:  52.27; ppl:  9.75; xent: 2.28; lr: 0.00000475;   0/1013 tok/s; 292890 sec
[2020-04-02 05:01:26,108 INFO] Loading train dataset from ../bert_data/cnndm.train.21.bert.pt, number of examples: 2001
[2020-04-02 05:02:20,050 INFO] Step 177100/210000; acc:  48.19; ppl: 11.74; xent: 2.46; lr: 0.00000475;   0/637 tok/s; 292975 sec
[2020-04-02 05:03:44,190 INFO] Step 177150/210000; acc:  52.66; ppl:  9.38; xent: 2.24; lr: 0.00000475;   0/962 tok/s; 293059 sec
[2020-04-02 05:04:13,292 INFO] Loading train dataset from ../bert_data/cnndm.train.22.bert.pt, number of examples: 1999
[2020-04-02 05:05:08,810 INFO] Step 177200/210000; acc:  59.93; ppl:  5.83; xent: 1.76; lr: 0.00000475;   0/663 tok/s; 293144 sec
[2020-04-02 05:06:33,134 INFO] Step 177250/210000; acc:  53.39; ppl:  8.00; xent: 2.08; lr: 0.00000475;   0/716 tok/s; 293228 sec
[2020-04-02 05:07:02,938 INFO] Loading train dataset from ../bert_data/cnndm.train.23.bert.pt, number of examples: 2001
[2020-04-02 05:07:58,672 INFO] Step 177300/210000; acc:  52.96; ppl:  8.93; xent: 2.19; lr: 0.00000475;   0/676 tok/s; 293313 sec
[2020-04-02 05:09:22,914 INFO] Step 177350/210000; acc:  60.27; ppl:  6.75; xent: 1.91; lr: 0.00000475;   0/618 tok/s; 293398 sec
[2020-04-02 05:09:50,626 INFO] Loading train dataset from ../bert_data/cnndm.train.24.bert.pt, number of examples: 2001
[2020-04-02 05:10:47,678 INFO] Step 177400/210000; acc:  49.73; ppl: 10.05; xent: 2.31; lr: 0.00000475;   0/771 tok/s; 293483 sec
[2020-04-02 05:12:11,653 INFO] Step 177450/210000; acc:  57.02; ppl:  7.32; xent: 1.99; lr: 0.00000475;   0/626 tok/s; 293566 sec
[2020-04-02 05:12:37,682 INFO] Loading train dataset from ../bert_data/cnndm.train.25.bert.pt, number of examples: 2001
[2020-04-02 05:13:36,773 INFO] Step 177500/210000; acc:  52.93; ppl:  8.65; xent: 2.16; lr: 0.00000475;   0/928 tok/s; 293652 sec
[2020-04-02 05:13:36,777 INFO] Saving checkpoint ../models/model_step_177500.pt
[2020-04-02 05:15:02,770 INFO] Step 177550/210000; acc:  54.11; ppl:  8.83; xent: 2.18; lr: 0.00000475;   0/920 tok/s; 293738 sec
[2020-04-02 05:15:28,914 INFO] Loading train dataset from ../bert_data/cnndm.train.26.bert.pt, number of examples: 2000
[2020-04-02 05:16:27,827 INFO] Step 177600/210000; acc:  55.69; ppl:  8.38; xent: 2.13; lr: 0.00000475;   0/820 tok/s; 293823 sec
[2020-04-02 05:17:51,817 INFO] Step 177650/210000; acc:  58.04; ppl:  6.50; xent: 1.87; lr: 0.00000475;   0/870 tok/s; 293907 sec
[2020-04-02 05:18:16,248 INFO] Loading train dataset from ../bert_data/cnndm.train.27.bert.pt, number of examples: 2001
[2020-04-02 05:19:17,232 INFO] Step 177700/210000; acc:  58.39; ppl:  7.07; xent: 1.96; lr: 0.00000474;   0/688 tok/s; 293992 sec
[2020-04-02 05:20:41,626 INFO] Step 177750/210000; acc:  48.37; ppl: 10.80; xent: 2.38; lr: 0.00000474;   0/787 tok/s; 294076 sec
[2020-04-02 05:21:06,425 INFO] Loading train dataset from ../bert_data/cnndm.train.28.bert.pt, number of examples: 2000
[2020-04-02 05:22:07,458 INFO] Step 177800/210000; acc:  57.59; ppl:  6.54; xent: 1.88; lr: 0.00000474;   0/496 tok/s; 294162 sec
[2020-04-02 05:23:31,406 INFO] Step 177850/210000; acc:  48.71; ppl: 10.85; xent: 2.38; lr: 0.00000474;   0/691 tok/s; 294246 sec
[2020-04-02 05:23:54,008 INFO] Loading train dataset from ../bert_data/cnndm.train.29.bert.pt, number of examples: 1999
[2020-04-02 05:24:56,204 INFO] Step 177900/210000; acc:  52.81; ppl:  9.00; xent: 2.20; lr: 0.00000474;   0/907 tok/s; 294331 sec
[2020-04-02 05:26:20,658 INFO] Step 177950/210000; acc:  47.78; ppl: 11.42; xent: 2.44; lr: 0.00000474;   0/703 tok/s; 294415 sec
[2020-04-02 05:26:41,533 INFO] Loading train dataset from ../bert_data/cnndm.train.3.bert.pt, number of examples: 2001
[2020-04-02 05:27:45,337 INFO] Step 178000/210000; acc:  53.14; ppl:  9.54; xent: 2.25; lr: 0.00000474;   0/592 tok/s; 294500 sec
[2020-04-02 05:27:45,340 INFO] Saving checkpoint ../models/model_step_178000.pt
[2020-04-02 05:29:11,836 INFO] Step 178050/210000; acc:  56.85; ppl:  7.13; xent: 1.96; lr: 0.00000474;   0/591 tok/s; 294587 sec
[2020-04-02 05:29:30,957 INFO] Loading train dataset from ../bert_data/cnndm.train.30.bert.pt, number of examples: 1996
[2020-04-02 05:30:36,410 INFO] Step 178100/210000; acc:  56.21; ppl:  7.63; xent: 2.03; lr: 0.00000474;   0/735 tok/s; 294671 sec
[2020-04-02 05:32:00,956 INFO] Step 178150/210000; acc:  49.24; ppl: 11.85; xent: 2.47; lr: 0.00000474;   0/740 tok/s; 294756 sec
[2020-04-02 05:32:18,813 INFO] Loading train dataset from ../bert_data/cnndm.train.31.bert.pt, number of examples: 2000
[2020-04-02 05:33:26,560 INFO] Step 178200/210000; acc:  54.08; ppl:  9.75; xent: 2.28; lr: 0.00000474;   0/776 tok/s; 294841 sec
[2020-04-02 05:34:50,865 INFO] Step 178250/210000; acc:  52.71; ppl: 10.42; xent: 2.34; lr: 0.00000474;   0/785 tok/s; 294926 sec
[2020-04-02 05:35:08,942 INFO] Loading train dataset from ../bert_data/cnndm.train.32.bert.pt, number of examples: 1998
[2020-04-02 05:36:16,171 INFO] Step 178300/210000; acc:  53.80; ppl:  9.47; xent: 2.25; lr: 0.00000474;   0/909 tok/s; 295011 sec
[2020-04-02 05:37:40,459 INFO] Step 178350/210000; acc:  51.93; ppl: 10.33; xent: 2.33; lr: 0.00000474;   0/904 tok/s; 295095 sec
[2020-04-02 05:37:56,260 INFO] Loading train dataset from ../bert_data/cnndm.train.33.bert.pt, number of examples: 1999
[2020-04-02 05:39:05,849 INFO] Step 178400/210000; acc:  55.84; ppl:  7.94; xent: 2.07; lr: 0.00000474;   0/878 tok/s; 295181 sec
[2020-04-02 05:40:29,567 INFO] Step 178450/210000; acc:  53.47; ppl:  9.21; xent: 2.22; lr: 0.00000473;   0/850 tok/s; 295264 sec
[2020-04-02 05:40:43,710 INFO] Loading train dataset from ../bert_data/cnndm.train.34.bert.pt, number of examples: 2000
[2020-04-02 05:41:54,471 INFO] Step 178500/210000; acc:  53.66; ppl:  9.69; xent: 2.27; lr: 0.00000473;   0/1002 tok/s; 295349 sec
[2020-04-02 05:41:54,497 INFO] Saving checkpoint ../models/model_step_178500.pt
[2020-04-02 05:43:21,026 INFO] Step 178550/210000; acc:  54.12; ppl:  8.78; xent: 2.17; lr: 0.00000473;   0/858 tok/s; 295436 sec
[2020-04-02 05:43:33,555 INFO] Loading train dataset from ../bert_data/cnndm.train.35.bert.pt, number of examples: 1998
[2020-04-02 05:44:46,023 INFO] Step 178600/210000; acc:  59.74; ppl:  6.49; xent: 1.87; lr: 0.00000473;   0/619 tok/s; 295521 sec
[2020-04-02 05:46:10,351 INFO] Step 178650/210000; acc:  55.67; ppl:  6.69; xent: 1.90; lr: 0.00000473;   0/486 tok/s; 295605 sec
[2020-04-02 05:46:22,821 INFO] Loading train dataset from ../bert_data/cnndm.train.36.bert.pt, number of examples: 2000
[2020-04-02 05:47:35,367 INFO] Step 178700/210000; acc:  54.31; ppl:  8.75; xent: 2.17; lr: 0.00000473;   0/809 tok/s; 295690 sec
[2020-04-02 05:49:00,291 INFO] Step 178750/210000; acc:  50.87; ppl: 10.39; xent: 2.34; lr: 0.00000473;   0/779 tok/s; 295775 sec
[2020-04-02 05:49:11,202 INFO] Loading train dataset from ../bert_data/cnndm.train.37.bert.pt, number of examples: 1999
[2020-04-02 05:50:25,476 INFO] Step 178800/210000; acc:  54.56; ppl:  8.66; xent: 2.16; lr: 0.00000473;   0/856 tok/s; 295860 sec
[2020-04-02 05:51:49,466 INFO] Step 178850/210000; acc:  55.25; ppl:  8.25; xent: 2.11; lr: 0.00000473;   0/743 tok/s; 295944 sec
[2020-04-02 05:51:58,642 INFO] Loading train dataset from ../bert_data/cnndm.train.38.bert.pt, number of examples: 2001
[2020-04-02 05:53:14,296 INFO] Step 178900/210000; acc:  52.64; ppl:  9.93; xent: 2.30; lr: 0.00000473;   0/899 tok/s; 296029 sec
[2020-04-02 05:54:38,861 INFO] Step 178950/210000; acc:  51.46; ppl: 10.92; xent: 2.39; lr: 0.00000473;   0/963 tok/s; 296114 sec
[2020-04-02 05:54:46,362 INFO] Loading train dataset from ../bert_data/cnndm.train.39.bert.pt, number of examples: 2000
[2020-04-02 05:56:03,574 INFO] Step 179000/210000; acc:  49.73; ppl: 10.59; xent: 2.36; lr: 0.00000473;   0/876 tok/s; 296198 sec
[2020-04-02 05:56:03,577 INFO] Saving checkpoint ../models/model_step_179000.pt
[2020-04-02 05:57:30,443 INFO] Step 179050/210000; acc:  54.96; ppl:  8.43; xent: 2.13; lr: 0.00000473;   0/916 tok/s; 296285 sec
[2020-04-02 05:57:37,856 INFO] Loading train dataset from ../bert_data/cnndm.train.4.bert.pt, number of examples: 2001
[2020-04-02 05:58:55,790 INFO] Step 179100/210000; acc:  54.63; ppl:  8.77; xent: 2.17; lr: 0.00000473;   0/595 tok/s; 296371 sec
[2020-04-02 06:00:19,852 INFO] Step 179150/210000; acc:  55.60; ppl:  7.74; xent: 2.05; lr: 0.00000473;   0/850 tok/s; 296455 sec
[2020-04-02 06:00:25,685 INFO] Loading train dataset from ../bert_data/cnndm.train.40.bert.pt, number of examples: 1999
[2020-04-02 06:01:45,032 INFO] Step 179200/210000; acc:  51.59; ppl: 10.04; xent: 2.31; lr: 0.00000472;   0/608 tok/s; 296540 sec
[2020-04-02 06:03:08,975 INFO] Step 179250/210000; acc:  51.20; ppl:  8.37; xent: 2.13; lr: 0.00000472;   0/599 tok/s; 296624 sec
[2020-04-02 06:03:13,069 INFO] Loading train dataset from ../bert_data/cnndm.train.41.bert.pt, number of examples: 2000
[2020-04-02 06:04:33,758 INFO] Step 179300/210000; acc:  57.96; ppl:  5.99; xent: 1.79; lr: 0.00000472;   0/554 tok/s; 296709 sec
[2020-04-02 06:05:58,306 INFO] Step 179350/210000; acc:  55.53; ppl:  7.88; xent: 2.06; lr: 0.00000472;   0/595 tok/s; 296793 sec
[2020-04-02 06:06:02,411 INFO] Loading train dataset from ../bert_data/cnndm.train.42.bert.pt, number of examples: 2000
[2020-04-02 06:07:23,605 INFO] Step 179400/210000; acc:  57.09; ppl:  7.74; xent: 2.05; lr: 0.00000472;   0/748 tok/s; 296878 sec
[2020-04-02 06:08:48,720 INFO] Step 179450/210000; acc:  54.26; ppl:  7.72; xent: 2.04; lr: 0.00000472;   0/690 tok/s; 296964 sec
[2020-04-02 06:08:50,786 INFO] Loading train dataset from ../bert_data/cnndm.train.43.bert.pt, number of examples: 2001
[2020-04-02 06:10:13,297 INFO] Step 179500/210000; acc:  57.36; ppl:  6.84; xent: 1.92; lr: 0.00000472;   0/602 tok/s; 297048 sec
[2020-04-02 06:10:13,322 INFO] Saving checkpoint ../models/model_step_179500.pt
[2020-04-02 06:11:40,470 INFO] Step 179550/210000; acc:  52.01; ppl:  9.07; xent: 2.21; lr: 0.00000472;   0/586 tok/s; 297135 sec
[2020-04-02 06:11:40,814 INFO] Loading train dataset from ../bert_data/cnndm.train.44.bert.pt, number of examples: 1998
[2020-04-02 06:13:05,692 INFO] Step 179600/210000; acc:  46.87; ppl: 11.79; xent: 2.47; lr: 0.00000472;   0/861 tok/s; 297221 sec
[2020-04-02 06:14:29,487 INFO] Loading train dataset from ../bert_data/cnndm.train.45.bert.pt, number of examples: 2001
[2020-04-02 06:14:31,335 INFO] Step 179650/210000; acc:  57.47; ppl:  7.14; xent: 1.97; lr: 0.00000472;   0/494 tok/s; 297306 sec
[2020-04-02 06:15:55,541 INFO] Step 179700/210000; acc:  54.64; ppl:  8.20; xent: 2.10; lr: 0.00000472;   0/966 tok/s; 297390 sec
[2020-04-02 06:17:17,198 INFO] Loading train dataset from ../bert_data/cnndm.train.46.bert.pt, number of examples: 2001
[2020-04-02 06:17:20,547 INFO] Step 179750/210000; acc:  53.91; ppl:  7.36; xent: 2.00; lr: 0.00000472;   0/541 tok/s; 297475 sec
[2020-04-02 06:18:44,420 INFO] Step 179800/210000; acc:  53.73; ppl:  8.53; xent: 2.14; lr: 0.00000472;   0/928 tok/s; 297559 sec
[2020-04-02 06:20:05,299 INFO] Loading train dataset from ../bert_data/cnndm.train.47.bert.pt, number of examples: 2000
[2020-04-02 06:20:08,676 INFO] Step 179850/210000; acc:  61.41; ppl:  5.01; xent: 1.61; lr: 0.00000472;   0/561 tok/s; 297643 sec
[2020-04-02 06:21:33,403 INFO] Step 179900/210000; acc:  48.38; ppl: 13.16; xent: 2.58; lr: 0.00000472;   0/819 tok/s; 297728 sec
[2020-04-02 06:22:52,936 INFO] Loading train dataset from ../bert_data/cnndm.train.48.bert.pt, number of examples: 2000
[2020-04-02 06:22:58,065 INFO] Step 179950/210000; acc:  53.99; ppl:  8.75; xent: 2.17; lr: 0.00000471;   0/714 tok/s; 297813 sec
[2020-04-02 06:24:22,064 INFO] Step 180000/210000; acc:  56.36; ppl:  7.45; xent: 2.01; lr: 0.00000471;   0/656 tok/s; 297897 sec
[2020-04-02 06:24:22,089 INFO] Saving checkpoint ../models/model_step_180000.pt
[2020-04-02 06:25:43,305 INFO] Loading train dataset from ../bert_data/cnndm.train.49.bert.pt, number of examples: 2001
[2020-04-02 06:25:50,059 INFO] Step 180050/210000; acc:  51.55; ppl: 10.86; xent: 2.38; lr: 0.00000471;   0/750 tok/s; 297985 sec
[2020-04-02 06:27:14,280 INFO] Step 180100/210000; acc:  54.61; ppl:  7.97; xent: 2.08; lr: 0.00000471;   0/684 tok/s; 298069 sec
[2020-04-02 06:28:32,310 INFO] Loading train dataset from ../bert_data/cnndm.train.5.bert.pt, number of examples: 2001
[2020-04-02 06:28:39,025 INFO] Step 180150/210000; acc:  53.96; ppl:  8.46; xent: 2.14; lr: 0.00000471;   0/875 tok/s; 298154 sec
[2020-04-02 06:30:02,634 INFO] Step 180200/210000; acc:  52.97; ppl:  8.38; xent: 2.13; lr: 0.00000471;   0/833 tok/s; 298237 sec
[2020-04-02 06:31:18,992 INFO] Loading train dataset from ../bert_data/cnndm.train.50.bert.pt, number of examples: 2001
[2020-04-02 06:31:27,357 INFO] Step 180250/210000; acc:  55.10; ppl:  8.31; xent: 2.12; lr: 0.00000471;   0/870 tok/s; 298322 sec
[2020-04-02 06:32:51,326 INFO] Step 180300/210000; acc:  51.18; ppl: 10.62; xent: 2.36; lr: 0.00000471;   0/863 tok/s; 298406 sec
[2020-04-02 06:34:07,332 INFO] Loading train dataset from ../bert_data/cnndm.train.51.bert.pt, number of examples: 2000
[2020-04-02 06:34:15,714 INFO] Step 180350/210000; acc:  54.84; ppl:  7.94; xent: 2.07; lr: 0.00000471;   0/863 tok/s; 298491 sec
[2020-04-02 06:35:39,718 INFO] Step 180400/210000; acc:  55.11; ppl:  7.30; xent: 1.99; lr: 0.00000471;   0/883 tok/s; 298575 sec
[2020-04-02 06:36:54,153 INFO] Loading train dataset from ../bert_data/cnndm.train.52.bert.pt, number of examples: 2001
[2020-04-02 06:37:04,500 INFO] Step 180450/210000; acc:  51.45; ppl:  9.96; xent: 2.30; lr: 0.00000471;   0/841 tok/s; 298659 sec
[2020-04-02 06:38:28,868 INFO] Step 180500/210000; acc:  55.06; ppl:  7.56; xent: 2.02; lr: 0.00000471;   0/824 tok/s; 298744 sec
[2020-04-02 06:38:28,892 INFO] Saving checkpoint ../models/model_step_180500.pt
[2020-04-02 06:39:45,599 INFO] Loading train dataset from ../bert_data/cnndm.train.53.bert.pt, number of examples: 1999
[2020-04-02 06:39:55,671 INFO] Step 180550/210000; acc:  57.99; ppl:  6.50; xent: 1.87; lr: 0.00000471;   0/767 tok/s; 298830 sec
[2020-04-02 06:41:19,787 INFO] Step 180600/210000; acc:  50.94; ppl:  8.78; xent: 2.17; lr: 0.00000471;   0/512 tok/s; 298915 sec
[2020-04-02 06:42:32,981 INFO] Loading train dataset from ../bert_data/cnndm.train.54.bert.pt, number of examples: 2001
[2020-04-02 06:42:44,691 INFO] Step 180650/210000; acc:  57.28; ppl:  7.77; xent: 2.05; lr: 0.00000471;   0/596 tok/s; 299000 sec
[2020-04-02 06:44:09,018 INFO] Step 180700/210000; acc:  51.89; ppl:  8.15; xent: 2.10; lr: 0.00000470;   0/814 tok/s; 299084 sec
[2020-04-02 06:45:20,719 INFO] Loading train dataset from ../bert_data/cnndm.train.55.bert.pt, number of examples: 1999
[2020-04-02 06:45:34,238 INFO] Step 180750/210000; acc:  56.65; ppl:  6.83; xent: 1.92; lr: 0.00000470;   0/642 tok/s; 299169 sec
[2020-04-02 06:46:58,407 INFO] Step 180800/210000; acc:  50.65; ppl:  9.02; xent: 2.20; lr: 0.00000470;   0/604 tok/s; 299253 sec
[2020-04-02 06:48:10,002 INFO] Loading train dataset from ../bert_data/cnndm.train.56.bert.pt, number of examples: 2001
[2020-04-02 06:48:23,331 INFO] Step 180850/210000; acc:  57.25; ppl:  7.12; xent: 1.96; lr: 0.00000470;   0/726 tok/s; 299338 sec
[2020-04-02 06:49:47,716 INFO] Step 180900/210000; acc:  53.59; ppl:  8.24; xent: 2.11; lr: 0.00000470;   0/542 tok/s; 299423 sec
[2020-04-02 06:50:57,786 INFO] Loading train dataset from ../bert_data/cnndm.train.57.bert.pt, number of examples: 1999
[2020-04-02 06:51:12,850 INFO] Step 180950/210000; acc:  56.67; ppl:  7.64; xent: 2.03; lr: 0.00000470;   0/857 tok/s; 299508 sec
[2020-04-02 06:52:37,281 INFO] Step 181000/210000; acc:  54.48; ppl:  8.01; xent: 2.08; lr: 0.00000470;   0/829 tok/s; 299592 sec
[2020-04-02 06:52:37,285 INFO] Saving checkpoint ../models/model_step_181000.pt
[2020-04-02 06:53:47,988 INFO] Loading train dataset from ../bert_data/cnndm.train.58.bert.pt, number of examples: 2000
[2020-04-02 06:54:04,696 INFO] Step 181050/210000; acc:  58.63; ppl:  6.59; xent: 1.89; lr: 0.00000470;   0/825 tok/s; 299680 sec
[2020-04-02 06:55:28,895 INFO] Step 181100/210000; acc:  56.53; ppl:  7.63; xent: 2.03; lr: 0.00000470;   0/860 tok/s; 299764 sec
[2020-04-02 06:56:35,869 INFO] Loading train dataset from ../bert_data/cnndm.train.59.bert.pt, number of examples: 2000
[2020-04-02 06:56:54,792 INFO] Step 181150/210000; acc:  50.03; ppl: 10.26; xent: 2.33; lr: 0.00000470;   0/994 tok/s; 299850 sec
[2020-04-02 06:58:19,148 INFO] Step 181200/210000; acc:  50.87; ppl:  9.23; xent: 2.22; lr: 0.00000470;   0/917 tok/s; 299934 sec
[2020-04-02 06:59:24,031 INFO] Loading train dataset from ../bert_data/cnndm.train.6.bert.pt, number of examples: 2001
[2020-04-02 06:59:44,468 INFO] Step 181250/210000; acc:  53.94; ppl:  9.41; xent: 2.24; lr: 0.00000470;   0/579 tok/s; 300019 sec
[2020-04-02 07:01:09,045 INFO] Step 181300/210000; acc:  55.94; ppl:  6.57; xent: 1.88; lr: 0.00000470;   0/583 tok/s; 300104 sec
[2020-04-02 07:02:12,349 INFO] Loading train dataset from ../bert_data/cnndm.train.60.bert.pt, number of examples: 2000
[2020-04-02 07:02:34,030 INFO] Step 181350/210000; acc:  56.11; ppl:  7.84; xent: 2.06; lr: 0.00000470;   0/710 tok/s; 300189 sec
[2020-04-02 07:03:58,220 INFO] Step 181400/210000; acc:  58.89; ppl:  6.39; xent: 1.85; lr: 0.00000470;   0/617 tok/s; 300273 sec
[2020-04-02 07:05:01,992 INFO] Loading train dataset from ../bert_data/cnndm.train.61.bert.pt, number of examples: 2001
[2020-04-02 07:05:23,701 INFO] Step 181450/210000; acc:  56.79; ppl:  8.11; xent: 2.09; lr: 0.00000470;   0/789 tok/s; 300359 sec
[2020-04-02 07:06:47,953 INFO] Step 181500/210000; acc:  52.28; ppl:  9.13; xent: 2.21; lr: 0.00000469;   0/881 tok/s; 300443 sec
[2020-04-02 07:06:47,957 INFO] Saving checkpoint ../models/model_step_181500.pt
[2020-04-02 07:07:51,408 INFO] Loading train dataset from ../bert_data/cnndm.train.62.bert.pt, number of examples: 2001
[2020-04-02 07:08:15,021 INFO] Step 181550/210000; acc:  55.70; ppl:  7.71; xent: 2.04; lr: 0.00000469;   0/839 tok/s; 300530 sec
[2020-04-02 07:09:39,147 INFO] Step 181600/210000; acc:  51.21; ppl:  9.48; xent: 2.25; lr: 0.00000469;   0/819 tok/s; 300614 sec
[2020-04-02 07:10:38,785 INFO] Loading train dataset from ../bert_data/cnndm.train.63.bert.pt, number of examples: 2001
[2020-04-02 07:11:04,163 INFO] Step 181650/210000; acc:  53.84; ppl:  8.64; xent: 2.16; lr: 0.00000469;   0/808 tok/s; 300699 sec
[2020-04-02 07:12:28,233 INFO] Step 181700/210000; acc:  53.06; ppl:  8.30; xent: 2.12; lr: 0.00000469;   0/617 tok/s; 300783 sec
[2020-04-02 07:13:27,669 INFO] Loading train dataset from ../bert_data/cnndm.train.64.bert.pt, number of examples: 2001
[2020-04-02 07:13:52,939 INFO] Step 181750/210000; acc:  53.94; ppl:  8.52; xent: 2.14; lr: 0.00000469;   0/828 tok/s; 300868 sec
[2020-04-02 07:15:16,865 INFO] Step 181800/210000; acc:  54.40; ppl:  7.62; xent: 2.03; lr: 0.00000469;   0/725 tok/s; 300952 sec
[2020-04-02 07:16:14,991 INFO] Loading train dataset from ../bert_data/cnndm.train.65.bert.pt, number of examples: 2000
[2020-04-02 07:16:42,239 INFO] Step 181850/210000; acc:  54.08; ppl:  8.90; xent: 2.19; lr: 0.00000469;   0/1029 tok/s; 301037 sec
[2020-04-02 07:18:06,433 INFO] Step 181900/210000; acc:  50.75; ppl: 10.58; xent: 2.36; lr: 0.00000469;   0/965 tok/s; 301121 sec
[2020-04-02 07:19:02,723 INFO] Loading train dataset from ../bert_data/cnndm.train.66.bert.pt, number of examples: 2001
[2020-04-02 07:19:31,302 INFO] Step 181950/210000; acc:  51.84; ppl:  7.98; xent: 2.08; lr: 0.00000469;   0/601 tok/s; 301206 sec
[2020-04-02 07:20:55,709 INFO] Step 182000/210000; acc:  52.54; ppl:  9.11; xent: 2.21; lr: 0.00000469;   0/880 tok/s; 301291 sec
[2020-04-02 07:20:55,733 INFO] Saving checkpoint ../models/model_step_182000.pt
[2020-04-02 07:21:54,899 INFO] Loading train dataset from ../bert_data/cnndm.train.67.bert.pt, number of examples: 1999
[2020-04-02 07:22:23,576 INFO] Step 182050/210000; acc:  57.29; ppl:  7.61; xent: 2.03; lr: 0.00000469;   0/566 tok/s; 301378 sec
[2020-04-02 07:23:47,916 INFO] Step 182100/210000; acc:  56.66; ppl:  6.89; xent: 1.93; lr: 0.00000469;   0/692 tok/s; 301463 sec
[2020-04-02 07:24:42,576 INFO] Loading train dataset from ../bert_data/cnndm.train.68.bert.pt, number of examples: 1999
[2020-04-02 07:25:12,796 INFO] Step 182150/210000; acc:  49.87; ppl: 10.61; xent: 2.36; lr: 0.00000469;   0/692 tok/s; 301548 sec
[2020-04-02 07:26:37,465 INFO] Step 182200/210000; acc:  55.00; ppl:  8.55; xent: 2.15; lr: 0.00000469;   0/581 tok/s; 301632 sec
[2020-04-02 07:27:30,733 INFO] Loading train dataset from ../bert_data/cnndm.train.69.bert.pt, number of examples: 2000
[2020-04-02 07:28:02,836 INFO] Step 182250/210000; acc:  54.98; ppl:  7.63; xent: 2.03; lr: 0.00000468;   0/924 tok/s; 301718 sec
[2020-04-02 07:29:27,359 INFO] Step 182300/210000; acc:  55.85; ppl:  8.49; xent: 2.14; lr: 0.00000468;   0/889 tok/s; 301802 sec
[2020-04-02 07:30:18,509 INFO] Loading train dataset from ../bert_data/cnndm.train.7.bert.pt, number of examples: 2001
[2020-04-02 07:30:52,097 INFO] Step 182350/210000; acc:  50.38; ppl: 10.39; xent: 2.34; lr: 0.00000468;   0/858 tok/s; 301887 sec
[2020-04-02 07:32:16,173 INFO] Step 182400/210000; acc:  49.66; ppl: 11.30; xent: 2.42; lr: 0.00000468;   0/863 tok/s; 301971 sec
[2020-04-02 07:33:06,412 INFO] Loading train dataset from ../bert_data/cnndm.train.70.bert.pt, number of examples: 2000
[2020-04-02 07:33:41,978 INFO] Step 182450/210000; acc:  52.54; ppl:  9.15; xent: 2.21; lr: 0.00000468;   0/877 tok/s; 302057 sec
[2020-04-02 07:35:06,106 INFO] Step 182500/210000; acc:  54.50; ppl:  8.28; xent: 2.11; lr: 0.00000468;   0/836 tok/s; 302141 sec
[2020-04-02 07:35:06,129 INFO] Saving checkpoint ../models/model_step_182500.pt
[2020-04-02 07:35:57,732 INFO] Loading train dataset from ../bert_data/cnndm.train.71.bert.pt, number of examples: 1999
[2020-04-02 07:36:33,121 INFO] Step 182550/210000; acc:  53.52; ppl:  8.28; xent: 2.11; lr: 0.00000468;   0/592 tok/s; 302228 sec
[2020-04-02 07:37:57,712 INFO] Step 182600/210000; acc:  48.04; ppl: 10.33; xent: 2.34; lr: 0.00000468;   0/569 tok/s; 302313 sec
[2020-04-02 07:38:45,626 INFO] Loading train dataset from ../bert_data/cnndm.train.72.bert.pt, number of examples: 2000
[2020-04-02 07:39:22,662 INFO] Step 182650/210000; acc:  56.18; ppl:  8.32; xent: 2.12; lr: 0.00000468;   0/735 tok/s; 302397 sec
[2020-04-02 07:40:47,555 INFO] Step 182700/210000; acc:  53.92; ppl:  8.32; xent: 2.12; lr: 0.00000468;   0/705 tok/s; 302482 sec
[2020-04-02 07:41:33,866 INFO] Loading train dataset from ../bert_data/cnndm.train.73.bert.pt, number of examples: 2000
[2020-04-02 07:42:12,612 INFO] Step 182750/210000; acc:  55.67; ppl:  7.40; xent: 2.00; lr: 0.00000468;   0/695 tok/s; 302567 sec
[2020-04-02 07:43:36,655 INFO] Step 182800/210000; acc:  58.66; ppl:  6.79; xent: 1.92; lr: 0.00000468;   0/608 tok/s; 302651 sec
[2020-04-02 07:44:21,405 INFO] Loading train dataset from ../bert_data/cnndm.train.74.bert.pt, number of examples: 2000
[2020-04-02 07:45:01,993 INFO] Step 182850/210000; acc:  54.22; ppl:  8.39; xent: 2.13; lr: 0.00000468;   0/708 tok/s; 302737 sec
[2020-04-02 07:46:26,435 INFO] Step 182900/210000; acc:  50.74; ppl:  9.16; xent: 2.22; lr: 0.00000468;   0/543 tok/s; 302821 sec
[2020-04-02 07:47:10,830 INFO] Loading train dataset from ../bert_data/cnndm.train.75.bert.pt, number of examples: 1999
[2020-04-02 07:47:51,390 INFO] Step 182950/210000; acc:  51.93; ppl:  9.23; xent: 2.22; lr: 0.00000468;   0/786 tok/s; 302906 sec
[2020-04-02 07:49:16,023 INFO] Step 183000/210000; acc:  59.47; ppl:  7.34; xent: 1.99; lr: 0.00000468;   0/635 tok/s; 302991 sec
[2020-04-02 07:49:16,047 INFO] Saving checkpoint ../models/model_step_183000.pt
[2020-04-02 07:50:01,415 INFO] Loading train dataset from ../bert_data/cnndm.train.76.bert.pt, number of examples: 1999
[2020-04-02 07:50:43,938 INFO] Step 183050/210000; acc:  57.43; ppl:  6.99; xent: 1.94; lr: 0.00000467;   0/885 tok/s; 303079 sec
[2020-04-02 07:52:08,077 INFO] Step 183100/210000; acc:  58.55; ppl:  6.47; xent: 1.87; lr: 0.00000467;   0/901 tok/s; 303163 sec
[2020-04-02 07:52:49,167 INFO] Loading train dataset from ../bert_data/cnndm.train.77.bert.pt, number of examples: 2001
[2020-04-02 07:53:33,483 INFO] Step 183150/210000; acc:  49.79; ppl: 11.33; xent: 2.43; lr: 0.00000467;   0/655 tok/s; 303248 sec
[2020-04-02 07:54:57,797 INFO] Step 183200/210000; acc:  52.55; ppl:  8.20; xent: 2.10; lr: 0.00000467;   0/1055 tok/s; 303333 sec
[2020-04-02 07:55:37,383 INFO] Loading train dataset from ../bert_data/cnndm.train.78.bert.pt, number of examples: 2001
[2020-04-02 07:56:22,590 INFO] Step 183250/210000; acc:  55.66; ppl:  7.16; xent: 1.97; lr: 0.00000467;   0/536 tok/s; 303417 sec
[2020-04-02 07:57:46,908 INFO] Step 183300/210000; acc:  54.47; ppl:  7.56; xent: 2.02; lr: 0.00000467;   0/507 tok/s; 303502 sec
[2020-04-02 07:58:26,302 INFO] Loading train dataset from ../bert_data/cnndm.train.79.bert.pt, number of examples: 1999
[2020-04-02 07:59:11,545 INFO] Step 183350/210000; acc:  59.21; ppl:  6.45; xent: 1.86; lr: 0.00000467;   0/564 tok/s; 303586 sec
[2020-04-02 08:00:36,343 INFO] Step 183400/210000; acc:  59.09; ppl:  5.81; xent: 1.76; lr: 0.00000467;   0/631 tok/s; 303671 sec
[2020-04-02 08:01:13,830 INFO] Loading train dataset from ../bert_data/cnndm.train.8.bert.pt, number of examples: 2000
[2020-04-02 08:02:01,383 INFO] Step 183450/210000; acc:  55.27; ppl:  8.54; xent: 2.14; lr: 0.00000467;   0/678 tok/s; 303756 sec
[2020-04-02 08:03:26,258 INFO] Step 183500/210000; acc:  56.10; ppl:  8.21; xent: 2.11; lr: 0.00000467;   0/597 tok/s; 303841 sec
[2020-04-02 08:03:26,281 INFO] Saving checkpoint ../models/model_step_183500.pt
[2020-04-02 08:04:04,900 INFO] Loading train dataset from ../bert_data/cnndm.train.80.bert.pt, number of examples: 1999
[2020-04-02 08:04:53,926 INFO] Step 183550/210000; acc:  60.55; ppl:  6.20; xent: 1.82; lr: 0.00000467;   0/804 tok/s; 303929 sec
[2020-04-02 08:06:17,983 INFO] Step 183600/210000; acc:  58.01; ppl:  7.88; xent: 2.06; lr: 0.00000467;   0/704 tok/s; 304013 sec
[2020-04-02 08:06:52,271 INFO] Loading train dataset from ../bert_data/cnndm.train.81.bert.pt, number of examples: 2000
[2020-04-02 08:07:42,934 INFO] Step 183650/210000; acc:  56.96; ppl:  7.29; xent: 1.99; lr: 0.00000467;   0/893 tok/s; 304098 sec
[2020-04-02 08:09:07,061 INFO] Step 183700/210000; acc:  51.21; ppl:  9.94; xent: 2.30; lr: 0.00000467;   0/889 tok/s; 304182 sec
[2020-04-02 08:09:39,667 INFO] Loading train dataset from ../bert_data/cnndm.train.82.bert.pt, number of examples: 2001
[2020-04-02 08:10:32,127 INFO] Step 183750/210000; acc:  52.01; ppl:  9.42; xent: 2.24; lr: 0.00000467;   0/907 tok/s; 304267 sec
[2020-04-02 08:11:56,354 INFO] Step 183800/210000; acc:  50.80; ppl:  9.97; xent: 2.30; lr: 0.00000467;   0/846 tok/s; 304351 sec
[2020-04-02 08:12:29,243 INFO] Loading train dataset from ../bert_data/cnndm.train.83.bert.pt, number of examples: 1999
[2020-04-02 08:13:21,847 INFO] Step 183850/210000; acc:  47.55; ppl: 10.32; xent: 2.33; lr: 0.00000466;   0/680 tok/s; 304437 sec
[2020-04-02 08:14:46,499 INFO] Step 183900/210000; acc:  56.76; ppl:  6.79; xent: 1.92; lr: 0.00000466;   0/480 tok/s; 304521 sec
[2020-04-02 08:15:18,356 INFO] Loading train dataset from ../bert_data/cnndm.train.84.bert.pt, number of examples: 2000
[2020-04-02 08:16:12,270 INFO] Step 183950/210000; acc:  61.52; ppl:  5.88; xent: 1.77; lr: 0.00000466;   0/662 tok/s; 304607 sec
[2020-04-02 08:17:36,635 INFO] Step 184000/210000; acc:  59.96; ppl:  5.46; xent: 1.70; lr: 0.00000466;   0/639 tok/s; 304691 sec
[2020-04-02 08:17:36,662 INFO] Saving checkpoint ../models/model_step_184000.pt
[2020-04-02 08:18:08,541 INFO] Loading train dataset from ../bert_data/cnndm.train.85.bert.pt, number of examples: 2001
[2020-04-02 08:19:04,346 INFO] Step 184050/210000; acc:  53.97; ppl:  8.74; xent: 2.17; lr: 0.00000466;   0/686 tok/s; 304779 sec
[2020-04-02 08:20:28,813 INFO] Step 184100/210000; acc:  53.92; ppl:  7.45; xent: 2.01; lr: 0.00000466;   0/707 tok/s; 304864 sec
[2020-04-02 08:20:56,453 INFO] Loading train dataset from ../bert_data/cnndm.train.86.bert.pt, number of examples: 1999
[2020-04-02 08:21:53,508 INFO] Step 184150/210000; acc:  54.14; ppl:  8.31; xent: 2.12; lr: 0.00000466;   0/717 tok/s; 304948 sec
[2020-04-02 08:23:18,376 INFO] Step 184200/210000; acc:  53.66; ppl:  8.28; xent: 2.11; lr: 0.00000466;   0/676 tok/s; 305033 sec
[2020-04-02 08:23:44,524 INFO] Loading train dataset from ../bert_data/cnndm.train.87.bert.pt, number of examples: 1999
[2020-04-02 08:24:44,160 INFO] Step 184250/210000; acc:  56.73; ppl:  7.27; xent: 1.98; lr: 0.00000466;   0/866 tok/s; 305119 sec
[2020-04-02 08:26:08,556 INFO] Step 184300/210000; acc:  56.93; ppl:  7.27; xent: 1.98; lr: 0.00000466;   0/816 tok/s; 305203 sec
[2020-04-02 08:26:32,883 INFO] Loading train dataset from ../bert_data/cnndm.train.88.bert.pt, number of examples: 1999
[2020-04-02 08:27:33,542 INFO] Step 184350/210000; acc:  52.83; ppl:  8.24; xent: 2.11; lr: 0.00000466;   0/673 tok/s; 305288 sec
[2020-04-02 08:28:57,603 INFO] Step 184400/210000; acc:  56.68; ppl:  7.04; xent: 1.95; lr: 0.00000466;   0/775 tok/s; 305372 sec
[2020-04-02 08:29:20,334 INFO] Loading train dataset from ../bert_data/cnndm.train.89.bert.pt, number of examples: 2001
[2020-04-02 08:30:22,337 INFO] Step 184450/210000; acc:  53.15; ppl:  8.99; xent: 2.20; lr: 0.00000466;   0/823 tok/s; 305457 sec
[2020-04-02 08:31:46,652 INFO] Step 184500/210000; acc:  53.43; ppl:  9.13; xent: 2.21; lr: 0.00000466;   0/754 tok/s; 305541 sec
[2020-04-02 08:31:46,674 INFO] Saving checkpoint ../models/model_step_184500.pt
[2020-04-02 08:32:11,790 INFO] Loading train dataset from ../bert_data/cnndm.train.9.bert.pt, number of examples: 1999
[2020-04-02 08:33:14,325 INFO] Step 184550/210000; acc:  57.64; ppl:  7.30; xent: 1.99; lr: 0.00000466;   0/643 tok/s; 305629 sec
[2020-04-02 08:34:38,743 INFO] Step 184600/210000; acc:  52.56; ppl:  8.54; xent: 2.14; lr: 0.00000465;   0/600 tok/s; 305714 sec
[2020-04-02 08:34:59,736 INFO] Loading train dataset from ../bert_data/cnndm.train.90.bert.pt, number of examples: 2000
[2020-04-02 08:36:04,224 INFO] Step 184650/210000; acc:  54.70; ppl:  7.38; xent: 2.00; lr: 0.00000465;   0/686 tok/s; 305799 sec
[2020-04-02 08:37:28,940 INFO] Step 184700/210000; acc:  59.00; ppl:  6.90; xent: 1.93; lr: 0.00000465;   0/590 tok/s; 305884 sec
[2020-04-02 08:37:48,232 INFO] Loading train dataset from ../bert_data/cnndm.train.9000.bert.pt, number of examples: 1984
[2020-04-02 08:38:53,551 INFO] Step 184750/210000; acc:  43.21; ppl: 14.92; xent: 2.70; lr: 0.00000465;   0/930 tok/s; 305968 sec
[2020-04-02 08:40:15,888 INFO] Step 184800/210000; acc:  45.18; ppl: 11.57; xent: 2.45; lr: 0.00000465;   0/1020 tok/s; 306051 sec
[2020-04-02 08:40:25,116 INFO] Loading train dataset from ../bert_data/cnndm.train.9001.bert.pt, number of examples: 1983
[2020-04-02 08:41:39,217 INFO] Step 184850/210000; acc:  56.73; ppl:  6.17; xent: 1.82; lr: 0.00000465;   0/450 tok/s; 306134 sec
[2020-04-02 08:43:02,320 INFO] Step 184900/210000; acc:  50.18; ppl:  9.48; xent: 2.25; lr: 0.00000465;   0/839 tok/s; 306217 sec
[2020-04-02 08:43:04,652 INFO] Loading train dataset from ../bert_data/cnndm.train.9002.bert.pt, number of examples: 1990
[2020-04-02 08:44:25,576 INFO] Step 184950/210000; acc:  56.52; ppl:  6.55; xent: 1.88; lr: 0.00000465;   0/472 tok/s; 306300 sec
[2020-04-02 08:45:42,203 INFO] Loading train dataset from ../bert_data/cnndm.train.9003.bert.pt, number of examples: 1985
[2020-04-02 08:45:48,727 INFO] Step 185000/210000; acc:  51.44; ppl:  8.27; xent: 2.11; lr: 0.00000465;   0/1063 tok/s; 306384 sec
[2020-04-02 08:45:48,729 INFO] Saving checkpoint ../models/model_step_185000.pt
[2020-04-02 08:47:14,068 INFO] Step 185050/210000; acc:  53.30; ppl:  8.41; xent: 2.13; lr: 0.00000465;   0/587 tok/s; 306469 sec
[2020-04-02 08:48:21,308 INFO] Loading train dataset from ../bert_data/cnndm.train.9004.bert.pt, number of examples: 1981
[2020-04-02 08:48:38,283 INFO] Step 185100/210000; acc:  46.71; ppl: 11.63; xent: 2.45; lr: 0.00000465;   0/1200 tok/s; 306553 sec
[2020-04-02 08:50:01,336 INFO] Step 185150/210000; acc:  60.12; ppl:  5.51; xent: 1.71; lr: 0.00000465;   0/565 tok/s; 306636 sec
[2020-04-02 08:51:00,402 INFO] Loading train dataset from ../bert_data/cnndm.train.9005.bert.pt, number of examples: 1986
[2020-04-02 08:51:25,411 INFO] Step 185200/210000; acc:  46.54; ppl: 11.64; xent: 2.45; lr: 0.00000465;   0/1300 tok/s; 306720 sec
[2020-04-02 08:52:48,512 INFO] Step 185250/210000; acc:  58.22; ppl:  5.80; xent: 1.76; lr: 0.00000465;   0/765 tok/s; 306803 sec
[2020-04-02 08:53:39,492 INFO] Loading train dataset from ../bert_data/cnndm.train.9006.bert.pt, number of examples: 1993
[2020-04-02 08:54:12,530 INFO] Step 185300/210000; acc:  44.71; ppl: 12.75; xent: 2.55; lr: 0.00000465;   0/1069 tok/s; 306887 sec
[2020-04-02 08:55:35,843 INFO] Step 185350/210000; acc:  52.46; ppl:  8.80; xent: 2.18; lr: 0.00000465;   0/869 tok/s; 306971 sec
[2020-04-02 08:56:18,100 INFO] Loading train dataset from ../bert_data/cnndm.train.9007.bert.pt, number of examples: 1983
[2020-04-02 08:57:00,167 INFO] Step 185400/210000; acc:  48.38; ppl:  9.81; xent: 2.28; lr: 0.00000464;   0/881 tok/s; 307055 sec
[2020-04-02 08:58:22,402 INFO] Step 185450/210000; acc:  52.67; ppl:  7.89; xent: 2.07; lr: 0.00000464;   0/889 tok/s; 307137 sec
[2020-04-02 08:58:56,684 INFO] Loading train dataset from ../bert_data/cnndm.train.9008.bert.pt, number of examples: 1990
[2020-04-02 08:59:46,586 INFO] Step 185500/210000; acc:  53.65; ppl:  5.97; xent: 1.79; lr: 0.00000464;   0/457 tok/s; 307221 sec
[2020-04-02 08:59:46,590 INFO] Saving checkpoint ../models/model_step_185500.pt
[2020-04-02 09:01:11,684 INFO] Step 185550/210000; acc:  51.92; ppl:  8.48; xent: 2.14; lr: 0.00000464;   0/863 tok/s; 307307 sec
[2020-04-02 09:01:37,206 INFO] Loading train dataset from ../bert_data/cnndm.train.9009.bert.pt, number of examples: 1988
[2020-04-02 09:02:36,060 INFO] Step 185600/210000; acc:  58.32; ppl:  5.20; xent: 1.65; lr: 0.00000464;   0/446 tok/s; 307391 sec
[2020-04-02 09:04:00,480 INFO] Step 185650/210000; acc:  49.25; ppl: 10.43; xent: 2.34; lr: 0.00000464;   0/927 tok/s; 307475 sec
[2020-04-02 09:04:17,879 INFO] Loading train dataset from ../bert_data/cnndm.train.9010.bert.pt, number of examples: 1984
[2020-04-02 09:05:24,524 INFO] Step 185700/210000; acc:  53.91; ppl:  7.79; xent: 2.05; lr: 0.00000464;   0/681 tok/s; 307559 sec
[2020-04-02 09:06:47,425 INFO] Step 185750/210000; acc:  46.79; ppl: 11.33; xent: 2.43; lr: 0.00000464;   0/1179 tok/s; 307642 sec
[2020-04-02 09:06:54,650 INFO] Loading train dataset from ../bert_data/cnndm.train.9011.bert.pt, number of examples: 1983
[2020-04-02 09:08:11,237 INFO] Step 185800/210000; acc:  49.30; ppl:  9.26; xent: 2.23; lr: 0.00000464;   0/750 tok/s; 307726 sec
[2020-04-02 09:09:34,054 INFO] Step 185850/210000; acc:  49.93; ppl:  9.98; xent: 2.30; lr: 0.00000464;   0/1250 tok/s; 307809 sec
[2020-04-02 09:09:34,342 INFO] Loading train dataset from ../bert_data/cnndm.train.9012.bert.pt, number of examples: 1985
[2020-04-02 09:10:57,198 INFO] Step 185900/210000; acc:  57.76; ppl:  5.47; xent: 1.70; lr: 0.00000464;   0/670 tok/s; 307892 sec
[2020-04-02 09:12:11,796 INFO] Loading train dataset from ../bert_data/cnndm.train.9013.bert.pt, number of examples: 1987
[2020-04-02 09:12:22,231 INFO] Step 185950/210000; acc:  48.26; ppl:  9.61; xent: 2.26; lr: 0.00000464;   0/700 tok/s; 307977 sec
[2020-04-02 09:13:45,001 INFO] Step 186000/210000; acc:  48.32; ppl:  9.05; xent: 2.20; lr: 0.00000464;   0/876 tok/s; 308060 sec
[2020-04-02 09:13:45,005 INFO] Saving checkpoint ../models/model_step_186000.pt
[2020-04-02 09:14:52,764 INFO] Loading train dataset from ../bert_data/cnndm.train.9014.bert.pt, number of examples: 1976
[2020-04-02 09:15:11,316 INFO] Step 186050/210000; acc:  60.80; ppl:  5.10; xent: 1.63; lr: 0.00000464;   0/469 tok/s; 308146 sec
[2020-04-02 09:16:34,646 INFO] Step 186100/210000; acc:  47.17; ppl: 10.56; xent: 2.36; lr: 0.00000464;   0/1165 tok/s; 308229 sec
[2020-04-02 09:17:30,008 INFO] Loading train dataset from ../bert_data/cnndm.train.9015.bert.pt, number of examples: 1982
[2020-04-02 09:17:58,928 INFO] Step 186150/210000; acc:  52.43; ppl:  7.00; xent: 1.95; lr: 0.00000464;   0/832 tok/s; 308314 sec
[2020-04-02 09:19:21,457 INFO] Step 186200/210000; acc:  48.11; ppl: 10.46; xent: 2.35; lr: 0.00000463;   0/1208 tok/s; 308396 sec
[2020-04-02 09:20:06,854 INFO] Loading train dataset from ../bert_data/cnndm.train.9016.bert.pt, number of examples: 1984
[2020-04-02 09:20:44,433 INFO] Step 186250/210000; acc:  52.80; ppl:  7.95; xent: 2.07; lr: 0.00000463;   0/941 tok/s; 308479 sec
[2020-04-02 09:22:07,109 INFO] Step 186300/210000; acc:  45.92; ppl: 11.27; xent: 2.42; lr: 0.00000463;   0/1058 tok/s; 308562 sec
[2020-04-02 09:22:44,169 INFO] Loading train dataset from ../bert_data/cnndm.train.9017.bert.pt, number of examples: 1984
[2020-04-02 09:23:30,749 INFO] Step 186350/210000; acc:  46.45; ppl: 11.31; xent: 2.43; lr: 0.00000463;   0/1125 tok/s; 308646 sec
[2020-04-02 09:24:52,866 INFO] Step 186400/210000; acc:  55.96; ppl:  7.15; xent: 1.97; lr: 0.00000463;   0/928 tok/s; 308728 sec
[2020-04-02 09:25:23,336 INFO] Loading train dataset from ../bert_data/cnndm.train.9018.bert.pt, number of examples: 1989
[2020-04-02 09:26:16,725 INFO] Step 186450/210000; acc:  50.47; ppl:  8.90; xent: 2.19; lr: 0.00000463;   0/1081 tok/s; 308812 sec
[2020-04-02 09:27:39,602 INFO] Step 186500/210000; acc:  53.58; ppl:  7.65; xent: 2.03; lr: 0.00000463;   0/707 tok/s; 308894 sec
[2020-04-02 09:27:39,605 INFO] Saving checkpoint ../models/model_step_186500.pt
[2020-04-02 09:28:03,877 INFO] Loading train dataset from ../bert_data/cnndm.train.9019.bert.pt, number of examples: 1986
[2020-04-02 09:29:05,079 INFO] Step 186550/210000; acc:  48.46; ppl: 10.70; xent: 2.37; lr: 0.00000463;   0/1195 tok/s; 308980 sec
[2020-04-02 09:30:27,956 INFO] Step 186600/210000; acc:  60.96; ppl:  4.43; xent: 1.49; lr: 0.00000463;   0/489 tok/s; 309063 sec
[2020-04-02 09:30:42,112 INFO] Loading train dataset from ../bert_data/cnndm.train.9020.bert.pt, number of examples: 1992
[2020-04-02 09:31:50,651 INFO] Step 186650/210000; acc:  49.33; ppl:  9.68; xent: 2.27; lr: 0.00000463;   0/995 tok/s; 309145 sec
[2020-04-02 09:33:14,254 INFO] Step 186700/210000; acc:  55.78; ppl:  6.44; xent: 1.86; lr: 0.00000463;   0/511 tok/s; 309229 sec
[2020-04-02 09:33:19,926 INFO] Loading train dataset from ../bert_data/cnndm.train.9021.bert.pt, number of examples: 1988
[2020-04-02 09:34:37,699 INFO] Step 186750/210000; acc:  45.40; ppl: 11.83; xent: 2.47; lr: 0.00000463;   0/1065 tok/s; 309313 sec
[2020-04-02 09:35:57,988 INFO] Loading train dataset from ../bert_data/cnndm.train.9022.bert.pt, number of examples: 1991
[2020-04-02 09:36:01,265 INFO] Step 186800/210000; acc:  58.13; ppl:  6.03; xent: 1.80; lr: 0.00000463;   0/535 tok/s; 309396 sec
[2020-04-02 09:37:23,562 INFO] Step 186850/210000; acc:  47.89; ppl: 10.12; xent: 2.31; lr: 0.00000463;   0/840 tok/s; 309478 sec
[2020-04-02 09:38:37,199 INFO] Loading train dataset from ../bert_data/cnndm.train.9023.bert.pt, number of examples: 1986
[2020-04-02 09:38:47,004 INFO] Step 186900/210000; acc:  56.99; ppl:  6.01; xent: 1.79; lr: 0.00000463;   0/480 tok/s; 309562 sec
[2020-04-02 09:40:09,600 INFO] Step 186950/210000; acc:  45.33; ppl: 11.83; xent: 2.47; lr: 0.00000463;   0/1079 tok/s; 309644 sec
[2020-04-02 09:41:15,572 INFO] Loading train dataset from ../bert_data/cnndm.train.9024.bert.pt, number of examples: 1989
[2020-04-02 09:41:33,505 INFO] Step 187000/210000; acc:  56.32; ppl:  5.64; xent: 1.73; lr: 0.00000462;   0/521 tok/s; 309728 sec
[2020-04-02 09:41:33,508 INFO] Saving checkpoint ../models/model_step_187000.pt
[2020-04-02 09:42:58,705 INFO] Step 187050/210000; acc:  46.20; ppl: 10.51; xent: 2.35; lr: 0.00000462;   0/1227 tok/s; 309814 sec
[2020-04-02 09:43:55,775 INFO] Loading train dataset from ../bert_data/cnndm.train.9025.bert.pt, number of examples: 1984
[2020-04-02 09:44:22,130 INFO] Step 187100/210000; acc:  52.15; ppl:  8.05; xent: 2.09; lr: 0.00000462;   0/583 tok/s; 309897 sec
[2020-04-02 09:45:44,831 INFO] Step 187150/210000; acc:  46.35; ppl: 11.71; xent: 2.46; lr: 0.00000462;   0/1113 tok/s; 309980 sec
[2020-04-02 09:46:31,947 INFO] Loading train dataset from ../bert_data/cnndm.train.9026.bert.pt, number of examples: 1986
[2020-04-02 09:47:08,853 INFO] Step 187200/210000; acc:  55.15; ppl:  6.06; xent: 1.80; lr: 0.00000462;   0/768 tok/s; 310064 sec
[2020-04-02 09:48:32,117 INFO] Step 187250/210000; acc:  47.16; ppl: 11.46; xent: 2.44; lr: 0.00000462;   0/1129 tok/s; 310147 sec
[2020-04-02 09:49:10,912 INFO] Loading train dataset from ../bert_data/cnndm.train.9027.bert.pt, number of examples: 1989
[2020-04-02 09:49:55,597 INFO] Step 187300/210000; acc:  52.22; ppl:  8.15; xent: 2.10; lr: 0.00000462;   0/896 tok/s; 310230 sec
[2020-04-02 09:51:18,732 INFO] Step 187350/210000; acc:  46.74; ppl: 10.83; xent: 2.38; lr: 0.00000462;   0/1310 tok/s; 310314 sec
[2020-04-02 09:51:49,038 INFO] Loading train dataset from ../bert_data/cnndm.train.9028.bert.pt, number of examples: 1986
[2020-04-02 09:52:42,465 INFO] Step 187400/210000; acc:  49.64; ppl:  9.88; xent: 2.29; lr: 0.00000462;   0/826 tok/s; 310397 sec
[2020-04-02 09:54:05,667 INFO] Step 187450/210000; acc:  45.57; ppl: 13.22; xent: 2.58; lr: 0.00000462;   0/1204 tok/s; 310480 sec
[2020-04-02 09:54:29,791 INFO] Loading train dataset from ../bert_data/cnndm.train.9029.bert.pt, number of examples: 1981
[2020-04-02 09:55:30,111 INFO] Step 187500/210000; acc:  53.45; ppl:  7.30; xent: 1.99; lr: 0.00000462;   0/862 tok/s; 310565 sec
[2020-04-02 09:55:30,115 INFO] Saving checkpoint ../models/model_step_187500.pt
[2020-04-02 09:56:56,974 INFO] Step 187550/210000; acc:  46.67; ppl: 10.63; xent: 2.36; lr: 0.00000462;   0/1016 tok/s; 310652 sec
[2020-04-02 09:57:10,751 INFO] Loading train dataset from ../bert_data/cnndm.train.9030.bert.pt, number of examples: 1984
[2020-04-02 09:58:20,786 INFO] Step 187600/210000; acc:  44.83; ppl: 11.76; xent: 2.47; lr: 0.00000462;   0/946 tok/s; 310736 sec
[2020-04-02 09:59:43,591 INFO] Step 187650/210000; acc:  57.79; ppl:  5.89; xent: 1.77; lr: 0.00000462;   0/458 tok/s; 310818 sec
[2020-04-02 09:59:49,216 INFO] Loading train dataset from ../bert_data/cnndm.train.9031.bert.pt, number of examples: 1975
[2020-04-02 10:01:08,095 INFO] Step 187700/210000; acc:  53.37; ppl:  7.61; xent: 2.03; lr: 0.00000462;   0/1025 tok/s; 310903 sec
[2020-04-02 10:02:26,490 INFO] Loading train dataset from ../bert_data/cnndm.train.9032.bert.pt, number of examples: 1986
[2020-04-02 10:02:33,086 INFO] Step 187750/210000; acc:  47.81; ppl: 10.31; xent: 2.33; lr: 0.00000462;   0/989 tok/s; 310988 sec
[2020-04-02 10:03:56,416 INFO] Step 187800/210000; acc:  51.28; ppl:  8.37; xent: 2.12; lr: 0.00000462;   0/687 tok/s; 311071 sec
[2020-04-02 10:05:05,041 INFO] Loading train dataset from ../bert_data/cnndm.train.9033.bert.pt, number of examples: 1986
[2020-04-02 10:05:20,187 INFO] Step 187850/210000; acc:  52.32; ppl:  7.94; xent: 2.07; lr: 0.00000461;   0/1145 tok/s; 311155 sec
[2020-04-02 10:06:42,410 INFO] Step 187900/210000; acc:  60.63; ppl:  4.77; xent: 1.56; lr: 0.00000461;   0/416 tok/s; 311237 sec
[2020-04-02 10:07:42,382 INFO] Loading train dataset from ../bert_data/cnndm.train.9034.bert.pt, number of examples: 1989
[2020-04-02 10:08:04,901 INFO] Step 187950/210000; acc:  44.34; ppl: 11.27; xent: 2.42; lr: 0.00000461;   0/910 tok/s; 311320 sec
[2020-04-02 10:09:28,452 INFO] Step 188000/210000; acc:  55.56; ppl:  6.17; xent: 1.82; lr: 0.00000461;   0/586 tok/s; 311403 sec
[2020-04-02 10:09:28,455 INFO] Saving checkpoint ../models/model_step_188000.pt
[2020-04-02 10:10:22,150 INFO] Loading train dataset from ../bert_data/cnndm.train.9035.bert.pt, number of examples: 1988
[2020-04-02 10:10:53,532 INFO] Step 188050/210000; acc:  48.29; ppl: 10.71; xent: 2.37; lr: 0.00000461;   0/1120 tok/s; 311488 sec
[2020-04-02 10:12:15,412 INFO] Step 188100/210000; acc:  53.61; ppl:  7.08; xent: 1.96; lr: 0.00000461;   0/529 tok/s; 311570 sec
[2020-04-02 10:12:58,829 INFO] Loading train dataset from ../bert_data/cnndm.train.9036.bert.pt, number of examples: 1984
[2020-04-02 10:13:38,788 INFO] Step 188150/210000; acc:  46.54; ppl: 11.47; xent: 2.44; lr: 0.00000461;   0/1190 tok/s; 311654 sec
[2020-04-02 10:15:02,615 INFO] Step 188200/210000; acc:  51.86; ppl:  8.36; xent: 2.12; lr: 0.00000461;   0/718 tok/s; 311737 sec
[2020-04-02 10:15:36,968 INFO] Loading train dataset from ../bert_data/cnndm.train.9037.bert.pt, number of examples: 1983
[2020-04-02 10:16:26,533 INFO] Step 188250/210000; acc:  61.90; ppl:  4.43; xent: 1.49; lr: 0.00000461;   0/546 tok/s; 311821 sec
[2020-04-02 10:17:49,343 INFO] Step 188300/210000; acc:  48.90; ppl:  9.30; xent: 2.23; lr: 0.00000461;   0/918 tok/s; 311904 sec
[2020-04-02 10:18:12,977 INFO] Loading train dataset from ../bert_data/cnndm.train.9038.bert.pt, number of examples: 1982
[2020-04-02 10:19:12,992 INFO] Step 188350/210000; acc:  57.34; ppl:  5.40; xent: 1.69; lr: 0.00000461;   0/613 tok/s; 311988 sec
[2020-04-02 10:20:36,399 INFO] Step 188400/210000; acc:  50.66; ppl:  8.67; xent: 2.16; lr: 0.00000461;   0/1202 tok/s; 312071 sec
[2020-04-02 10:20:51,618 INFO] Loading train dataset from ../bert_data/cnndm.train.9039.bert.pt, number of examples: 1983
[2020-04-02 10:21:59,149 INFO] Step 188450/210000; acc:  54.60; ppl:  6.60; xent: 1.89; lr: 0.00000461;   0/631 tok/s; 312154 sec
[2020-04-02 10:23:21,341 INFO] Step 188500/210000; acc:  46.06; ppl: 11.89; xent: 2.48; lr: 0.00000461;   0/1270 tok/s; 312236 sec
[2020-04-02 10:23:21,345 INFO] Saving checkpoint ../models/model_step_188500.pt
[2020-04-02 10:23:31,194 INFO] Loading train dataset from ../bert_data/cnndm.train.9040.bert.pt, number of examples: 1982
[2020-04-02 10:24:48,532 INFO] Step 188550/210000; acc:  48.09; ppl:  9.57; xent: 2.26; lr: 0.00000461;   0/1128 tok/s; 312323 sec
[2020-04-02 10:26:08,870 INFO] Loading train dataset from ../bert_data/cnndm.train.9041.bert.pt, number of examples: 1986
[2020-04-02 10:26:12,455 INFO] Step 188600/210000; acc:  56.85; ppl:  5.94; xent: 1.78; lr: 0.00000461;   0/566 tok/s; 312407 sec
[2020-04-02 10:27:34,976 INFO] Step 188650/210000; acc:  48.82; ppl:  9.73; xent: 2.28; lr: 0.00000460;   0/1182 tok/s; 312490 sec
[2020-04-02 10:28:46,812 INFO] Loading train dataset from ../bert_data/cnndm.train.9042.bert.pt, number of examples: 1984
[2020-04-02 10:28:58,333 INFO] Step 188700/210000; acc:  59.19; ppl:  5.92; xent: 1.78; lr: 0.00000460;   0/602 tok/s; 312573 sec
[2020-04-02 10:30:20,622 INFO] Step 188750/210000; acc:  48.71; ppl:  9.50; xent: 2.25; lr: 0.00000460;   0/1035 tok/s; 312655 sec
[2020-04-02 10:31:24,256 INFO] Loading train dataset from ../bert_data/cnndm.train.9043.bert.pt, number of examples: 1984
[2020-04-02 10:31:44,174 INFO] Step 188800/210000; acc:  56.33; ppl:  6.21; xent: 1.83; lr: 0.00000460;   0/484 tok/s; 312739 sec
[2020-04-02 10:33:07,331 INFO] Step 188850/210000; acc:  47.83; ppl: 10.41; xent: 2.34; lr: 0.00000460;   0/1167 tok/s; 312822 sec
[2020-04-02 10:34:02,839 INFO] Loading train dataset from ../bert_data/cnndm.train.9044.bert.pt, number of examples: 1988
[2020-04-02 10:34:31,106 INFO] Step 188900/210000; acc:  53.12; ppl:  7.30; xent: 1.99; lr: 0.00000460;   0/705 tok/s; 312906 sec
[2020-04-02 10:35:53,827 INFO] Step 188950/210000; acc:  42.86; ppl: 13.31; xent: 2.59; lr: 0.00000460;   0/1131 tok/s; 312989 sec
[2020-04-02 10:36:41,154 INFO] Loading train dataset from ../bert_data/cnndm.train.9045.bert.pt, number of examples: 1983
[2020-04-02 10:37:17,236 INFO] Step 189000/210000; acc:  53.12; ppl:  7.29; xent: 1.99; lr: 0.00000460;   0/812 tok/s; 313072 sec
[2020-04-02 10:37:17,240 INFO] Saving checkpoint ../models/model_step_189000.pt
[2020-04-02 10:38:42,833 INFO] Step 189050/210000; acc:  52.50; ppl:  9.36; xent: 2.24; lr: 0.00000460;   0/751 tok/s; 313158 sec
[2020-04-02 10:39:19,594 INFO] Loading train dataset from ../bert_data/cnndm.train.9046.bert.pt, number of examples: 1983
[2020-04-02 10:40:05,945 INFO] Step 189100/210000; acc:  50.83; ppl:  8.74; xent: 2.17; lr: 0.00000460;   0/1032 tok/s; 313241 sec
[2020-04-02 10:41:29,534 INFO] Step 189150/210000; acc:  57.95; ppl:  6.13; xent: 1.81; lr: 0.00000460;   0/477 tok/s; 313324 sec
[2020-04-02 10:41:58,391 INFO] Loading train dataset from ../bert_data/cnndm.train.9047.bert.pt, number of examples: 1983
[2020-04-02 10:42:53,411 INFO] Step 189200/210000; acc:  51.91; ppl:  8.26; xent: 2.11; lr: 0.00000460;   0/1084 tok/s; 313408 sec
[2020-04-02 10:44:16,037 INFO] Step 189250/210000; acc:  53.00; ppl:  6.37; xent: 1.85; lr: 0.00000460;   0/507 tok/s; 313491 sec
[2020-04-02 10:44:36,673 INFO] Loading train dataset from ../bert_data/cnndm.train.9048.bert.pt, number of examples: 1984
[2020-04-02 10:45:39,046 INFO] Step 189300/210000; acc:  47.83; ppl:  9.57; xent: 2.26; lr: 0.00000460;   0/1320 tok/s; 313574 sec
[2020-04-02 10:47:01,321 INFO] Step 189350/210000; acc:  54.33; ppl:  6.27; xent: 1.84; lr: 0.00000460;   0/583 tok/s; 313656 sec
[2020-04-02 10:47:13,727 INFO] Loading train dataset from ../bert_data/cnndm.train.9049.bert.pt, number of examples: 1978
[2020-04-02 10:48:25,950 INFO] Step 189400/210000; acc:  47.83; ppl: 10.48; xent: 2.35; lr: 0.00000460;   0/950 tok/s; 313741 sec
[2020-04-02 10:49:48,069 INFO] Step 189450/210000; acc:  54.48; ppl:  6.90; xent: 1.93; lr: 0.00000459;   0/934 tok/s; 313823 sec
[2020-04-02 10:49:50,344 INFO] Loading train dataset from ../bert_data/cnndm.train.9050.bert.pt, number of examples: 1978
[2020-04-02 10:51:11,759 INFO] Step 189500/210000; acc:  53.62; ppl:  8.28; xent: 2.11; lr: 0.00000459;   0/691 tok/s; 313907 sec
[2020-04-02 10:51:11,762 INFO] Saving checkpoint ../models/model_step_189500.pt
[2020-04-02 10:52:30,150 INFO] Loading train dataset from ../bert_data/cnndm.train.9051.bert.pt, number of examples: 1981
[2020-04-02 10:52:36,879 INFO] Step 189550/210000; acc:  50.63; ppl:  8.80; xent: 2.17; lr: 0.00000459;   0/967 tok/s; 313992 sec
[2020-04-02 10:54:00,137 INFO] Step 189600/210000; acc:  58.99; ppl:  5.52; xent: 1.71; lr: 0.00000459;   0/585 tok/s; 314075 sec
[2020-04-02 10:55:06,776 INFO] Loading train dataset from ../bert_data/cnndm.train.9052.bert.pt, number of examples: 1981
[2020-04-02 10:55:23,391 INFO] Step 189650/210000; acc:  51.75; ppl:  8.13; xent: 2.10; lr: 0.00000459;   0/1170 tok/s; 314158 sec
[2020-04-02 10:56:46,257 INFO] Step 189700/210000; acc:  50.73; ppl:  8.30; xent: 2.12; lr: 0.00000459;   0/827 tok/s; 314241 sec
[2020-04-02 10:57:43,354 INFO] Loading train dataset from ../bert_data/cnndm.train.9053.bert.pt, number of examples: 1981
[2020-04-02 10:58:09,438 INFO] Step 189750/210000; acc:  59.87; ppl:  5.27; xent: 1.66; lr: 0.00000459;   0/525 tok/s; 314324 sec
[2020-04-02 10:59:32,049 INFO] Step 189800/210000; acc:  49.23; ppl:  9.77; xent: 2.28; lr: 0.00000459;   0/960 tok/s; 314407 sec
[2020-04-02 11:00:20,705 INFO] Loading train dataset from ../bert_data/cnndm.train.9054.bert.pt, number of examples: 1987
[2020-04-02 11:00:55,413 INFO] Step 189850/210000; acc:  59.08; ppl:  5.26; xent: 1.66; lr: 0.00000459;   0/501 tok/s; 314490 sec
[2020-04-02 11:02:18,917 INFO] Step 189900/210000; acc:  50.89; ppl:  8.88; xent: 2.18; lr: 0.00000459;   0/1080 tok/s; 314574 sec
[2020-04-02 11:02:59,601 INFO] Loading train dataset from ../bert_data/cnndm.train.9055.bert.pt, number of examples: 1984
[2020-04-02 11:03:43,258 INFO] Step 189950/210000; acc:  57.62; ppl:  5.62; xent: 1.73; lr: 0.00000459;   0/553 tok/s; 314658 sec
[2020-04-02 11:05:06,259 INFO] Step 190000/210000; acc:  50.54; ppl:  8.87; xent: 2.18; lr: 0.00000459;   0/1085 tok/s; 314741 sec
[2020-04-02 11:05:06,262 INFO] Saving checkpoint ../models/model_step_190000.pt
[2020-04-02 11:05:39,160 INFO] Loading train dataset from ../bert_data/cnndm.train.9056.bert.pt, number of examples: 1985
[2020-04-02 11:06:31,951 INFO] Step 190050/210000; acc:  58.51; ppl:  5.10; xent: 1.63; lr: 0.00000459;   0/673 tok/s; 314827 sec
[2020-04-02 11:07:56,237 INFO] Step 190100/210000; acc:  43.83; ppl: 12.61; xent: 2.53; lr: 0.00000459;   0/1207 tok/s; 314911 sec
[2020-04-02 11:08:18,235 INFO] Loading train dataset from ../bert_data/cnndm.train.9057.bert.pt, number of examples: 1986
[2020-04-02 11:09:19,230 INFO] Step 190150/210000; acc:  55.20; ppl:  6.04; xent: 1.80; lr: 0.00000459;   0/664 tok/s; 314994 sec
[2020-04-02 11:10:42,940 INFO] Step 190200/210000; acc:  55.99; ppl:  6.68; xent: 1.90; lr: 0.00000459;   0/758 tok/s; 315078 sec
[2020-04-02 11:10:56,663 INFO] Loading train dataset from ../bert_data/cnndm.train.9058.bert.pt, number of examples: 1979
[2020-04-02 11:12:06,596 INFO] Step 190250/210000; acc:  50.09; ppl:  8.60; xent: 2.15; lr: 0.00000459;   0/1030 tok/s; 315161 sec
[2020-04-02 11:13:30,033 INFO] Step 190300/210000; acc:  61.51; ppl:  4.83; xent: 1.57; lr: 0.00000458;   0/518 tok/s; 315245 sec
[2020-04-02 11:13:33,836 INFO] Loading train dataset from ../bert_data/cnndm.train.9059.bert.pt, number of examples: 1987
[2020-04-02 11:14:53,551 INFO] Step 190350/210000; acc:  48.19; ppl:  9.83; xent: 2.28; lr: 0.00000458;   0/1217 tok/s; 315328 sec
[2020-04-02 11:16:11,959 INFO] Loading train dataset from ../bert_data/cnndm.train.9060.bert.pt, number of examples: 1992
[2020-04-02 11:16:16,955 INFO] Step 190400/210000; acc:  48.98; ppl:  9.03; xent: 2.20; lr: 0.00000458;   0/786 tok/s; 315412 sec
[2020-04-02 11:17:39,853 INFO] Step 190450/210000; acc:  45.97; ppl:  9.85; xent: 2.29; lr: 0.00000458;   0/1097 tok/s; 315495 sec
[2020-04-02 11:18:50,390 INFO] Loading train dataset from ../bert_data/cnndm.train.9061.bert.pt, number of examples: 1985
[2020-04-02 11:19:03,925 INFO] Step 190500/210000; acc:  53.48; ppl:  6.63; xent: 1.89; lr: 0.00000458;   0/791 tok/s; 315579 sec
[2020-04-02 11:19:03,928 INFO] Saving checkpoint ../models/model_step_190500.pt
[2020-04-02 11:20:29,164 INFO] Step 190550/210000; acc:  43.85; ppl: 12.15; xent: 2.50; lr: 0.00000458;   0/1135 tok/s; 315664 sec
[2020-04-02 11:21:33,724 INFO] Loading train dataset from ../bert_data/cnndm.train.9062.bert.pt, number of examples: 1982
[2020-04-02 11:21:53,168 INFO] Step 190600/210000; acc:  58.29; ppl:  5.64; xent: 1.73; lr: 0.00000458;   0/639 tok/s; 315748 sec
[2020-04-02 11:23:16,090 INFO] Step 190650/210000; acc:  48.97; ppl:  9.51; xent: 2.25; lr: 0.00000458;   0/1193 tok/s; 315831 sec
[2020-04-02 11:24:10,847 INFO] Loading train dataset from ../bert_data/cnndm.train.9063.bert.pt, number of examples: 1988
[2020-04-02 11:24:38,861 INFO] Step 190700/210000; acc:  54.79; ppl:  7.34; xent: 1.99; lr: 0.00000458;   0/867 tok/s; 315914 sec
[2020-04-02 11:26:01,927 INFO] Step 190750/210000; acc:  50.97; ppl:  8.42; xent: 2.13; lr: 0.00000458;   0/917 tok/s; 315997 sec
[2020-04-02 11:26:47,486 INFO] Loading train dataset from ../bert_data/cnndm.train.9064.bert.pt, number of examples: 1984
[2020-04-02 11:27:25,019 INFO] Step 190800/210000; acc:  51.34; ppl:  8.98; xent: 2.19; lr: 0.00000458;   0/925 tok/s; 316080 sec
[2020-04-02 11:28:47,575 INFO] Step 190850/210000; acc:  52.98; ppl:  7.70; xent: 2.04; lr: 0.00000458;   0/775 tok/s; 316162 sec
[2020-04-02 11:29:24,422 INFO] Loading train dataset from ../bert_data/cnndm.train.9065.bert.pt, number of examples: 1986
[2020-04-02 11:30:11,296 INFO] Step 190900/210000; acc:  50.44; ppl:  8.54; xent: 2.14; lr: 0.00000458;   0/1120 tok/s; 316246 sec
[2020-04-02 11:31:33,441 INFO] Step 190950/210000; acc:  57.28; ppl:  5.55; xent: 1.71; lr: 0.00000458;   0/441 tok/s; 316328 sec
[2020-04-02 11:32:01,732 INFO] Loading train dataset from ../bert_data/cnndm.train.9066.bert.pt, number of examples: 1988
[2020-04-02 11:32:57,242 INFO] Step 191000/210000; acc:  47.28; ppl: 10.40; xent: 2.34; lr: 0.00000458;   0/1196 tok/s; 316412 sec
[2020-04-02 11:32:57,246 INFO] Saving checkpoint ../models/model_step_191000.pt
[2020-04-02 11:34:21,991 INFO] Step 191050/210000; acc:  59.68; ppl:  4.74; xent: 1.56; lr: 0.00000458;   0/422 tok/s; 316497 sec
[2020-04-02 11:34:42,470 INFO] Loading train dataset from ../bert_data/cnndm.train.9067.bert.pt, number of examples: 1986
[2020-04-02 11:35:44,674 INFO] Step 191100/210000; acc:  50.00; ppl:  9.71; xent: 2.27; lr: 0.00000458;   0/1096 tok/s; 316579 sec
[2020-04-02 11:37:07,325 INFO] Step 191150/210000; acc:  63.68; ppl:  4.09; xent: 1.41; lr: 0.00000457;   0/494 tok/s; 316662 sec
[2020-04-02 11:37:19,369 INFO] Loading train dataset from ../bert_data/cnndm.train.9068.bert.pt, number of examples: 1988
[2020-04-02 11:38:30,908 INFO] Step 191200/210000; acc:  49.07; ppl:  9.62; xent: 2.26; lr: 0.00000457;   0/1033 tok/s; 316746 sec
[2020-04-02 11:39:53,676 INFO] Step 191250/210000; acc:  56.37; ppl:  6.03; xent: 1.80; lr: 0.00000457;   0/679 tok/s; 316828 sec
[2020-04-02 11:39:57,632 INFO] Loading train dataset from ../bert_data/cnndm.train.9069.bert.pt, number of examples: 1982
[2020-04-02 11:41:17,454 INFO] Step 191300/210000; acc:  48.21; ppl: 10.38; xent: 2.34; lr: 0.00000457;   0/1228 tok/s; 316912 sec
[2020-04-02 11:42:34,459 INFO] Loading train dataset from ../bert_data/cnndm.train.9070.bert.pt, number of examples: 1988
[2020-04-02 11:42:41,212 INFO] Step 191350/210000; acc:  48.05; ppl:  9.21; xent: 2.22; lr: 0.00000457;   0/1139 tok/s; 316996 sec
[2020-04-02 11:44:03,998 INFO] Step 191400/210000; acc:  60.58; ppl:  4.54; xent: 1.51; lr: 0.00000457;   0/427 tok/s; 317079 sec
[2020-04-02 11:45:12,772 INFO] Loading train dataset from ../bert_data/cnndm.train.9071.bert.pt, number of examples: 1988
[2020-04-02 11:45:27,697 INFO] Step 191450/210000; acc:  45.14; ppl: 12.82; xent: 2.55; lr: 0.00000457;   0/1202 tok/s; 317163 sec
[2020-04-02 11:46:50,862 INFO] Step 191500/210000; acc:  62.25; ppl:  4.36; xent: 1.47; lr: 0.00000457;   0/417 tok/s; 317246 sec
[2020-04-02 11:46:50,866 INFO] Saving checkpoint ../models/model_step_191500.pt
[2020-04-02 11:47:53,328 INFO] Loading train dataset from ../bert_data/cnndm.train.9072.bert.pt, number of examples: 1992
[2020-04-02 11:48:16,745 INFO] Step 191550/210000; acc:  51.29; ppl:  8.72; xent: 2.17; lr: 0.00000457;   0/939 tok/s; 317332 sec
[2020-04-02 11:49:39,613 INFO] Step 191600/210000; acc:  65.47; ppl:  3.77; xent: 1.33; lr: 0.00000457;   0/445 tok/s; 317414 sec
[2020-04-02 11:50:32,031 INFO] Loading train dataset from ../bert_data/cnndm.train.9073.bert.pt, number of examples: 1987
[2020-04-02 11:51:03,720 INFO] Step 191650/210000; acc:  44.37; ppl: 11.90; xent: 2.48; lr: 0.00000457;   0/1096 tok/s; 317499 sec
[2020-04-02 11:52:27,460 INFO] Step 191700/210000; acc:  63.55; ppl:  4.84; xent: 1.58; lr: 0.00000457;   0/533 tok/s; 317582 sec
[2020-04-02 11:53:10,857 INFO] Loading train dataset from ../bert_data/cnndm.train.9074.bert.pt, number of examples: 1991
[2020-04-02 11:53:50,580 INFO] Step 191750/210000; acc:  48.70; ppl: 10.63; xent: 2.36; lr: 0.00000457;   0/1238 tok/s; 317665 sec
[2020-04-02 11:55:13,876 INFO] Step 191800/210000; acc:  55.12; ppl:  6.81; xent: 1.92; lr: 0.00000457;   0/656 tok/s; 317749 sec
[2020-04-02 11:55:49,004 INFO] Loading train dataset from ../bert_data/cnndm.train.9075.bert.pt, number of examples: 1983
[2020-04-02 11:56:37,580 INFO] Step 191850/210000; acc:  47.19; ppl:  9.59; xent: 2.26; lr: 0.00000457;   0/1305 tok/s; 317832 sec
[2020-04-02 11:58:00,347 INFO] Step 191900/210000; acc:  56.27; ppl:  5.93; xent: 1.78; lr: 0.00000457;   0/716 tok/s; 317915 sec
[2020-04-02 11:58:26,998 INFO] Loading train dataset from ../bert_data/cnndm.train.9076.bert.pt, number of examples: 1986
[2020-04-02 11:59:23,291 INFO] Step 191950/210000; acc:  48.19; ppl:  9.29; xent: 2.23; lr: 0.00000456;   0/1177 tok/s; 317998 sec
[2020-04-02 12:00:45,551 INFO] Step 192000/210000; acc:  52.83; ppl:  7.58; xent: 2.03; lr: 0.00000456;   0/702 tok/s; 318080 sec
[2020-04-02 12:00:45,555 INFO] Saving checkpoint ../models/model_step_192000.pt
[2020-04-02 12:01:06,876 INFO] Loading train dataset from ../bert_data/cnndm.train.9077.bert.pt, number of examples: 1990
[2020-04-02 12:02:11,756 INFO] Step 192050/210000; acc:  52.64; ppl:  8.69; xent: 2.16; lr: 0.00000456;   0/714 tok/s; 318167 sec
[2020-04-02 12:03:34,480 INFO] Step 192100/210000; acc:  49.85; ppl:  9.17; xent: 2.22; lr: 0.00000456;   0/1084 tok/s; 318249 sec
[2020-04-02 12:03:45,463 INFO] Loading train dataset from ../bert_data/cnndm.train.9078.bert.pt, number of examples: 1992
[2020-04-02 12:04:57,488 INFO] Step 192150/210000; acc:  46.09; ppl: 10.77; xent: 2.38; lr: 0.00000456;   0/772 tok/s; 318332 sec
[2020-04-02 12:06:20,734 INFO] Step 192200/210000; acc:  54.81; ppl:  6.96; xent: 1.94; lr: 0.00000456;   0/764 tok/s; 318416 sec
[2020-04-02 12:06:24,662 INFO] Loading train dataset from ../bert_data/cnndm.train.9079.bert.pt, number of examples: 1984
[2020-04-02 12:07:44,399 INFO] Step 192250/210000; acc:  49.58; ppl:  9.02; xent: 2.20; lr: 0.00000456;   0/790 tok/s; 318499 sec
[2020-04-02 12:09:00,665 INFO] Loading train dataset from ../bert_data/cnndm.train.9080.bert.pt, number of examples: 1988
[2020-04-02 12:09:07,488 INFO] Step 192300/210000; acc:  51.04; ppl:  8.88; xent: 2.18; lr: 0.00000456;   0/1269 tok/s; 318582 sec
[2020-04-02 12:10:29,946 INFO] Step 192350/210000; acc:  58.78; ppl:  5.32; xent: 1.67; lr: 0.00000456;   0/575 tok/s; 318665 sec
[2020-04-02 12:11:38,718 INFO] Loading train dataset from ../bert_data/cnndm.train.9081.bert.pt, number of examples: 1991
[2020-04-02 12:11:53,746 INFO] Step 192400/210000; acc:  47.05; ppl: 11.28; xent: 2.42; lr: 0.00000456;   0/1038 tok/s; 318749 sec
[2020-04-02 12:13:16,757 INFO] Step 192450/210000; acc:  54.88; ppl:  6.68; xent: 1.90; lr: 0.00000456;   0/591 tok/s; 318832 sec
[2020-04-02 12:14:17,363 INFO] Loading train dataset from ../bert_data/cnndm.train.9082.bert.pt, number of examples: 1985
[2020-04-02 12:14:40,236 INFO] Step 192500/210000; acc:  46.01; ppl: 10.90; xent: 2.39; lr: 0.00000456;   0/1036 tok/s; 318915 sec
[2020-04-02 12:14:40,241 INFO] Saving checkpoint ../models/model_step_192500.pt
[2020-04-02 12:16:05,224 INFO] Step 192550/210000; acc:  56.25; ppl:  6.04; xent: 1.80; lr: 0.00000456;   0/598 tok/s; 319000 sec
[2020-04-02 12:16:55,243 INFO] Loading train dataset from ../bert_data/cnndm.train.9083.bert.pt, number of examples: 1986
[2020-04-02 12:17:28,338 INFO] Step 192600/210000; acc:  47.22; ppl: 12.01; xent: 2.49; lr: 0.00000456;   0/1282 tok/s; 319083 sec
[2020-04-02 12:18:51,215 INFO] Step 192650/210000; acc:  56.17; ppl:  5.88; xent: 1.77; lr: 0.00000456;   0/625 tok/s; 319166 sec
[2020-04-02 12:19:35,427 INFO] Loading train dataset from ../bert_data/cnndm.train.9084.bert.pt, number of examples: 1982
[2020-04-02 12:20:15,679 INFO] Step 192700/210000; acc:  49.82; ppl:  8.89; xent: 2.19; lr: 0.00000456;   0/1235 tok/s; 319251 sec
[2020-04-02 12:21:38,688 INFO] Step 192750/210000; acc:  53.44; ppl:  6.93; xent: 1.94; lr: 0.00000456;   0/700 tok/s; 319334 sec
[2020-04-02 12:22:14,311 INFO] Loading train dataset from ../bert_data/cnndm.train.9085.bert.pt, number of examples: 1985
[2020-04-02 12:23:02,049 INFO] Step 192800/210000; acc:  52.51; ppl:  8.60; xent: 2.15; lr: 0.00000455;   0/968 tok/s; 319417 sec
[2020-04-02 12:24:25,189 INFO] Step 192850/210000; acc:  52.25; ppl:  7.40; xent: 2.00; lr: 0.00000455;   0/741 tok/s; 319500 sec
[2020-04-02 12:24:53,181 INFO] Loading train dataset from ../bert_data/cnndm.train.9086.bert.pt, number of examples: 1983
[2020-04-02 12:25:49,561 INFO] Step 192900/210000; acc:  51.12; ppl:  9.69; xent: 2.27; lr: 0.00000455;   0/649 tok/s; 319584 sec
[2020-04-02 12:27:12,244 INFO] Step 192950/210000; acc:  46.44; ppl: 10.90; xent: 2.39; lr: 0.00000455;   0/1074 tok/s; 319667 sec
[2020-04-02 12:27:29,324 INFO] Loading train dataset from ../bert_data/cnndm.train.9087.bert.pt, number of examples: 1986
[2020-04-02 12:28:35,624 INFO] Step 193000/210000; acc:  62.27; ppl:  4.50; xent: 1.50; lr: 0.00000455;   0/496 tok/s; 319750 sec
[2020-04-02 12:28:35,627 INFO] Saving checkpoint ../models/model_step_193000.pt
[2020-04-02 12:30:00,506 INFO] Step 193050/210000; acc:  48.15; ppl:  8.63; xent: 2.16; lr: 0.00000455;   0/955 tok/s; 319835 sec
[2020-04-02 12:30:09,479 INFO] Loading train dataset from ../bert_data/cnndm.train.9088.bert.pt, number of examples: 1986
[2020-04-02 12:31:23,746 INFO] Step 193100/210000; acc:  57.00; ppl:  6.46; xent: 1.87; lr: 0.00000455;   0/531 tok/s; 319919 sec
[2020-04-02 12:32:47,045 INFO] Step 193150/210000; acc:  51.63; ppl:  8.97; xent: 2.19; lr: 0.00000455;   0/777 tok/s; 320002 sec
[2020-04-02 12:32:47,342 INFO] Loading train dataset from ../bert_data/cnndm.train.9089.bert.pt, number of examples: 1986
[2020-04-02 12:34:09,527 INFO] Step 193200/210000; acc:  58.65; ppl:  5.51; xent: 1.71; lr: 0.00000455;   0/555 tok/s; 320084 sec
[2020-04-02 12:35:24,543 INFO] Loading train dataset from ../bert_data/cnndm.train.9090.bert.pt, number of examples: 20
[2020-04-02 12:35:26,594 INFO] Loading train dataset from ../bert_data/cnndm.train.9100.bert.pt, number of examples: 1988
[2020-04-02 12:35:32,547 INFO] Step 193250/210000; acc:  54.44; ppl:  6.11; xent: 1.81; lr: 0.00000455;   0/493 tok/s; 320167 sec
[2020-04-02 12:36:44,052 INFO] Step 193300/210000; acc:  60.68; ppl:  4.68; xent: 1.54; lr: 0.00000455;   0/655 tok/s; 320239 sec
[2020-04-02 12:37:44,889 INFO] Loading train dataset from ../bert_data/cnndm.train.9101.bert.pt, number of examples: 1983
[2020-04-02 12:37:57,889 INFO] Step 193350/210000; acc:  65.96; ppl:  4.19; xent: 1.43; lr: 0.00000455;   0/551 tok/s; 320313 sec
[2020-04-02 12:39:10,303 INFO] Step 193400/210000; acc:  64.01; ppl:  4.51; xent: 1.51; lr: 0.00000455;   0/579 tok/s; 320385 sec
[2020-04-02 12:40:01,748 INFO] Loading train dataset from ../bert_data/cnndm.train.9102.bert.pt, number of examples: 1992
[2020-04-02 12:40:23,608 INFO] Step 193450/210000; acc:  69.59; ppl:  3.69; xent: 1.31; lr: 0.00000455;   0/541 tok/s; 320458 sec
[2020-04-02 12:41:37,466 INFO] Step 193500/210000; acc:  67.74; ppl:  3.65; xent: 1.29; lr: 0.00000455;   0/391 tok/s; 320532 sec
[2020-04-02 12:41:37,469 INFO] Saving checkpoint ../models/model_step_193500.pt
[2020-04-02 12:42:23,776 INFO] Loading train dataset from ../bert_data/cnndm.train.9103.bert.pt, number of examples: 1978
[2020-04-02 12:42:53,082 INFO] Step 193550/210000; acc:  67.85; ppl:  3.97; xent: 1.38; lr: 0.00000455;   0/420 tok/s; 320608 sec
[2020-04-02 12:44:06,668 INFO] Step 193600/210000; acc:  63.20; ppl:  4.59; xent: 1.52; lr: 0.00000455;   0/657 tok/s; 320681 sec
[2020-04-02 12:44:42,446 INFO] Loading train dataset from ../bert_data/cnndm.train.9104.bert.pt, number of examples: 1986
[2020-04-02 12:45:20,665 INFO] Step 193650/210000; acc:  59.83; ppl:  5.19; xent: 1.65; lr: 0.00000454;   0/674 tok/s; 320755 sec
[2020-04-02 12:46:33,625 INFO] Step 193700/210000; acc:  71.76; ppl:  3.43; xent: 1.23; lr: 0.00000454;   0/481 tok/s; 320828 sec
[2020-04-02 12:47:01,450 INFO] Loading train dataset from ../bert_data/cnndm.train.9105.bert.pt, number of examples: 1988
[2020-04-02 12:47:46,373 INFO] Step 193750/210000; acc:  66.40; ppl:  3.78; xent: 1.33; lr: 0.00000454;   0/579 tok/s; 320901 sec
[2020-04-02 12:48:59,460 INFO] Step 193800/210000; acc:  64.80; ppl:  4.01; xent: 1.39; lr: 0.00000454;   0/547 tok/s; 320974 sec
[2020-04-02 12:49:19,947 INFO] Loading train dataset from ../bert_data/cnndm.train.9106.bert.pt, number of examples: 1978
[2020-04-02 12:50:12,990 INFO] Step 193850/210000; acc:  63.57; ppl:  4.53; xent: 1.51; lr: 0.00000454;   0/707 tok/s; 321048 sec
[2020-04-02 12:51:26,312 INFO] Step 193900/210000; acc:  70.92; ppl:  3.61; xent: 1.28; lr: 0.00000454;   0/549 tok/s; 321121 sec
[2020-04-02 12:51:38,936 INFO] Loading train dataset from ../bert_data/cnndm.train.9107.bert.pt, number of examples: 1985
[2020-04-02 12:52:40,177 INFO] Step 193950/210000; acc:  72.85; ppl:  3.04; xent: 1.11; lr: 0.00000454;   0/596 tok/s; 321195 sec
[2020-04-02 12:53:52,876 INFO] Step 194000/210000; acc:  72.17; ppl:  2.99; xent: 1.10; lr: 0.00000454;   0/387 tok/s; 321268 sec
[2020-04-02 12:53:52,879 INFO] Saving checkpoint ../models/model_step_194000.pt
[2020-04-02 12:53:59,815 INFO] Loading train dataset from ../bert_data/cnndm.train.9108.bert.pt, number of examples: 1989
[2020-04-02 12:55:08,965 INFO] Step 194050/210000; acc:  60.39; ppl:  4.99; xent: 1.61; lr: 0.00000454;   0/614 tok/s; 321344 sec
[2020-04-02 12:56:19,102 INFO] Loading train dataset from ../bert_data/cnndm.train.9109.bert.pt, number of examples: 1987
[2020-04-02 12:56:22,108 INFO] Step 194100/210000; acc:  67.84; ppl:  3.49; xent: 1.25; lr: 0.00000454;   0/411 tok/s; 321417 sec
[2020-04-02 12:57:35,517 INFO] Step 194150/210000; acc:  64.05; ppl:  4.86; xent: 1.58; lr: 0.00000454;   0/647 tok/s; 321490 sec
[2020-04-02 12:58:37,667 INFO] Loading train dataset from ../bert_data/cnndm.train.9110.bert.pt, number of examples: 1985
[2020-04-02 12:58:49,218 INFO] Step 194200/210000; acc:  58.86; ppl:  5.90; xent: 1.78; lr: 0.00000454;   0/645 tok/s; 321564 sec
[2020-04-02 13:00:02,299 INFO] Step 194250/210000; acc:  68.01; ppl:  4.08; xent: 1.41; lr: 0.00000454;   0/584 tok/s; 321637 sec
[2020-04-02 13:00:57,629 INFO] Loading train dataset from ../bert_data/cnndm.train.9111.bert.pt, number of examples: 1985
[2020-04-02 13:01:17,108 INFO] Step 194300/210000; acc:  68.09; ppl:  3.79; xent: 1.33; lr: 0.00000454;   0/507 tok/s; 321712 sec
[2020-04-02 13:02:30,443 INFO] Step 194350/210000; acc:  74.72; ppl:  2.97; xent: 1.09; lr: 0.00000454;   0/376 tok/s; 321785 sec
[2020-04-02 13:03:17,882 INFO] Loading train dataset from ../bert_data/cnndm.train.9112.bert.pt, number of examples: 1983
[2020-04-02 13:03:44,712 INFO] Step 194400/210000; acc:  65.17; ppl:  4.41; xent: 1.48; lr: 0.00000454;   0/529 tok/s; 321860 sec
[2020-04-02 13:04:58,429 INFO] Step 194450/210000; acc:  69.68; ppl:  3.49; xent: 1.25; lr: 0.00000454;   0/527 tok/s; 321933 sec
[2020-04-02 13:05:36,808 INFO] Loading train dataset from ../bert_data/cnndm.train.9113.bert.pt, number of examples: 1979
[2020-04-02 13:06:11,588 INFO] Step 194500/210000; acc:  71.48; ppl:  3.40; xent: 1.22; lr: 0.00000453;   0/550 tok/s; 322006 sec
[2020-04-02 13:06:11,602 INFO] Saving checkpoint ../models/model_step_194500.pt
[2020-04-02 13:07:27,438 INFO] Step 194550/210000; acc:  73.25; ppl:  2.87; xent: 1.05; lr: 0.00000453;   0/434 tok/s; 322082 sec
[2020-04-02 13:07:59,263 INFO] Loading train dataset from ../bert_data/cnndm.train.9114.bert.pt, number of examples: 1989
[2020-04-02 13:08:42,031 INFO] Step 194600/210000; acc:  77.02; ppl:  2.72; xent: 1.00; lr: 0.00000453;   0/442 tok/s; 322157 sec
[2020-04-02 13:09:55,119 INFO] Step 194650/210000; acc:  63.08; ppl:  4.36; xent: 1.47; lr: 0.00000453;   0/513 tok/s; 322230 sec
[2020-04-02 13:10:18,725 INFO] Loading train dataset from ../bert_data/cnndm.train.9115.bert.pt, number of examples: 1989
[2020-04-02 13:11:08,228 INFO] Step 194700/210000; acc:  67.52; ppl:  3.89; xent: 1.36; lr: 0.00000453;   0/567 tok/s; 322303 sec
[2020-04-02 13:12:21,710 INFO] Step 194750/210000; acc:  72.33; ppl:  3.08; xent: 1.13; lr: 0.00000453;   0/481 tok/s; 322377 sec
[2020-04-02 13:12:37,639 INFO] Loading train dataset from ../bert_data/cnndm.train.9116.bert.pt, number of examples: 1985
[2020-04-02 13:13:35,882 INFO] Step 194800/210000; acc:  73.62; ppl:  2.99; xent: 1.10; lr: 0.00000453;   0/498 tok/s; 322451 sec
[2020-04-02 13:14:49,400 INFO] Step 194850/210000; acc:  67.98; ppl:  3.69; xent: 1.31; lr: 0.00000453;   0/475 tok/s; 322524 sec
[2020-04-02 13:14:57,164 INFO] Loading train dataset from ../bert_data/cnndm.train.9117.bert.pt, number of examples: 1979
[2020-04-02 13:16:03,244 INFO] Step 194900/210000; acc:  74.41; ppl:  2.95; xent: 1.08; lr: 0.00000453;   0/473 tok/s; 322598 sec
[2020-04-02 13:17:16,140 INFO] Step 194950/210000; acc:  63.27; ppl:  4.54; xent: 1.51; lr: 0.00000453;   0/617 tok/s; 322671 sec
[2020-04-02 13:17:16,323 INFO] Loading train dataset from ../bert_data/cnndm.train.9118.bert.pt, number of examples: 1980
[2020-04-02 13:18:29,717 INFO] Step 195000/210000; acc:  66.67; ppl:  4.14; xent: 1.42; lr: 0.00000453;   0/580 tok/s; 322745 sec
[2020-04-02 13:18:29,720 INFO] Saving checkpoint ../models/model_step_195000.pt
[2020-04-02 13:19:38,903 INFO] Loading train dataset from ../bert_data/cnndm.train.9119.bert.pt, number of examples: 1985
[2020-04-02 13:19:46,114 INFO] Step 195050/210000; acc:  62.83; ppl:  4.67; xent: 1.54; lr: 0.00000453;   0/514 tok/s; 322821 sec
[2020-04-02 13:20:59,571 INFO] Step 195100/210000; acc:  61.99; ppl:  5.06; xent: 1.62; lr: 0.00000453;   0/577 tok/s; 322894 sec
[2020-04-02 13:21:56,351 INFO] Loading train dataset from ../bert_data/cnndm.train.9120.bert.pt, number of examples: 1981
[2020-04-02 13:22:12,962 INFO] Step 195150/210000; acc:  72.89; ppl:  3.25; xent: 1.18; lr: 0.00000453;   0/431 tok/s; 322968 sec
[2020-04-02 13:23:25,807 INFO] Step 195200/210000; acc:  66.41; ppl:  3.87; xent: 1.35; lr: 0.00000453;   0/547 tok/s; 323041 sec
[2020-04-02 13:24:15,763 INFO] Loading train dataset from ../bert_data/cnndm.train.9121.bert.pt, number of examples: 1985
[2020-04-02 13:24:38,762 INFO] Step 195250/210000; acc:  69.54; ppl:  3.77; xent: 1.33; lr: 0.00000453;   0/578 tok/s; 323114 sec
[2020-04-02 13:25:53,686 INFO] Step 195300/210000; acc:  73.27; ppl:  3.49; xent: 1.25; lr: 0.00000453;   0/477 tok/s; 323189 sec
[2020-04-02 13:26:34,827 INFO] Loading train dataset from ../bert_data/cnndm.train.9122.bert.pt, number of examples: 1986
[2020-04-02 13:27:07,599 INFO] Step 195350/210000; acc:  73.46; ppl:  3.06; xent: 1.12; lr: 0.00000453;   0/496 tok/s; 323262 sec
[2020-04-02 13:28:20,613 INFO] Step 195400/210000; acc:  64.97; ppl:  4.25; xent: 1.45; lr: 0.00000452;   0/651 tok/s; 323335 sec
[2020-04-02 13:28:54,646 INFO] Loading train dataset from ../bert_data/cnndm.train.9123.bert.pt, number of examples: 1983
[2020-04-02 13:29:34,595 INFO] Step 195450/210000; acc:  67.81; ppl:  3.92; xent: 1.36; lr: 0.00000452;   0/496 tok/s; 323409 sec
[2020-04-02 13:30:47,492 INFO] Step 195500/210000; acc:  64.73; ppl:  4.51; xent: 1.51; lr: 0.00000452;   0/720 tok/s; 323482 sec
[2020-04-02 13:30:47,495 INFO] Saving checkpoint ../models/model_step_195500.pt
[2020-04-02 13:31:16,841 INFO] Loading train dataset from ../bert_data/cnndm.train.9124.bert.pt, number of examples: 1983
[2020-04-02 13:32:03,875 INFO] Step 195550/210000; acc:  62.90; ppl:  4.65; xent: 1.54; lr: 0.00000452;   0/578 tok/s; 323559 sec
[2020-04-02 13:33:16,877 INFO] Step 195600/210000; acc:  70.87; ppl:  3.56; xent: 1.27; lr: 0.00000452;   0/517 tok/s; 323632 sec
[2020-04-02 13:33:35,760 INFO] Loading train dataset from ../bert_data/cnndm.train.9125.bert.pt, number of examples: 1991
[2020-04-02 13:34:29,974 INFO] Step 195650/210000; acc:  68.13; ppl:  3.61; xent: 1.28; lr: 0.00000452;   0/456 tok/s; 323705 sec
[2020-04-02 13:35:42,960 INFO] Step 195700/210000; acc:  65.24; ppl:  3.93; xent: 1.37; lr: 0.00000452;   0/476 tok/s; 323778 sec
[2020-04-02 13:35:53,046 INFO] Loading train dataset from ../bert_data/cnndm.train.9126.bert.pt, number of examples: 1985
[2020-04-02 13:36:55,769 INFO] Step 195750/210000; acc:  65.57; ppl:  4.17; xent: 1.43; lr: 0.00000452;   0/549 tok/s; 323851 sec
[2020-04-02 13:38:08,480 INFO] Step 195800/210000; acc:  74.67; ppl:  3.12; xent: 1.14; lr: 0.00000452;   0/459 tok/s; 323923 sec
[2020-04-02 13:38:10,796 INFO] Loading train dataset from ../bert_data/cnndm.train.9127.bert.pt, number of examples: 1980
[2020-04-02 13:39:22,099 INFO] Step 195850/210000; acc:  67.46; ppl:  3.78; xent: 1.33; lr: 0.00000452;   0/603 tok/s; 323997 sec
[2020-04-02 13:40:29,961 INFO] Loading train dataset from ../bert_data/cnndm.train.9128.bert.pt, number of examples: 1981
[2020-04-02 13:40:35,924 INFO] Step 195900/210000; acc:  68.52; ppl:  3.61; xent: 1.28; lr: 0.00000452;   0/540 tok/s; 324071 sec
[2020-04-02 13:41:48,355 INFO] Step 195950/210000; acc:  73.88; ppl:  2.96; xent: 1.09; lr: 0.00000452;   0/492 tok/s; 324143 sec
[2020-04-02 13:42:48,141 INFO] Loading train dataset from ../bert_data/cnndm.train.9129.bert.pt, number of examples: 1987
[2020-04-02 13:43:00,955 INFO] Step 196000/210000; acc:  71.33; ppl:  3.40; xent: 1.22; lr: 0.00000452;   0/673 tok/s; 324216 sec
[2020-04-02 13:43:00,957 INFO] Saving checkpoint ../models/model_step_196000.pt
[2020-04-02 13:44:16,604 INFO] Step 196050/210000; acc:  70.92; ppl:  3.60; xent: 1.28; lr: 0.00000452;   0/516 tok/s; 324291 sec
[2020-04-02 13:45:08,638 INFO] Loading train dataset from ../bert_data/cnndm.train.9130.bert.pt, number of examples: 1986
[2020-04-02 13:45:30,250 INFO] Step 196100/210000; acc:  73.90; ppl:  3.02; xent: 1.11; lr: 0.00000452;   0/558 tok/s; 324365 sec
[2020-04-02 13:46:43,905 INFO] Step 196150/210000; acc:  72.34; ppl:  3.18; xent: 1.16; lr: 0.00000452;   0/539 tok/s; 324439 sec
[2020-04-02 13:47:27,953 INFO] Loading train dataset from ../bert_data/cnndm.train.9131.bert.pt, number of examples: 1984
[2020-04-02 13:47:57,442 INFO] Step 196200/210000; acc:  71.28; ppl:  3.34; xent: 1.21; lr: 0.00000452;   0/397 tok/s; 324512 sec
[2020-04-02 13:49:11,213 INFO] Step 196250/210000; acc:  67.54; ppl:  3.79; xent: 1.33; lr: 0.00000451;   0/622 tok/s; 324586 sec
[2020-04-02 13:49:47,798 INFO] Loading train dataset from ../bert_data/cnndm.train.9132.bert.pt, number of examples: 1990
[2020-04-02 13:50:23,699 INFO] Step 196300/210000; acc:  76.28; ppl:  2.91; xent: 1.07; lr: 0.00000451;   0/540 tok/s; 324659 sec
[2020-04-02 13:51:36,927 INFO] Step 196350/210000; acc:  75.30; ppl:  2.76; xent: 1.02; lr: 0.00000451;   0/491 tok/s; 324732 sec
[2020-04-02 13:52:06,698 INFO] Loading train dataset from ../bert_data/cnndm.train.9133.bert.pt, number of examples: 1978
[2020-04-02 13:52:50,443 INFO] Step 196400/210000; acc:  70.92; ppl:  3.33; xent: 1.20; lr: 0.00000451;   0/524 tok/s; 324805 sec
[2020-04-02 13:54:04,167 INFO] Step 196450/210000; acc:  65.28; ppl:  4.42; xent: 1.49; lr: 0.00000451;   0/570 tok/s; 324879 sec
[2020-04-02 13:54:25,248 INFO] Loading train dataset from ../bert_data/cnndm.train.9134.bert.pt, number of examples: 156
[2020-04-02 13:54:35,905 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
[2020-04-02 13:55:24,416 INFO] Step 196500/210000; acc:  49.71; ppl: 12.73; xent: 2.54; lr: 0.00000451;   0/809 tok/s; 324959 sec
[2020-04-02 13:55:24,419 INFO] Saving checkpoint ../models/model_step_196500.pt
[2020-04-02 13:56:51,093 INFO] Step 196550/210000; acc:  56.70; ppl:  7.77; xent: 2.05; lr: 0.00000451;   0/642 tok/s; 325046 sec
[2020-04-02 13:57:27,820 INFO] Loading train dataset from ../bert_data/cnndm.train.0091.bert.pt, number of examples: 1998
[2020-04-02 13:58:16,663 INFO] Step 196600/210000; acc:  54.67; ppl:  8.88; xent: 2.18; lr: 0.00000451;   0/806 tok/s; 325131 sec
[2020-04-02 13:59:40,691 INFO] Step 196650/210000; acc:  55.14; ppl:  8.65; xent: 2.16; lr: 0.00000451;   0/894 tok/s; 325216 sec
[2020-04-02 14:00:14,737 INFO] Loading train dataset from ../bert_data/cnndm.train.0092.bert.pt, number of examples: 2000
[2020-04-02 14:01:05,425 INFO] Step 196700/210000; acc:  52.22; ppl: 10.50; xent: 2.35; lr: 0.00000451;   0/1041 tok/s; 325300 sec
[2020-04-02 14:02:29,868 INFO] Step 196750/210000; acc:  54.17; ppl:  9.11; xent: 2.21; lr: 0.00000451;   0/838 tok/s; 325385 sec
[2020-04-02 14:03:02,261 INFO] Loading train dataset from ../bert_data/cnndm.train.0093.bert.pt, number of examples: 1997
[2020-04-02 14:03:54,449 INFO] Step 196800/210000; acc:  59.02; ppl:  7.46; xent: 2.01; lr: 0.00000451;   0/893 tok/s; 325469 sec
[2020-04-02 14:05:19,023 INFO] Step 196850/210000; acc:  56.19; ppl:  8.59; xent: 2.15; lr: 0.00000451;   0/886 tok/s; 325554 sec
[2020-04-02 14:05:52,219 INFO] Loading train dataset from ../bert_data/cnndm.train.0094.bert.pt, number of examples: 2001
[2020-04-02 14:06:44,440 INFO] Step 196900/210000; acc:  54.94; ppl:  7.76; xent: 2.05; lr: 0.00000451;   0/542 tok/s; 325639 sec
[2020-04-02 14:08:08,676 INFO] Step 196950/210000; acc:  47.11; ppl: 12.92; xent: 2.56; lr: 0.00000451;   0/1026 tok/s; 325723 sec
[2020-04-02 14:08:39,799 INFO] Loading train dataset from ../bert_data/cnndm.train.0095.bert.pt, number of examples: 2001
[2020-04-02 14:09:33,986 INFO] Step 197000/210000; acc:  56.78; ppl:  6.88; xent: 1.93; lr: 0.00000451;   0/592 tok/s; 325809 sec
[2020-04-02 14:09:33,990 INFO] Saving checkpoint ../models/model_step_197000.pt
[2020-04-02 14:11:00,801 INFO] Step 197050/210000; acc:  48.18; ppl: 11.51; xent: 2.44; lr: 0.00000451;   0/496 tok/s; 325896 sec
[2020-04-02 14:11:30,285 INFO] Loading train dataset from ../bert_data/cnndm.train.0096.bert.pt, number of examples: 2001
[2020-04-02 14:12:25,844 INFO] Step 197100/210000; acc:  47.13; ppl: 12.52; xent: 2.53; lr: 0.00000450;   0/622 tok/s; 325981 sec
[2020-04-02 14:13:50,018 INFO] Step 197150/210000; acc:  58.33; ppl:  7.59; xent: 2.03; lr: 0.00000450;   0/939 tok/s; 326065 sec
[2020-04-02 14:14:19,765 INFO] Loading train dataset from ../bert_data/cnndm.train.0097.bert.pt, number of examples: 2001
[2020-04-02 14:15:15,379 INFO] Step 197200/210000; acc:  53.75; ppl:  8.25; xent: 2.11; lr: 0.00000450;   0/793 tok/s; 326150 sec
[2020-04-02 14:16:39,499 INFO] Step 197250/210000; acc:  50.84; ppl:  9.17; xent: 2.22; lr: 0.00000450;   0/615 tok/s; 326234 sec
[2020-04-02 14:17:07,130 INFO] Loading train dataset from ../bert_data/cnndm.train.0098.bert.pt, number of examples: 2000
[2020-04-02 14:18:04,621 INFO] Step 197300/210000; acc:  51.98; ppl: 10.24; xent: 2.33; lr: 0.00000450;   0/866 tok/s; 326319 sec
[2020-04-02 14:19:28,639 INFO] Step 197350/210000; acc:  52.54; ppl:  9.68; xent: 2.27; lr: 0.00000450;   0/750 tok/s; 326403 sec
[2020-04-02 14:19:54,608 INFO] Loading train dataset from ../bert_data/cnndm.train.0099.bert.pt, number of examples: 2000
[2020-04-02 14:20:54,012 INFO] Step 197400/210000; acc:  59.91; ppl:  6.52; xent: 1.88; lr: 0.00000450;   0/809 tok/s; 326489 sec
[2020-04-02 14:22:18,127 INFO] Step 197450/210000; acc:  55.49; ppl:  7.79; xent: 2.05; lr: 0.00000450;   0/834 tok/s; 326573 sec
[2020-04-02 14:22:42,375 INFO] Loading train dataset from ../bert_data/cnndm.train.1.bert.pt, number of examples: 2001
[2020-04-02 14:23:42,475 INFO] Step 197500/210000; acc:  53.40; ppl:  9.33; xent: 2.23; lr: 0.00000450;   0/827 tok/s; 326657 sec
[2020-04-02 14:23:42,478 INFO] Saving checkpoint ../models/model_step_197500.pt
[2020-04-02 14:25:09,204 INFO] Step 197550/210000; acc:  53.30; ppl:  9.42; xent: 2.24; lr: 0.00000450;   0/876 tok/s; 326744 sec
[2020-04-02 14:25:33,526 INFO] Loading train dataset from ../bert_data/cnndm.train.10.bert.pt, number of examples: 2001
[2020-04-02 14:26:34,491 INFO] Step 197600/210000; acc:  50.18; ppl: 10.31; xent: 2.33; lr: 0.00000450;   0/501 tok/s; 326829 sec
[2020-04-02 14:27:58,579 INFO] Step 197650/210000; acc:  51.67; ppl:  9.01; xent: 2.20; lr: 0.00000450;   0/501 tok/s; 326913 sec
[2020-04-02 14:28:21,287 INFO] Loading train dataset from ../bert_data/cnndm.train.100.bert.pt, number of examples: 1999
[2020-04-02 14:29:23,820 INFO] Step 197700/210000; acc:  46.74; ppl: 16.17; xent: 2.78; lr: 0.00000450;   0/542 tok/s; 326999 sec
[2020-04-02 14:30:47,862 INFO] Step 197750/210000; acc:  47.18; ppl: 14.43; xent: 2.67; lr: 0.00000450;   0/843 tok/s; 327083 sec
[2020-04-02 14:31:08,932 INFO] Loading train dataset from ../bert_data/cnndm.train.101.bert.pt, number of examples: 2000
[2020-04-02 14:32:13,374 INFO] Step 197800/210000; acc:  55.18; ppl:  8.43; xent: 2.13; lr: 0.00000450;   0/713 tok/s; 327168 sec
[2020-04-02 14:33:38,053 INFO] Step 197850/210000; acc:  54.68; ppl:  7.66; xent: 2.04; lr: 0.00000450;   0/595 tok/s; 327253 sec
[2020-04-02 14:33:57,312 INFO] Loading train dataset from ../bert_data/cnndm.train.102.bert.pt, number of examples: 1999
[2020-04-02 14:35:02,901 INFO] Step 197900/210000; acc:  53.95; ppl:  8.78; xent: 2.17; lr: 0.00000450;   0/654 tok/s; 327338 sec
[2020-04-02 14:36:27,505 INFO] Step 197950/210000; acc:  54.85; ppl:  8.23; xent: 2.11; lr: 0.00000450;   0/619 tok/s; 327422 sec
[2020-04-02 14:36:46,767 INFO] Loading train dataset from ../bert_data/cnndm.train.103.bert.pt, number of examples: 2000
[2020-04-02 14:37:52,729 INFO] Step 198000/210000; acc:  51.37; ppl: 10.55; xent: 2.36; lr: 0.00000449;   0/903 tok/s; 327508 sec
[2020-04-02 14:37:52,733 INFO] Saving checkpoint ../models/model_step_198000.pt
[2020-04-02 14:39:19,351 INFO] Step 198050/210000; acc:  53.08; ppl:  9.52; xent: 2.25; lr: 0.00000449;   0/695 tok/s; 327594 sec
[2020-04-02 14:39:36,911 INFO] Loading train dataset from ../bert_data/cnndm.train.104.bert.pt, number of examples: 2000
[2020-04-02 14:40:44,781 INFO] Step 198100/210000; acc:  53.21; ppl:  8.69; xent: 2.16; lr: 0.00000449;   0/843 tok/s; 327680 sec
[2020-04-02 14:42:09,215 INFO] Step 198150/210000; acc:  55.67; ppl:  7.76; xent: 2.05; lr: 0.00000449;   0/851 tok/s; 327764 sec
[2020-04-02 14:42:25,218 INFO] Loading train dataset from ../bert_data/cnndm.train.105.bert.pt, number of examples: 2001
[2020-04-02 14:43:34,475 INFO] Step 198200/210000; acc:  52.94; ppl:  8.29; xent: 2.11; lr: 0.00000449;   0/592 tok/s; 327849 sec
[2020-04-02 14:44:58,407 INFO] Step 198250/210000; acc:  56.07; ppl:  7.50; xent: 2.01; lr: 0.00000449;   0/738 tok/s; 327933 sec
[2020-04-02 14:45:12,687 INFO] Loading train dataset from ../bert_data/cnndm.train.106.bert.pt, number of examples: 1999
[2020-04-02 14:46:23,847 INFO] Step 198300/210000; acc:  53.75; ppl:  8.10; xent: 2.09; lr: 0.00000449;   0/565 tok/s; 328019 sec
[2020-04-02 14:47:48,335 INFO] Step 198350/210000; acc:  49.02; ppl: 12.65; xent: 2.54; lr: 0.00000449;   0/771 tok/s; 328103 sec
[2020-04-02 14:48:02,898 INFO] Loading train dataset from ../bert_data/cnndm.train.107.bert.pt, number of examples: 1999
[2020-04-02 14:49:13,771 INFO] Step 198400/210000; acc:  50.09; ppl: 11.18; xent: 2.41; lr: 0.00000449;   0/707 tok/s; 328189 sec
[2020-04-02 14:50:38,598 INFO] Step 198450/210000; acc:  55.25; ppl:  8.50; xent: 2.14; lr: 0.00000449;   0/592 tok/s; 328273 sec
[2020-04-02 14:50:49,365 INFO] Loading train dataset from ../bert_data/cnndm.train.108.bert.pt, number of examples: 2000
[2020-04-02 14:52:03,501 INFO] Step 198500/210000; acc:  48.35; ppl: 10.35; xent: 2.34; lr: 0.00000449;   0/525 tok/s; 328358 sec
[2020-04-02 14:52:03,505 INFO] Saving checkpoint ../models/model_step_198500.pt
[2020-04-02 14:53:29,535 INFO] Step 198550/210000; acc:  51.68; ppl:  9.08; xent: 2.21; lr: 0.00000449;   0/774 tok/s; 328444 sec
[2020-04-02 14:53:40,259 INFO] Loading train dataset from ../bert_data/cnndm.train.109.bert.pt, number of examples: 2000
[2020-04-02 14:54:54,249 INFO] Step 198600/210000; acc:  60.81; ppl:  6.01; xent: 1.79; lr: 0.00000449;   0/768 tok/s; 328529 sec
[2020-04-02 14:56:18,743 INFO] Step 198650/210000; acc:  50.78; ppl: 10.62; xent: 2.36; lr: 0.00000449;   0/768 tok/s; 328614 sec
[2020-04-02 14:56:28,002 INFO] Loading train dataset from ../bert_data/cnndm.train.11.bert.pt, number of examples: 1999
[2020-04-02 14:57:44,102 INFO] Step 198700/210000; acc:  59.91; ppl:  6.49; xent: 1.87; lr: 0.00000449;   0/900 tok/s; 328699 sec
[2020-04-02 14:59:08,711 INFO] Step 198750/210000; acc:  47.42; ppl: 11.56; xent: 2.45; lr: 0.00000449;   0/928 tok/s; 328784 sec
[2020-04-02 14:59:16,083 INFO] Loading train dataset from ../bert_data/cnndm.train.110.bert.pt, number of examples: 2000
[2020-04-02 15:00:33,813 INFO] Step 198800/210000; acc:  58.56; ppl:  6.42; xent: 1.86; lr: 0.00000449;   0/870 tok/s; 328869 sec
[2020-04-02 15:01:58,108 INFO] Step 198850/210000; acc:  49.61; ppl: 12.05; xent: 2.49; lr: 0.00000449;   0/1039 tok/s; 328953 sec
[2020-04-02 15:02:03,974 INFO] Loading train dataset from ../bert_data/cnndm.train.111.bert.pt, number of examples: 2000
[2020-04-02 15:03:22,774 INFO] Step 198900/210000; acc:  53.10; ppl:  8.03; xent: 2.08; lr: 0.00000448;   0/461 tok/s; 329038 sec
[2020-04-02 15:04:47,298 INFO] Step 198950/210000; acc:  52.94; ppl:  9.32; xent: 2.23; lr: 0.00000448;   0/681 tok/s; 329122 sec
[2020-04-02 15:04:51,458 INFO] Loading train dataset from ../bert_data/cnndm.train.112.bert.pt, number of examples: 1999
[2020-04-02 15:06:11,868 INFO] Step 199000/210000; acc:  60.74; ppl:  5.98; xent: 1.79; lr: 0.00000448;   0/687 tok/s; 329207 sec
[2020-04-02 15:06:11,872 INFO] Saving checkpoint ../models/model_step_199000.pt
[2020-04-02 15:07:38,733 INFO] Step 199050/210000; acc:  55.87; ppl:  8.30; xent: 2.12; lr: 0.00000448;   0/657 tok/s; 329294 sec
[2020-04-02 15:07:43,527 INFO] Loading train dataset from ../bert_data/cnndm.train.113.bert.pt, number of examples: 1999
[2020-04-02 15:09:04,424 INFO] Step 199100/210000; acc:  53.97; ppl:  7.58; xent: 2.02; lr: 0.00000448;   0/656 tok/s; 329379 sec
[2020-04-02 15:10:28,771 INFO] Step 199150/210000; acc:  50.00; ppl: 10.27; xent: 2.33; lr: 0.00000448;   0/608 tok/s; 329464 sec
[2020-04-02 15:10:30,803 INFO] Loading train dataset from ../bert_data/cnndm.train.114.bert.pt, number of examples: 1998
[2020-04-02 15:11:52,933 INFO] Step 199200/210000; acc:  51.36; ppl:  8.97; xent: 2.19; lr: 0.00000448;   0/665 tok/s; 329548 sec
[2020-04-02 15:13:17,689 INFO] Step 199250/210000; acc:  58.49; ppl:  7.26; xent: 1.98; lr: 0.00000448;   0/790 tok/s; 329633 sec
[2020-04-02 15:13:18,022 INFO] Loading train dataset from ../bert_data/cnndm.train.115.bert.pt, number of examples: 2000
[2020-04-02 15:14:42,658 INFO] Step 199300/210000; acc:  49.54; ppl: 11.33; xent: 2.43; lr: 0.00000448;   0/831 tok/s; 329717 sec
[2020-04-02 15:16:06,115 INFO] Loading train dataset from ../bert_data/cnndm.train.116.bert.pt, number of examples: 2000
[2020-04-02 15:16:07,895 INFO] Step 199350/210000; acc:  56.48; ppl:  6.83; xent: 1.92; lr: 0.00000448;   0/455 tok/s; 329803 sec
[2020-04-02 15:17:32,182 INFO] Step 199400/210000; acc:  51.75; ppl:  9.36; xent: 2.24; lr: 0.00000448;   0/855 tok/s; 329887 sec
[2020-04-02 15:18:53,894 INFO] Loading train dataset from ../bert_data/cnndm.train.117.bert.pt, number of examples: 2000
[2020-04-02 15:18:57,272 INFO] Step 199450/210000; acc:  50.40; ppl: 10.20; xent: 2.32; lr: 0.00000448;   0/603 tok/s; 329972 sec
[2020-04-02 15:20:21,899 INFO] Step 199500/210000; acc:  58.89; ppl:  6.34; xent: 1.85; lr: 0.00000448;   0/614 tok/s; 330057 sec
[2020-04-02 15:20:21,902 INFO] Saving checkpoint ../models/model_step_199500.pt
[2020-04-02 15:21:44,595 INFO] Loading train dataset from ../bert_data/cnndm.train.118.bert.pt, number of examples: 2001
[2020-04-02 15:21:49,790 INFO] Step 199550/210000; acc:  53.03; ppl:  8.43; xent: 2.13; lr: 0.00000448;   0/705 tok/s; 330145 sec
[2020-04-02 15:23:14,051 INFO] Step 199600/210000; acc:  50.95; ppl: 10.25; xent: 2.33; lr: 0.00000448;   0/691 tok/s; 330229 sec
[2020-04-02 15:24:34,356 INFO] Loading train dataset from ../bert_data/cnndm.train.119.bert.pt, number of examples: 2000
[2020-04-02 15:24:39,365 INFO] Step 199650/210000; acc:  49.44; ppl: 11.51; xent: 2.44; lr: 0.00000448;   0/852 tok/s; 330314 sec
[2020-04-02 15:26:03,525 INFO] Step 199700/210000; acc:  55.62; ppl:  7.71; xent: 2.04; lr: 0.00000448;   0/636 tok/s; 330398 sec
[2020-04-02 15:27:21,893 INFO] Loading train dataset from ../bert_data/cnndm.train.12.bert.pt, number of examples: 2001
[2020-04-02 15:27:28,679 INFO] Step 199750/210000; acc:  51.72; ppl:  9.06; xent: 2.20; lr: 0.00000447;   0/813 tok/s; 330484 sec
[2020-04-02 15:28:53,284 INFO] Step 199800/210000; acc:  55.45; ppl:  7.91; xent: 2.07; lr: 0.00000447;   0/700 tok/s; 330568 sec
[2020-04-02 15:30:10,035 INFO] Loading train dataset from ../bert_data/cnndm.train.120.bert.pt, number of examples: 2001
[2020-04-02 15:30:18,351 INFO] Step 199850/210000; acc:  52.22; ppl:  9.73; xent: 2.27; lr: 0.00000447;   0/773 tok/s; 330653 sec
[2020-04-02 15:31:43,331 INFO] Step 199900/210000; acc:  53.53; ppl:  8.97; xent: 2.19; lr: 0.00000447;   0/814 tok/s; 330738 sec
[2020-04-02 15:33:00,846 INFO] Loading train dataset from ../bert_data/cnndm.train.121.bert.pt, number of examples: 2001
[2020-04-02 15:33:09,408 INFO] Step 199950/210000; acc:  53.93; ppl:  9.53; xent: 2.25; lr: 0.00000447;   0/843 tok/s; 330824 sec
[2020-04-02 15:34:33,817 INFO] Step 200000/210000; acc:  52.05; ppl:  9.87; xent: 2.29; lr: 0.00000447;   0/822 tok/s; 330909 sec
[2020-04-02 15:34:33,821 INFO] Saving checkpoint ../models/model_step_200000.pt
[2020-04-02 15:35:51,220 INFO] Loading train dataset from ../bert_data/cnndm.train.122.bert.pt, number of examples: 2000
[2020-04-02 15:36:01,546 INFO] Step 200050/210000; acc:  59.48; ppl:  6.33; xent: 1.85; lr: 0.00000447;   0/474 tok/s; 330996 sec
[2020-04-02 15:37:26,111 INFO] Step 200100/210000; acc:  54.66; ppl:  7.93; xent: 2.07; lr: 0.00000447;   0/484 tok/s; 331081 sec
[2020-04-02 15:38:37,982 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001
[2020-04-02 15:38:51,413 INFO] Step 200150/210000; acc:  56.98; ppl:  6.98; xent: 1.94; lr: 0.00000447;   0/690 tok/s; 331166 sec
[2020-04-02 15:40:15,680 INFO] Step 200200/210000; acc:  53.69; ppl:  7.94; xent: 2.07; lr: 0.00000447;   0/677 tok/s; 331251 sec
[2020-04-02 15:41:26,955 INFO] Loading train dataset from ../bert_data/cnndm.train.124.bert.pt, number of examples: 2001
[2020-04-02 15:41:40,563 INFO] Step 200250/210000; acc:  58.04; ppl:  6.93; xent: 1.94; lr: 0.00000447;   0/723 tok/s; 331335 sec
[2020-04-02 15:43:05,090 INFO] Step 200300/210000; acc:  58.59; ppl:  6.21; xent: 1.83; lr: 0.00000447;   0/634 tok/s; 331420 sec
[2020-04-02 15:44:15,162 INFO] Loading train dataset from ../bert_data/cnndm.train.125.bert.pt, number of examples: 2000
[2020-04-02 15:44:30,458 INFO] Step 200350/210000; acc:  56.69; ppl:  6.92; xent: 1.94; lr: 0.00000447;   0/858 tok/s; 331505 sec
[2020-04-02 15:45:55,375 INFO] Step 200400/210000; acc:  54.67; ppl:  8.03; xent: 2.08; lr: 0.00000447;   0/905 tok/s; 331590 sec
[2020-04-02 15:47:04,162 INFO] Loading train dataset from ../bert_data/cnndm.train.126.bert.pt, number of examples: 1999
[2020-04-02 15:47:20,968 INFO] Step 200450/210000; acc:  58.02; ppl:  6.41; xent: 1.86; lr: 0.00000447;   0/769 tok/s; 331676 sec
[2020-04-02 15:48:45,406 INFO] Step 200500/210000; acc:  55.21; ppl:  7.73; xent: 2.05; lr: 0.00000447;   0/776 tok/s; 331760 sec
[2020-04-02 15:48:45,432 INFO] Saving checkpoint ../models/model_step_200500.pt
[2020-04-02 15:49:54,849 INFO] Loading train dataset from ../bert_data/cnndm.train.127.bert.pt, number of examples: 2000
[2020-04-02 15:50:13,302 INFO] Step 200550/210000; acc:  52.47; ppl:  8.92; xent: 2.19; lr: 0.00000447;   0/765 tok/s; 331848 sec
[2020-04-02 15:51:38,111 INFO] Step 200600/210000; acc:  54.17; ppl:  7.65; xent: 2.04; lr: 0.00000447;   0/718 tok/s; 331933 sec
[2020-04-02 15:52:42,882 INFO] Loading train dataset from ../bert_data/cnndm.train.128.bert.pt, number of examples: 2001
[2020-04-02 15:53:03,106 INFO] Step 200650/210000; acc:  52.28; ppl:  9.13; xent: 2.21; lr: 0.00000446;   0/861 tok/s; 332018 sec
[2020-04-02 15:54:27,557 INFO] Step 200700/210000; acc:  55.53; ppl:  7.50; xent: 2.01; lr: 0.00000446;   0/713 tok/s; 332102 sec
[2020-04-02 15:55:32,373 INFO] Loading train dataset from ../bert_data/cnndm.train.129.bert.pt, number of examples: 2000
[2020-04-02 15:55:52,898 INFO] Step 200750/210000; acc:  56.72; ppl:  7.72; xent: 2.04; lr: 0.00000446;   0/611 tok/s; 332188 sec
[2020-04-02 15:57:17,236 INFO] Step 200800/210000; acc:  56.52; ppl:  7.02; xent: 1.95; lr: 0.00000446;   0/638 tok/s; 332272 sec
[2020-04-02 15:58:20,413 INFO] Loading train dataset from ../bert_data/cnndm.train.13.bert.pt, number of examples: 2001
[2020-04-02 15:58:42,384 INFO] Step 200850/210000; acc:  56.16; ppl:  8.03; xent: 2.08; lr: 0.00000446;   0/819 tok/s; 332357 sec
[2020-04-02 16:00:06,758 INFO] Step 200900/210000; acc:  54.24; ppl:  7.89; xent: 2.07; lr: 0.00000446;   0/751 tok/s; 332442 sec
[2020-04-02 16:01:08,325 INFO] Loading train dataset from ../bert_data/cnndm.train.130.bert.pt, number of examples: 2000
[2020-04-02 16:01:32,094 INFO] Step 200950/210000; acc:  52.76; ppl:  8.71; xent: 2.16; lr: 0.00000446;   0/928 tok/s; 332527 sec
[2020-04-02 16:02:56,166 INFO] Step 201000/210000; acc:  53.15; ppl:  9.53; xent: 2.25; lr: 0.00000446;   0/808 tok/s; 332611 sec
[2020-04-02 16:02:56,191 INFO] Saving checkpoint ../models/model_step_201000.pt
[2020-04-02 16:03:59,913 INFO] Loading train dataset from ../bert_data/cnndm.train.131.bert.pt, number of examples: 2001
[2020-04-02 16:04:23,513 INFO] Step 201050/210000; acc:  52.84; ppl:  9.60; xent: 2.26; lr: 0.00000446;   0/801 tok/s; 332698 sec
[2020-04-02 16:05:47,129 INFO] Step 201100/210000; acc:  54.98; ppl:  7.65; xent: 2.03; lr: 0.00000446;   0/787 tok/s; 332782 sec
[2020-04-02 16:06:46,929 INFO] Loading train dataset from ../bert_data/cnndm.train.132.bert.pt, number of examples: 2001
[2020-04-02 16:07:11,998 INFO] Step 201150/210000; acc:  56.11; ppl:  8.42; xent: 2.13; lr: 0.00000446;   0/956 tok/s; 332867 sec
[2020-04-02 16:08:36,002 INFO] Step 201200/210000; acc:  54.89; ppl:  8.36; xent: 2.12; lr: 0.00000446;   0/873 tok/s; 332951 sec
[2020-04-02 16:09:34,090 INFO] Loading train dataset from ../bert_data/cnndm.train.133.bert.pt, number of examples: 1999
[2020-04-02 16:10:01,215 INFO] Step 201250/210000; acc:  50.08; ppl: 11.56; xent: 2.45; lr: 0.00000446;   0/761 tok/s; 333036 sec
[2020-04-02 16:11:25,524 INFO] Step 201300/210000; acc:  57.50; ppl:  7.37; xent: 2.00; lr: 0.00000446;   0/742 tok/s; 333120 sec
[2020-04-02 16:12:24,039 INFO] Loading train dataset from ../bert_data/cnndm.train.134.bert.pt, number of examples: 2001
[2020-04-02 16:12:51,019 INFO] Step 201350/210000; acc:  58.14; ppl:  6.79; xent: 1.92; lr: 0.00000446;   0/566 tok/s; 333206 sec
[2020-04-02 16:14:15,061 INFO] Step 201400/210000; acc:  54.64; ppl:  7.93; xent: 2.07; lr: 0.00000446;   0/575 tok/s; 333290 sec
[2020-04-02 16:15:11,858 INFO] Loading train dataset from ../bert_data/cnndm.train.135.bert.pt, number of examples: 1999
[2020-04-02 16:15:40,389 INFO] Step 201450/210000; acc:  58.68; ppl:  6.32; xent: 1.84; lr: 0.00000446;   0/641 tok/s; 333375 sec
[2020-04-02 16:17:04,564 INFO] Step 201500/210000; acc:  55.52; ppl:  7.86; xent: 2.06; lr: 0.00000446;   0/645 tok/s; 333459 sec
[2020-04-02 16:17:04,589 INFO] Saving checkpoint ../models/model_step_201500.pt
[2020-04-02 16:18:01,494 INFO] Loading train dataset from ../bert_data/cnndm.train.136.bert.pt, number of examples: 2001
[2020-04-02 16:18:31,654 INFO] Step 201550/210000; acc:  52.74; ppl:  8.84; xent: 2.18; lr: 0.00000445;   0/723 tok/s; 333546 sec
[2020-04-02 16:19:55,584 INFO] Step 201600/210000; acc:  50.50; ppl:  9.28; xent: 2.23; lr: 0.00000445;   0/678 tok/s; 333630 sec
[2020-04-02 16:20:48,582 INFO] Loading train dataset from ../bert_data/cnndm.train.137.bert.pt, number of examples: 2000
[2020-04-02 16:21:20,238 INFO] Step 201650/210000; acc:  47.84; ppl: 12.10; xent: 2.49; lr: 0.00000445;   0/758 tok/s; 333715 sec
[2020-04-02 16:22:44,240 INFO] Step 201700/210000; acc:  53.79; ppl:  7.96; xent: 2.07; lr: 0.00000445;   0/817 tok/s; 333799 sec
[2020-04-02 16:23:37,290 INFO] Loading train dataset from ../bert_data/cnndm.train.138.bert.pt, number of examples: 2000
[2020-04-02 16:24:09,247 INFO] Step 201750/210000; acc:  51.76; ppl: 10.18; xent: 2.32; lr: 0.00000445;   0/823 tok/s; 333884 sec
[2020-04-02 16:25:33,630 INFO] Step 201800/210000; acc:  54.04; ppl:  8.23; xent: 2.11; lr: 0.00000445;   0/802 tok/s; 333968 sec
[2020-04-02 16:26:24,936 INFO] Loading train dataset from ../bert_data/cnndm.train.139.bert.pt, number of examples: 2001
[2020-04-02 16:26:58,454 INFO] Step 201850/210000; acc:  57.06; ppl:  6.77; xent: 1.91; lr: 0.00000445;   0/850 tok/s; 334053 sec
[2020-04-02 16:28:22,715 INFO] Step 201900/210000; acc:  55.57; ppl:  7.61; xent: 2.03; lr: 0.00000445;   0/801 tok/s; 334138 sec
[2020-04-02 16:29:14,366 INFO] Loading train dataset from ../bert_data/cnndm.train.14.bert.pt, number of examples: 1998
[2020-04-02 16:29:48,131 INFO] Step 201950/210000; acc:  53.75; ppl:  7.78; xent: 2.05; lr: 0.00000445;   0/829 tok/s; 334223 sec
[2020-04-02 16:31:12,705 INFO] Step 202000/210000; acc:  57.21; ppl:  7.13; xent: 1.96; lr: 0.00000445;   0/867 tok/s; 334308 sec
[2020-04-02 16:31:12,727 INFO] Saving checkpoint ../models/model_step_202000.pt
[2020-04-02 16:32:05,190 INFO] Loading train dataset from ../bert_data/cnndm.train.140.bert.pt, number of examples: 2000
[2020-04-02 16:32:40,862 INFO] Step 202050/210000; acc:  60.00; ppl:  5.79; xent: 1.76; lr: 0.00000445;   0/501 tok/s; 334396 sec
[2020-04-02 16:34:05,214 INFO] Step 202100/210000; acc:  55.51; ppl:  8.11; xent: 2.09; lr: 0.00000445;   0/836 tok/s; 334480 sec
[2020-04-02 16:34:53,007 INFO] Loading train dataset from ../bert_data/cnndm.train.141.bert.pt, number of examples: 1999
[2020-04-02 16:35:30,050 INFO] Step 202150/210000; acc:  57.66; ppl:  7.23; xent: 1.98; lr: 0.00000445;   0/655 tok/s; 334565 sec
[2020-04-02 16:36:54,034 INFO] Step 202200/210000; acc:  55.64; ppl:  7.35; xent: 1.99; lr: 0.00000445;   0/465 tok/s; 334649 sec
[2020-04-02 16:37:40,188 INFO] Loading train dataset from ../bert_data/cnndm.train.142.bert.pt, number of examples: 2000
[2020-04-02 16:38:19,048 INFO] Step 202250/210000; acc:  59.84; ppl:  6.47; xent: 1.87; lr: 0.00000445;   0/588 tok/s; 334734 sec
[2020-04-02 16:39:43,256 INFO] Step 202300/210000; acc:  55.78; ppl:  7.37; xent: 2.00; lr: 0.00000445;   0/477 tok/s; 334818 sec
[2020-04-02 16:40:27,804 INFO] Loading train dataset from ../bert_data/cnndm.train.143.bert.pt, number of examples: 1084
[2020-04-02 16:41:08,283 INFO] Step 202350/210000; acc:  58.75; ppl:  6.19; xent: 1.82; lr: 0.00000445;   0/626 tok/s; 334903 sec
[2020-04-02 16:41:59,175 INFO] Loading train dataset from ../bert_data/cnndm.train.15.bert.pt, number of examples: 1999
[2020-04-02 16:42:32,708 INFO] Step 202400/210000; acc:  50.42; ppl: 10.88; xent: 2.39; lr: 0.00000445;   0/827 tok/s; 334988 sec
[2020-04-02 16:43:56,660 INFO] Step 202450/210000; acc:  56.73; ppl:  7.13; xent: 1.96; lr: 0.00000444;   0/866 tok/s; 335071 sec
[2020-04-02 16:44:47,522 INFO] Loading train dataset from ../bert_data/cnndm.train.150.bert.pt, number of examples: 1985
[2020-04-02 16:45:17,129 INFO] Step 202500/210000; acc:  75.98; ppl:  2.50; xent: 0.92; lr: 0.00000444;   0/389 tok/s; 335152 sec
[2020-04-02 16:45:17,141 INFO] Saving checkpoint ../models/model_step_202500.pt
[2020-04-02 16:46:32,506 INFO] Step 202550/210000; acc:  67.80; ppl:  3.31; xent: 1.20; lr: 0.00000444;   0/512 tok/s; 335227 sec
[2020-04-02 16:47:07,991 INFO] Loading train dataset from ../bert_data/cnndm.train.151.bert.pt, number of examples: 1990
[2020-04-02 16:47:45,353 INFO] Step 202600/210000; acc:  70.79; ppl:  3.20; xent: 1.16; lr: 0.00000444;   0/635 tok/s; 335300 sec
[2020-04-02 16:48:59,079 INFO] Step 202650/210000; acc:  70.57; ppl:  3.19; xent: 1.16; lr: 0.00000444;   0/574 tok/s; 335374 sec
[2020-04-02 16:49:28,069 INFO] Loading train dataset from ../bert_data/cnndm.train.152.bert.pt, number of examples: 993
[2020-04-02 16:50:13,664 INFO] Step 202700/210000; acc:  71.03; ppl:  3.08; xent: 1.13; lr: 0.00000444;   0/580 tok/s; 335448 sec
[2020-04-02 16:50:39,399 INFO] Loading train dataset from ../bert_data/cnndm.train.153.bert.pt, number of examples: 1982
[2020-04-02 16:51:34,959 INFO] Step 202750/210000; acc:  45.84; ppl: 12.11; xent: 2.49; lr: 0.00000444;   0/1245 tok/s; 335530 sec
[2020-04-02 16:52:58,948 INFO] Step 202800/210000; acc:  54.57; ppl:  6.31; xent: 1.84; lr: 0.00000444;   0/667 tok/s; 335614 sec
[2020-04-02 16:53:17,896 INFO] Loading train dataset from ../bert_data/cnndm.train.154.bert.pt, number of examples: 1978
[2020-04-02 16:54:23,358 INFO] Step 202850/210000; acc:  47.95; ppl: 10.46; xent: 2.35; lr: 0.00000444;   0/912 tok/s; 335698 sec
[2020-04-02 16:55:47,065 INFO] Step 202900/210000; acc:  52.68; ppl:  7.39; xent: 2.00; lr: 0.00000444;   0/834 tok/s; 335782 sec
[2020-04-02 16:55:56,336 INFO] Loading train dataset from ../bert_data/cnndm.train.155.bert.pt, number of examples: 1988
[2020-04-02 16:57:11,281 INFO] Step 202950/210000; acc:  56.60; ppl:  5.67; xent: 1.73; lr: 0.00000444;   0/529 tok/s; 335866 sec
[2020-04-02 16:58:34,868 INFO] Step 203000/210000; acc:  45.64; ppl: 10.83; xent: 2.38; lr: 0.00000444;   0/1206 tok/s; 335950 sec
[2020-04-02 16:58:34,872 INFO] Saving checkpoint ../models/model_step_203000.pt
[2020-04-02 16:58:37,510 INFO] Loading train dataset from ../bert_data/cnndm.train.156.bert.pt, number of examples: 1993
[2020-04-02 17:00:00,883 INFO] Step 203050/210000; acc:  59.16; ppl:  5.71; xent: 1.74; lr: 0.00000444;   0/670 tok/s; 336036 sec
[2020-04-02 17:01:16,782 INFO] Loading train dataset from ../bert_data/cnndm.train.157.bert.pt, number of examples: 1981
[2020-04-02 17:01:25,317 INFO] Step 203100/210000; acc:  48.79; ppl:  9.45; xent: 2.25; lr: 0.00000444;   0/1215 tok/s; 336120 sec
[2020-04-02 17:02:48,926 INFO] Step 203150/210000; acc:  51.60; ppl:  7.89; xent: 2.07; lr: 0.00000444;   0/713 tok/s; 336204 sec
[2020-04-02 17:03:56,204 INFO] Loading train dataset from ../bert_data/cnndm.train.158.bert.pt, number of examples: 1987
[2020-04-02 17:04:14,656 INFO] Step 203200/210000; acc:  60.53; ppl:  4.39; xent: 1.48; lr: 0.00000444;   0/453 tok/s; 336289 sec
[2020-04-02 17:05:37,759 INFO] Step 203250/210000; acc:  48.91; ppl:  9.09; xent: 2.21; lr: 0.00000444;   0/1065 tok/s; 336373 sec
[2020-04-02 17:06:34,904 INFO] Loading train dataset from ../bert_data/cnndm.train.159.bert.pt, number of examples: 1986
[2020-04-02 17:07:01,825 INFO] Step 203300/210000; acc:  63.18; ppl:  4.46; xent: 1.49; lr: 0.00000444;   0/487 tok/s; 336457 sec
[2020-04-02 17:08:24,978 INFO] Step 203350/210000; acc:  47.70; ppl:  9.69; xent: 2.27; lr: 0.00000444;   0/956 tok/s; 336540 sec
[2020-04-02 17:09:14,811 INFO] Loading train dataset from ../bert_data/cnndm.train.16.bert.pt, number of examples: 2001
[2020-04-02 17:09:50,156 INFO] Step 203400/210000; acc:  53.68; ppl:  8.06; xent: 2.09; lr: 0.00000443;   0/527 tok/s; 336625 sec
[2020-04-02 17:11:14,368 INFO] Step 203450/210000; acc:  51.36; ppl: 10.45; xent: 2.35; lr: 0.00000443;   0/965 tok/s; 336709 sec
[2020-04-02 17:12:02,411 INFO] Loading train dataset from ../bert_data/cnndm.train.160.bert.pt, number of examples: 1983
[2020-04-02 17:12:39,377 INFO] Step 203500/210000; acc:  53.25; ppl:  7.18; xent: 1.97; lr: 0.00000443;   0/868 tok/s; 336794 sec
[2020-04-02 17:12:39,380 INFO] Saving checkpoint ../models/model_step_203500.pt
[2020-04-02 17:14:05,796 INFO] Step 203550/210000; acc:  46.73; ppl: 11.13; xent: 2.41; lr: 0.00000443;   0/1173 tok/s; 336881 sec
[2020-04-02 17:14:45,166 INFO] Loading train dataset from ../bert_data/cnndm.train.161.bert.pt, number of examples: 1990
[2020-04-02 17:15:30,115 INFO] Step 203600/210000; acc:  49.40; ppl:  8.11; xent: 2.09; lr: 0.00000443;   0/917 tok/s; 336965 sec
[2020-04-02 17:16:53,245 INFO] Step 203650/210000; acc:  50.25; ppl:  9.35; xent: 2.24; lr: 0.00000443;   0/1153 tok/s; 337048 sec
[2020-04-02 17:17:23,950 INFO] Loading train dataset from ../bert_data/cnndm.train.162.bert.pt, number of examples: 1986
[2020-04-02 17:18:18,085 INFO] Step 203700/210000; acc:  50.85; ppl:  8.31; xent: 2.12; lr: 0.00000443;   0/918 tok/s; 337133 sec
[2020-04-02 17:19:41,741 INFO] Step 203750/210000; acc:  47.77; ppl: 10.29; xent: 2.33; lr: 0.00000443;   0/862 tok/s; 337217 sec
[2020-04-02 17:20:04,293 INFO] Loading train dataset from ../bert_data/cnndm.train.163.bert.pt, number of examples: 1982
[2020-04-02 17:21:06,045 INFO] Step 203800/210000; acc:  50.15; ppl:  8.62; xent: 2.15; lr: 0.00000443;   0/1006 tok/s; 337301 sec
[2020-04-02 17:22:29,268 INFO] Step 203850/210000; acc:  57.50; ppl:  6.53; xent: 1.88; lr: 0.00000443;   0/551 tok/s; 337384 sec
[2020-04-02 17:22:41,193 INFO] Loading train dataset from ../bert_data/cnndm.train.164.bert.pt, number of examples: 1984
[2020-04-02 17:23:52,560 INFO] Step 203900/210000; acc:  50.08; ppl:  8.84; xent: 2.18; lr: 0.00000443;   0/1115 tok/s; 337467 sec
[2020-04-02 17:25:16,378 INFO] Step 203950/210000; acc:  58.01; ppl:  5.78; xent: 1.75; lr: 0.00000443;   0/630 tok/s; 337551 sec
[2020-04-02 17:25:20,338 INFO] Loading train dataset from ../bert_data/cnndm.train.165.bert.pt, number of examples: 1984
[2020-04-02 17:26:39,977 INFO] Step 204000/210000; acc:  46.83; ppl: 10.96; xent: 2.39; lr: 0.00000443;   0/1247 tok/s; 337635 sec
[2020-04-02 17:26:39,981 INFO] Saving checkpoint ../models/model_step_204000.pt
[2020-04-02 17:28:01,287 INFO] Loading train dataset from ../bert_data/cnndm.train.166.bert.pt, number of examples: 1370
[2020-04-02 17:28:06,189 INFO] Step 204050/210000; acc:  53.97; ppl:  7.03; xent: 1.95; lr: 0.00000443;   0/1065 tok/s; 337721 sec
[2020-04-02 17:29:30,043 INFO] Step 204100/210000; acc:  46.90; ppl: 10.26; xent: 2.33; lr: 0.00000443;   0/1182 tok/s; 337805 sec
[2020-04-02 17:29:50,600 INFO] Loading train dataset from ../bert_data/cnndm.train.167.bert.pt, number of examples: 1089
[2020-04-02 17:30:52,348 INFO] Step 204150/210000; acc:  58.39; ppl:  5.52; xent: 1.71; lr: 0.00000443;   0/1024 tok/s; 337887 sec
[2020-04-02 17:31:16,458 INFO] Loading train dataset from ../bert_data/cnndm.train.168.bert.pt, number of examples: 1991
[2020-04-02 17:32:12,846 INFO] Step 204200/210000; acc:  51.34; ppl:  8.93; xent: 2.19; lr: 0.00000443;   0/779 tok/s; 337968 sec
[2020-04-02 17:33:35,652 INFO] Step 204250/210000; acc:  53.50; ppl:  7.26; xent: 1.98; lr: 0.00000443;   0/964 tok/s; 338050 sec
[2020-04-02 17:33:53,380 INFO] Loading train dataset from ../bert_data/cnndm.train.169.bert.pt, number of examples: 1995
[2020-04-02 17:34:56,756 INFO] Step 204300/210000; acc:  59.42; ppl:  5.85; xent: 1.77; lr: 0.00000442;   0/671 tok/s; 338132 sec
[2020-04-02 17:36:18,771 INFO] Step 204350/210000; acc:  56.01; ppl:  6.73; xent: 1.91; lr: 0.00000442;   0/898 tok/s; 338214 sec
[2020-04-02 17:36:29,602 INFO] Loading train dataset from ../bert_data/cnndm.train.17.bert.pt, number of examples: 2000
[2020-04-02 17:37:43,643 INFO] Step 204400/210000; acc:  51.51; ppl:  8.72; xent: 2.17; lr: 0.00000442;   0/822 tok/s; 338298 sec
[2020-04-02 17:39:07,674 INFO] Step 204450/210000; acc:  48.76; ppl: 10.41; xent: 2.34; lr: 0.00000442;   0/817 tok/s; 338382 sec
[2020-04-02 17:39:16,900 INFO] Loading train dataset from ../bert_data/cnndm.train.170.bert.pt, number of examples: 1981
[2020-04-02 17:40:30,954 INFO] Step 204500/210000; acc:  62.40; ppl:  4.20; xent: 1.43; lr: 0.00000442;   0/638 tok/s; 338466 sec
[2020-04-02 17:40:30,957 INFO] Saving checkpoint ../models/model_step_204500.pt
[2020-04-02 17:41:54,903 INFO] Loading train dataset from ../bert_data/cnndm.train.171.bert.pt, number of examples: 1990
[2020-04-02 17:41:56,620 INFO] Step 204550/210000; acc:  60.27; ppl:  5.18; xent: 1.64; lr: 0.00000442;   0/449 tok/s; 338551 sec
[2020-04-02 17:43:17,742 INFO] Step 204600/210000; acc:  56.97; ppl:  5.79; xent: 1.76; lr: 0.00000442;   0/933 tok/s; 338633 sec
[2020-04-02 17:44:30,363 INFO] Loading train dataset from ../bert_data/cnndm.train.172.bert.pt, number of examples: 1983
[2020-04-02 17:44:40,294 INFO] Step 204650/210000; acc:  57.77; ppl:  5.94; xent: 1.78; lr: 0.00000442;   0/541 tok/s; 338715 sec
[2020-04-02 17:46:01,609 INFO] Step 204700/210000; acc:  54.86; ppl:  6.80; xent: 1.92; lr: 0.00000442;   0/1038 tok/s; 338796 sec
[2020-04-02 17:47:05,779 INFO] Loading train dataset from ../bert_data/cnndm.train.18.bert.pt, number of examples: 1998
[2020-04-02 17:47:24,561 INFO] Step 204750/210000; acc:  55.35; ppl:  7.15; xent: 1.97; lr: 0.00000442;   0/542 tok/s; 338879 sec
[2020-04-02 17:48:48,819 INFO] Step 204800/210000; acc:  51.48; ppl:  9.49; xent: 2.25; lr: 0.00000442;   0/590 tok/s; 338964 sec
[2020-04-02 17:49:53,428 INFO] Loading train dataset from ../bert_data/cnndm.train.19.bert.pt, number of examples: 2000
[2020-04-02 17:50:13,706 INFO] Step 204850/210000; acc:  57.72; ppl:  6.71; xent: 1.90; lr: 0.00000442;   0/598 tok/s; 339049 sec
[2020-04-02 17:51:38,110 INFO] Step 204900/210000; acc:  53.06; ppl:  8.16; xent: 2.10; lr: 0.00000442;   0/755 tok/s; 339133 sec
[2020-04-02 17:52:41,316 INFO] Loading train dataset from ../bert_data/cnndm.train.2.bert.pt, number of examples: 2001
[2020-04-02 17:53:03,148 INFO] Step 204950/210000; acc:  54.01; ppl:  9.79; xent: 2.28; lr: 0.00000442;   0/681 tok/s; 339218 sec
[2020-04-02 17:54:27,531 INFO] Step 205000/210000; acc:  48.93; ppl: 11.42; xent: 2.44; lr: 0.00000442;   0/588 tok/s; 339302 sec
[2020-04-02 17:54:27,556 INFO] Saving checkpoint ../models/model_step_205000.pt
[2020-04-02 17:55:31,464 INFO] Loading train dataset from ../bert_data/cnndm.train.20.bert.pt, number of examples: 2000
[2020-04-02 17:55:54,937 INFO] Step 205050/210000; acc:  57.05; ppl:  7.04; xent: 1.95; lr: 0.00000442;   0/856 tok/s; 339390 sec
[2020-04-02 17:57:19,965 INFO] Step 205100/210000; acc:  55.12; ppl:  7.43; xent: 2.00; lr: 0.00000442;   0/852 tok/s; 339475 sec
[2020-04-02 17:58:19,647 INFO] Loading train dataset from ../bert_data/cnndm.train.21.bert.pt, number of examples: 2001
[2020-04-02 17:58:44,969 INFO] Step 205150/210000; acc:  56.64; ppl:  7.33; xent: 1.99; lr: 0.00000442;   0/854 tok/s; 339560 sec
[2020-04-02 18:00:09,360 INFO] Step 205200/210000; acc:  61.24; ppl:  5.85; xent: 1.77; lr: 0.00000442;   0/777 tok/s; 339644 sec
[2020-04-02 18:01:08,922 INFO] Loading train dataset from ../bert_data/cnndm.train.22.bert.pt, number of examples: 1999
[2020-04-02 18:01:34,450 INFO] Step 205250/210000; acc:  52.60; ppl:  8.41; xent: 2.13; lr: 0.00000441;   0/882 tok/s; 339729 sec
[2020-04-02 18:02:58,640 INFO] Step 205300/210000; acc:  57.85; ppl:  6.56; xent: 1.88; lr: 0.00000441;   0/781 tok/s; 339813 sec
[2020-04-02 18:03:56,752 INFO] Loading train dataset from ../bert_data/cnndm.train.23.bert.pt, number of examples: 2001
[2020-04-02 18:04:23,925 INFO] Step 205350/210000; acc:  56.20; ppl:  7.49; xent: 2.01; lr: 0.00000441;   0/803 tok/s; 339899 sec
[2020-04-02 18:05:47,991 INFO] Step 205400/210000; acc:  52.33; ppl: 10.08; xent: 2.31; lr: 0.00000441;   0/801 tok/s; 339983 sec
[2020-04-02 18:06:44,514 INFO] Loading train dataset from ../bert_data/cnndm.train.24.bert.pt, number of examples: 2001
[2020-04-02 18:07:12,985 INFO] Step 205450/210000; acc:  54.06; ppl:  8.35; xent: 2.12; lr: 0.00000441;   0/551 tok/s; 340068 sec
[2020-04-02 18:08:37,683 INFO] Step 205500/210000; acc:  56.23; ppl:  7.68; xent: 2.04; lr: 0.00000441;   0/741 tok/s; 340153 sec
[2020-04-02 18:08:37,715 INFO] Saving checkpoint ../models/model_step_205500.pt
[2020-04-02 18:09:36,445 INFO] Loading train dataset from ../bert_data/cnndm.train.25.bert.pt, number of examples: 2001
[2020-04-02 18:10:05,036 INFO] Step 205550/210000; acc:  58.69; ppl:  6.28; xent: 1.84; lr: 0.00000441;   0/574 tok/s; 340240 sec
[2020-04-02 18:11:29,417 INFO] Step 205600/210000; acc:  55.88; ppl:  7.80; xent: 2.05; lr: 0.00000441;   0/567 tok/s; 340324 sec
[2020-04-02 18:12:24,159 INFO] Loading train dataset from ../bert_data/cnndm.train.26.bert.pt, number of examples: 2000
[2020-04-02 18:12:54,768 INFO] Step 205650/210000; acc:  53.79; ppl:  8.80; xent: 2.18; lr: 0.00000441;   0/613 tok/s; 340410 sec
[2020-04-02 18:14:18,367 INFO] Step 205700/210000; acc:  52.08; ppl:  8.91; xent: 2.19; lr: 0.00000441;   0/556 tok/s; 340493 sec
[2020-04-02 18:15:13,273 INFO] Loading train dataset from ../bert_data/cnndm.train.27.bert.pt, number of examples: 2001
[2020-04-02 18:15:43,675 INFO] Step 205750/210000; acc:  49.73; ppl: 10.16; xent: 2.32; lr: 0.00000441;   0/790 tok/s; 340578 sec
[2020-04-02 18:17:08,123 INFO] Step 205800/210000; acc:  50.00; ppl: 11.71; xent: 2.46; lr: 0.00000441;   0/776 tok/s; 340663 sec
[2020-04-02 18:18:01,170 INFO] Loading train dataset from ../bert_data/cnndm.train.28.bert.pt, number of examples: 2000
[2020-04-02 18:18:33,380 INFO] Step 205850/210000; acc:  56.95; ppl:  7.37; xent: 2.00; lr: 0.00000441;   0/893 tok/s; 340748 sec
[2020-04-02 18:19:58,176 INFO] Step 205900/210000; acc:  53.77; ppl:  8.57; xent: 2.15; lr: 0.00000441;   0/759 tok/s; 340833 sec
[2020-04-02 18:20:49,577 INFO] Loading train dataset from ../bert_data/cnndm.train.29.bert.pt, number of examples: 1999
[2020-04-02 18:21:23,282 INFO] Step 205950/210000; acc:  56.79; ppl:  7.86; xent: 2.06; lr: 0.00000441;   0/906 tok/s; 340918 sec
[2020-04-02 18:22:47,716 INFO] Step 206000/210000; acc:  57.29; ppl:  7.38; xent: 2.00; lr: 0.00000441;   0/770 tok/s; 341003 sec
[2020-04-02 18:22:47,741 INFO] Saving checkpoint ../models/model_step_206000.pt
[2020-04-02 18:23:39,770 INFO] Loading train dataset from ../bert_data/cnndm.train.3.bert.pt, number of examples: 2001
[2020-04-02 18:24:14,880 INFO] Step 206050/210000; acc:  56.45; ppl:  7.98; xent: 2.08; lr: 0.00000441;   0/853 tok/s; 341090 sec
[2020-04-02 18:25:38,814 INFO] Step 206100/210000; acc:  59.91; ppl:  6.53; xent: 1.88; lr: 0.00000441;   0/890 tok/s; 341174 sec
[2020-04-02 18:26:28,364 INFO] Loading train dataset from ../bert_data/cnndm.train.30.bert.pt, number of examples: 1996
[2020-04-02 18:27:03,653 INFO] Step 206150/210000; acc:  50.76; ppl:  9.02; xent: 2.20; lr: 0.00000440;   0/611 tok/s; 341258 sec
[2020-04-02 18:28:28,278 INFO] Step 206200/210000; acc:  52.36; ppl:  9.68; xent: 2.27; lr: 0.00000440;   0/642 tok/s; 341343 sec
[2020-04-02 18:29:16,320 INFO] Loading train dataset from ../bert_data/cnndm.train.31.bert.pt, number of examples: 2000
[2020-04-02 18:29:53,616 INFO] Step 206250/210000; acc:  51.81; ppl:  9.03; xent: 2.20; lr: 0.00000440;   0/661 tok/s; 341428 sec
[2020-04-02 18:31:17,917 INFO] Step 206300/210000; acc:  54.41; ppl:  8.04; xent: 2.08; lr: 0.00000440;   0/517 tok/s; 341513 sec
[2020-04-02 18:32:04,145 INFO] Loading train dataset from ../bert_data/cnndm.train.32.bert.pt, number of examples: 1998
[2020-04-02 18:32:43,120 INFO] Step 206350/210000; acc:  55.00; ppl:  8.15; xent: 2.10; lr: 0.00000440;   0/664 tok/s; 341598 sec
[2020-04-02 18:34:06,934 INFO] Step 206400/210000; acc:  54.35; ppl:  7.83; xent: 2.06; lr: 0.00000440;   0/630 tok/s; 341682 sec
[2020-04-02 18:34:51,791 INFO] Loading train dataset from ../bert_data/cnndm.train.33.bert.pt, number of examples: 1999
[2020-04-02 18:35:32,361 INFO] Step 206450/210000; acc:  49.53; ppl: 10.95; xent: 2.39; lr: 0.00000440;   0/706 tok/s; 341767 sec
[2020-04-02 18:36:56,300 INFO] Step 206500/210000; acc:  55.85; ppl:  7.57; xent: 2.02; lr: 0.00000440;   0/671 tok/s; 341851 sec
[2020-04-02 18:36:56,325 INFO] Saving checkpoint ../models/model_step_206500.pt
[2020-04-02 18:37:41,433 INFO] Loading train dataset from ../bert_data/cnndm.train.34.bert.pt, number of examples: 2000
[2020-04-02 18:38:23,246 INFO] Step 206550/210000; acc:  56.59; ppl:  7.53; xent: 2.02; lr: 0.00000440;   0/759 tok/s; 341938 sec
[2020-04-02 18:39:47,682 INFO] Step 206600/210000; acc:  54.24; ppl:  8.19; xent: 2.10; lr: 0.00000440;   0/747 tok/s; 342023 sec
[2020-04-02 18:40:30,485 INFO] Loading train dataset from ../bert_data/cnndm.train.35.bert.pt, number of examples: 1998
[2020-04-02 18:41:12,804 INFO] Step 206650/210000; acc:  49.91; ppl: 10.49; xent: 2.35; lr: 0.00000440;   0/969 tok/s; 342108 sec
[2020-04-02 18:42:36,670 INFO] Step 206700/210000; acc:  50.49; ppl: 11.70; xent: 2.46; lr: 0.00000440;   0/800 tok/s; 342191 sec
[2020-04-02 18:43:18,045 INFO] Loading train dataset from ../bert_data/cnndm.train.36.bert.pt, number of examples: 2000
[2020-04-02 18:44:01,982 INFO] Step 206750/210000; acc:  47.45; ppl: 12.50; xent: 2.53; lr: 0.00000440;   0/672 tok/s; 342277 sec
[2020-04-02 18:45:26,799 INFO] Step 206800/210000; acc:  52.86; ppl:  9.33; xent: 2.23; lr: 0.00000440;   0/560 tok/s; 342362 sec
[2020-04-02 18:46:06,528 INFO] Loading train dataset from ../bert_data/cnndm.train.37.bert.pt, number of examples: 1999
[2020-04-02 18:46:52,427 INFO] Step 206850/210000; acc:  58.70; ppl:  6.59; xent: 1.89; lr: 0.00000440;   0/582 tok/s; 342447 sec
[2020-04-02 18:48:16,505 INFO] Step 206900/210000; acc:  47.41; ppl: 13.19; xent: 2.58; lr: 0.00000440;   0/649 tok/s; 342531 sec
[2020-04-02 18:48:54,234 INFO] Loading train dataset from ../bert_data/cnndm.train.38.bert.pt, number of examples: 2001
[2020-04-02 18:49:41,078 INFO] Step 206950/210000; acc:  59.60; ppl:  6.07; xent: 1.80; lr: 0.00000440;   0/665 tok/s; 342616 sec
[2020-04-02 18:51:05,584 INFO] Step 207000/210000; acc:  53.78; ppl:  7.59; xent: 2.03; lr: 0.00000440;   0/721 tok/s; 342700 sec
[2020-04-02 18:51:05,608 INFO] Saving checkpoint ../models/model_step_207000.pt
[2020-04-02 18:51:45,853 INFO] Loading train dataset from ../bert_data/cnndm.train.39.bert.pt, number of examples: 2000
[2020-04-02 18:52:33,015 INFO] Step 207050/210000; acc:  52.45; ppl:  9.17; xent: 2.22; lr: 0.00000440;   0/770 tok/s; 342788 sec
[2020-04-02 18:53:57,285 INFO] Step 207100/210000; acc:  54.15; ppl:  8.11; xent: 2.09; lr: 0.00000439;   0/753 tok/s; 342872 sec
[2020-04-02 18:54:33,665 INFO] Loading train dataset from ../bert_data/cnndm.train.4.bert.pt, number of examples: 2001
[2020-04-02 18:55:22,580 INFO] Step 207150/210000; acc:  57.85; ppl:  7.06; xent: 1.95; lr: 0.00000439;   0/886 tok/s; 342957 sec
[2020-04-02 18:56:46,640 INFO] Step 207200/210000; acc:  49.96; ppl: 11.02; xent: 2.40; lr: 0.00000439;   0/738 tok/s; 343041 sec
[2020-04-02 18:57:23,363 INFO] Loading train dataset from ../bert_data/cnndm.train.40.bert.pt, number of examples: 1999
[2020-04-02 18:58:12,187 INFO] Step 207250/210000; acc:  53.24; ppl:  8.01; xent: 2.08; lr: 0.00000439;   0/779 tok/s; 343127 sec
[2020-04-02 18:59:36,311 INFO] Step 207300/210000; acc:  52.20; ppl:  8.94; xent: 2.19; lr: 0.00000439;   0/764 tok/s; 343211 sec
[2020-04-02 19:00:10,734 INFO] Loading train dataset from ../bert_data/cnndm.train.41.bert.pt, number of examples: 2000
[2020-04-02 19:01:01,356 INFO] Step 207350/210000; acc:  59.72; ppl:  6.15; xent: 1.82; lr: 0.00000439;   0/813 tok/s; 343296 sec
[2020-04-02 19:02:25,570 INFO] Step 207400/210000; acc:  53.21; ppl:  8.36; xent: 2.12; lr: 0.00000439;   0/895 tok/s; 343380 sec
[2020-04-02 19:02:58,348 INFO] Loading train dataset from ../bert_data/cnndm.train.42.bert.pt, number of examples: 2000
[2020-04-02 19:03:51,395 INFO] Step 207450/210000; acc:  61.14; ppl:  6.10; xent: 1.81; lr: 0.00000439;   0/617 tok/s; 343466 sec
[2020-04-02 19:05:15,679 INFO] Step 207500/210000; acc:  57.64; ppl:  6.39; xent: 1.86; lr: 0.00000439;   0/577 tok/s; 343551 sec
[2020-04-02 19:05:15,706 INFO] Saving checkpoint ../models/model_step_207500.pt
[2020-04-02 19:05:49,169 INFO] Loading train dataset from ../bert_data/cnndm.train.43.bert.pt, number of examples: 2001
[2020-04-02 19:06:43,105 INFO] Step 207550/210000; acc:  57.58; ppl:  6.63; xent: 1.89; lr: 0.00000439;   0/999 tok/s; 343638 sec
[2020-04-02 19:08:07,430 INFO] Step 207600/210000; acc:  57.13; ppl:  7.96; xent: 2.07; lr: 0.00000439;   0/896 tok/s; 343722 sec
[2020-04-02 19:08:36,773 INFO] Loading train dataset from ../bert_data/cnndm.train.44.bert.pt, number of examples: 1998
[2020-04-02 19:09:32,735 INFO] Step 207650/210000; acc:  59.50; ppl:  6.11; xent: 1.81; lr: 0.00000439;   0/640 tok/s; 343808 sec
[2020-04-02 19:10:57,754 INFO] Step 207700/210000; acc:  55.29; ppl:  7.43; xent: 2.01; lr: 0.00000439;   0/670 tok/s; 343893 sec
[2020-04-02 19:11:25,591 INFO] Loading train dataset from ../bert_data/cnndm.train.45.bert.pt, number of examples: 2001
[2020-04-02 19:12:22,967 INFO] Step 207750/210000; acc:  56.65; ppl:  7.22; xent: 1.98; lr: 0.00000439;   0/839 tok/s; 343978 sec
[2020-04-02 19:13:47,373 INFO] Step 207800/210000; acc:  54.13; ppl:  8.23; xent: 2.11; lr: 0.00000439;   0/738 tok/s; 344062 sec
[2020-04-02 19:14:15,192 INFO] Loading train dataset from ../bert_data/cnndm.train.46.bert.pt, number of examples: 2001
[2020-04-02 19:15:12,171 INFO] Step 207850/210000; acc:  56.03; ppl:  8.25; xent: 2.11; lr: 0.00000439;   0/899 tok/s; 344147 sec
[2020-04-02 19:16:36,048 INFO] Step 207900/210000; acc:  52.49; ppl:  9.43; xent: 2.24; lr: 0.00000439;   0/663 tok/s; 344231 sec
[2020-04-02 19:17:02,128 INFO] Loading train dataset from ../bert_data/cnndm.train.47.bert.pt, number of examples: 2000
[2020-04-02 19:18:01,165 INFO] Step 207950/210000; acc:  54.72; ppl:  8.42; xent: 2.13; lr: 0.00000439;   0/802 tok/s; 344316 sec
[2020-04-02 19:19:25,562 INFO] Step 208000/210000; acc:  57.42; ppl:  6.93; xent: 1.94; lr: 0.00000439;   0/617 tok/s; 344400 sec
[2020-04-02 19:19:25,566 INFO] Saving checkpoint ../models/model_step_208000.pt
[2020-04-02 19:19:53,877 INFO] Loading train dataset from ../bert_data/cnndm.train.48.bert.pt, number of examples: 2000
[2020-04-02 19:20:52,712 INFO] Step 208050/210000; acc:  59.61; ppl:  6.27; xent: 1.84; lr: 0.00000438;   0/871 tok/s; 344488 sec
[2020-04-02 19:22:17,210 INFO] Step 208100/210000; acc:  56.01; ppl:  7.63; xent: 2.03; lr: 0.00000438;   0/847 tok/s; 344572 sec
[2020-04-02 19:22:41,890 INFO] Loading train dataset from ../bert_data/cnndm.train.49.bert.pt, number of examples: 2001
[2020-04-02 19:23:42,842 INFO] Step 208150/210000; acc:  62.65; ppl:  5.31; xent: 1.67; lr: 0.00000438;   0/898 tok/s; 344658 sec
[2020-04-02 19:25:07,121 INFO] Step 208200/210000; acc:  52.68; ppl:  9.23; xent: 2.22; lr: 0.00000438;   0/861 tok/s; 344742 sec
[2020-04-02 19:25:29,630 INFO] Loading train dataset from ../bert_data/cnndm.train.5.bert.pt, number of examples: 2001
[2020-04-02 19:26:31,985 INFO] Step 208250/210000; acc:  54.27; ppl:  8.06; xent: 2.09; lr: 0.00000438;   0/523 tok/s; 344827 sec
[2020-04-02 19:27:56,245 INFO] Step 208300/210000; acc:  58.23; ppl:  6.05; xent: 1.80; lr: 0.00000438;   0/563 tok/s; 344911 sec
[2020-04-02 19:28:18,913 INFO] Loading train dataset from ../bert_data/cnndm.train.50.bert.pt, number of examples: 2001
[2020-04-02 19:29:21,242 INFO] Step 208350/210000; acc:  59.59; ppl:  6.04; xent: 1.80; lr: 0.00000438;   0/565 tok/s; 344996 sec
[2020-04-02 19:30:45,407 INFO] Step 208400/210000; acc:  56.22; ppl:  7.35; xent: 1.99; lr: 0.00000438;   0/965 tok/s; 345080 sec
[2020-04-02 19:31:06,195 INFO] Loading train dataset from ../bert_data/cnndm.train.51.bert.pt, number of examples: 2000
[2020-04-02 19:32:10,035 INFO] Step 208450/210000; acc:  53.77; ppl:  8.13; xent: 2.10; lr: 0.00000438;   0/590 tok/s; 345165 sec
[2020-04-02 19:33:34,437 INFO] Step 208500/210000; acc:  54.89; ppl:  8.38; xent: 2.13; lr: 0.00000438;   0/820 tok/s; 345249 sec
[2020-04-02 19:33:34,441 INFO] Saving checkpoint ../models/model_step_208500.pt
[2020-04-02 19:33:57,555 INFO] Loading train dataset from ../bert_data/cnndm.train.52.bert.pt, number of examples: 2001
[2020-04-02 19:35:01,913 INFO] Step 208550/210000; acc:  56.00; ppl:  8.29; xent: 2.11; lr: 0.00000438;   0/777 tok/s; 345337 sec
[2020-04-02 19:36:25,961 INFO] Step 208600/210000; acc:  55.15; ppl:  7.66; xent: 2.04; lr: 0.00000438;   0/743 tok/s; 345421 sec
[2020-04-02 19:36:45,298 INFO] Loading train dataset from ../bert_data/cnndm.train.53.bert.pt, number of examples: 1999
[2020-04-02 19:37:50,810 INFO] Step 208650/210000; acc:  51.78; ppl:  9.87; xent: 2.29; lr: 0.00000438;   0/810 tok/s; 345506 sec
[2020-04-02 19:39:15,237 INFO] Step 208700/210000; acc:  52.79; ppl:  9.12; xent: 2.21; lr: 0.00000438;   0/629 tok/s; 345590 sec
[2020-04-02 19:39:32,841 INFO] Loading train dataset from ../bert_data/cnndm.train.54.bert.pt, number of examples: 2001
[2020-04-02 19:40:40,105 INFO] Step 208750/210000; acc:  48.83; ppl: 11.56; xent: 2.45; lr: 0.00000438;   0/854 tok/s; 345675 sec
[2020-04-02 19:42:04,632 INFO] Step 208800/210000; acc:  56.28; ppl:  7.87; xent: 2.06; lr: 0.00000438;   0/754 tok/s; 345759 sec
[2020-04-02 19:42:22,431 INFO] Loading train dataset from ../bert_data/cnndm.train.55.bert.pt, number of examples: 1999
[2020-04-02 19:43:29,557 INFO] Step 208850/210000; acc:  48.85; ppl: 10.53; xent: 2.35; lr: 0.00000438;   0/816 tok/s; 345844 sec
[2020-04-02 19:44:53,974 INFO] Step 208900/210000; acc:  55.31; ppl:  8.17; xent: 2.10; lr: 0.00000438;   0/821 tok/s; 345929 sec
[2020-04-02 19:45:09,943 INFO] Loading train dataset from ../bert_data/cnndm.train.56.bert.pt, number of examples: 2001
[2020-04-02 19:46:19,457 INFO] Step 208950/210000; acc:  53.92; ppl:  8.57; xent: 2.15; lr: 0.00000438;   0/859 tok/s; 346014 sec
[2020-04-02 19:47:44,081 INFO] Step 209000/210000; acc:  57.50; ppl:  6.75; xent: 1.91; lr: 0.00000437;   0/861 tok/s; 346099 sec
[2020-04-02 19:47:44,085 INFO] Saving checkpoint ../models/model_step_209000.pt
[2020-04-02 19:48:00,644 INFO] Loading train dataset from ../bert_data/cnndm.train.57.bert.pt, number of examples: 1999
[2020-04-02 19:49:11,239 INFO] Step 209050/210000; acc:  56.56; ppl:  7.58; xent: 2.03; lr: 0.00000437;   0/544 tok/s; 346186 sec
[2020-04-02 19:50:35,751 INFO] Step 209100/210000; acc:  61.28; ppl:  5.80; xent: 1.76; lr: 0.00000437;   0/586 tok/s; 346271 sec
[2020-04-02 19:50:48,317 INFO] Loading train dataset from ../bert_data/cnndm.train.58.bert.pt, number of examples: 2000
[2020-04-02 19:52:01,181 INFO] Step 209150/210000; acc:  58.60; ppl:  6.82; xent: 1.92; lr: 0.00000437;   0/636 tok/s; 346356 sec
[2020-04-02 19:53:25,640 INFO] Step 209200/210000; acc:  54.52; ppl:  8.53; xent: 2.14; lr: 0.00000437;   0/591 tok/s; 346440 sec
[2020-04-02 19:53:38,696 INFO] Loading train dataset from ../bert_data/cnndm.train.59.bert.pt, number of examples: 2000
[2020-04-02 19:54:51,658 INFO] Step 209250/210000; acc:  58.42; ppl:  6.30; xent: 1.84; lr: 0.00000437;   0/736 tok/s; 346526 sec
[2020-04-02 19:56:16,243 INFO] Step 209300/210000; acc:  50.84; ppl: 10.47; xent: 2.35; lr: 0.00000437;   0/592 tok/s; 346611 sec
[2020-04-02 19:56:27,613 INFO] Loading train dataset from ../bert_data/cnndm.train.6.bert.pt, number of examples: 2001
[2020-04-02 19:57:41,916 INFO] Step 209350/210000; acc:  55.12; ppl:  8.04; xent: 2.08; lr: 0.00000437;   0/931 tok/s; 346697 sec
[2020-04-02 19:59:06,697 INFO] Step 209400/210000; acc:  53.65; ppl:  8.34; xent: 2.12; lr: 0.00000437;   0/849 tok/s; 346782 sec
[2020-04-02 19:59:15,961 INFO] Loading train dataset from ../bert_data/cnndm.train.60.bert.pt, number of examples: 2000
[2020-04-02 20:00:31,859 INFO] Step 209450/210000; acc:  54.96; ppl:  8.62; xent: 2.15; lr: 0.00000437;   0/722 tok/s; 346867 sec
[2020-04-02 20:01:56,373 INFO] Step 209500/210000; acc:  53.34; ppl:  9.50; xent: 2.25; lr: 0.00000437;   0/843 tok/s; 346951 sec
[2020-04-02 20:01:56,397 INFO] Saving checkpoint ../models/model_step_209500.pt
[2020-04-02 20:02:06,271 INFO] Loading train dataset from ../bert_data/cnndm.train.61.bert.pt, number of examples: 2001
[2020-04-02 20:03:24,128 INFO] Step 209550/210000; acc:  59.98; ppl:  5.62; xent: 1.73; lr: 0.00000437;   0/536 tok/s; 347039 sec
[2020-04-02 20:04:48,180 INFO] Step 209600/210000; acc:  50.65; ppl:  8.86; xent: 2.18; lr: 0.00000437;   0/563 tok/s; 347123 sec
[2020-04-02 20:04:53,963 INFO] Loading train dataset from ../bert_data/cnndm.train.62.bert.pt, number of examples: 2001
[2020-04-02 20:06:13,584 INFO] Step 209650/210000; acc:  57.03; ppl:  6.69; xent: 1.90; lr: 0.00000437;   0/537 tok/s; 347208 sec
[2020-04-02 20:07:37,830 INFO] Step 209700/210000; acc:  54.85; ppl:  8.12; xent: 2.09; lr: 0.00000437;   0/918 tok/s; 347293 sec
[2020-04-02 20:07:43,458 INFO] Loading train dataset from ../bert_data/cnndm.train.63.bert.pt, number of examples: 2001
[2020-04-02 20:09:02,586 INFO] Step 209750/210000; acc:  54.45; ppl:  7.12; xent: 1.96; lr: 0.00000437;   0/574 tok/s; 347377 sec
[2020-04-02 20:10:26,618 INFO] Step 209800/210000; acc:  51.98; ppl:  9.37; xent: 2.24; lr: 0.00000437;   0/839 tok/s; 347461 sec
[2020-04-02 20:10:30,740 INFO] Loading train dataset from ../bert_data/cnndm.train.64.bert.pt, number of examples: 2001
[2020-04-02 20:11:51,938 INFO] Step 209850/210000; acc:  53.21; ppl:  8.72; xent: 2.17; lr: 0.00000437;   0/510 tok/s; 347547 sec
[2020-04-02 20:13:16,532 INFO] Step 209900/210000; acc:  59.68; ppl:  5.80; xent: 1.76; lr: 0.00000437;   0/879 tok/s; 347631 sec
[2020-04-02 20:13:18,915 INFO] Loading train dataset from ../bert_data/cnndm.train.65.bert.pt, number of examples: 2000
[2020-04-02 20:14:41,286 INFO] Step 209950/210000; acc:  56.02; ppl:  7.71; xent: 2.04; lr: 0.00000436;   0/712 tok/s; 347716 sec
[2020-04-02 20:16:06,407 INFO] Step 210000/210000; acc:  60.76; ppl:  6.45; xent: 1.86; lr: 0.00000436;   0/630 tok/s; 347801 sec
[2020-04-02 20:16:06,433 INFO] Saving checkpoint ../models/model_step_210000.pt
[2020-04-02 20:16:08,768 INFO] Loading train dataset from ../bert_data/cnndm.train.0.bert.pt, number of examples: 2001
